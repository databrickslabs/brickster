[{"path":"https://databrickslabs.github.io/brickster/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"Apache License","title":"Apache License","text":"Version 2.0, January 2004 <http://www.apache.org/licenses/>","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/LICENSE.html","id":"id_1-definitions","dir":"","previous_headings":"Terms and Conditions for use, reproduction, and distribution","what":"1. Definitions","title":"Apache License","text":"“License” shall mean terms conditions use, reproduction, distribution defined Sections 1 9 document. “Licensor” shall mean copyright owner entity authorized copyright owner granting License. “Legal Entity” shall mean union acting entity entities control, controlled , common control entity. purposes definition, “control” means () power, direct indirect, cause direction management entity, whether contract otherwise, (ii) ownership fifty percent (50%) outstanding shares, (iii) beneficial ownership entity. “” (“”) shall mean individual Legal Entity exercising permissions granted License. “Source” form shall mean preferred form making modifications, including limited software source code, documentation source, configuration files. “Object” form shall mean form resulting mechanical transformation translation Source form, including limited compiled object code, generated documentation, conversions media types. “Work” shall mean work authorship, whether Source Object form, made available License, indicated copyright notice included attached work (example provided Appendix ). “Derivative Works” shall mean work, whether Source Object form, based (derived ) Work editorial revisions, annotations, elaborations, modifications represent, whole, original work authorship. purposes License, Derivative Works shall include works remain separable , merely link (bind name) interfaces , Work Derivative Works thereof. “Contribution” shall mean work authorship, including original version Work modifications additions Work Derivative Works thereof, intentionally submitted Licensor inclusion Work copyright owner individual Legal Entity authorized submit behalf copyright owner. purposes definition, “submitted” means form electronic, verbal, written communication sent Licensor representatives, including limited communication electronic mailing lists, source code control systems, issue tracking systems managed , behalf , Licensor purpose discussing improving Work, excluding communication conspicuously marked otherwise designated writing copyright owner “Contribution.” “Contributor” shall mean Licensor individual Legal Entity behalf Contribution received Licensor subsequently incorporated within Work.","code":""},{"path":"https://databrickslabs.github.io/brickster/LICENSE.html","id":"id_2-grant-of-copyright-license","dir":"","previous_headings":"Terms and Conditions for use, reproduction, and distribution","what":"2. Grant of Copyright License","title":"Apache License","text":"Subject terms conditions License, Contributor hereby grants perpetual, worldwide, non-exclusive, -charge, royalty-free, irrevocable copyright license reproduce, prepare Derivative Works , publicly display, publicly perform, sublicense, distribute Work Derivative Works Source Object form.","code":""},{"path":"https://databrickslabs.github.io/brickster/LICENSE.html","id":"id_3-grant-of-patent-license","dir":"","previous_headings":"Terms and Conditions for use, reproduction, and distribution","what":"3. Grant of Patent License","title":"Apache License","text":"Subject terms conditions License, Contributor hereby grants perpetual, worldwide, non-exclusive, -charge, royalty-free, irrevocable (except stated section) patent license make, made, use, offer sell, sell, import, otherwise transfer Work, license applies patent claims licensable Contributor necessarily infringed Contribution(s) alone combination Contribution(s) Work Contribution(s) submitted. institute patent litigation entity (including cross-claim counterclaim lawsuit) alleging Work Contribution incorporated within Work constitutes direct contributory patent infringement, patent licenses granted License Work shall terminate date litigation filed.","code":""},{"path":"https://databrickslabs.github.io/brickster/LICENSE.html","id":"id_4-redistribution","dir":"","previous_headings":"Terms and Conditions for use, reproduction, and distribution","what":"4. Redistribution","title":"Apache License","text":"may reproduce distribute copies Work Derivative Works thereof medium, without modifications, Source Object form, provided meet following conditions: () must give recipients Work Derivative Works copy License; (b) must cause modified files carry prominent notices stating changed files; (c) must retain, Source form Derivative Works distribute, copyright, patent, trademark, attribution notices Source form Work, excluding notices pertain part Derivative Works; (d) Work includes “NOTICE” text file part distribution, Derivative Works distribute must include readable copy attribution notices contained within NOTICE file, excluding notices pertain part Derivative Works, least one following places: within NOTICE text file distributed part Derivative Works; within Source form documentation, provided along Derivative Works; , within display generated Derivative Works, wherever third-party notices normally appear. contents NOTICE file informational purposes modify License. may add attribution notices within Derivative Works distribute, alongside addendum NOTICE text Work, provided additional attribution notices construed modifying License. may add copyright statement modifications may provide additional different license terms conditions use, reproduction, distribution modifications, Derivative Works whole, provided use, reproduction, distribution Work otherwise complies conditions stated License.","code":""},{"path":"https://databrickslabs.github.io/brickster/LICENSE.html","id":"id_5-submission-of-contributions","dir":"","previous_headings":"Terms and Conditions for use, reproduction, and distribution","what":"5. Submission of Contributions","title":"Apache License","text":"Unless explicitly state otherwise, Contribution intentionally submitted inclusion Work Licensor shall terms conditions License, without additional terms conditions. Notwithstanding , nothing herein shall supersede modify terms separate license agreement may executed Licensor regarding Contributions.","code":""},{"path":"https://databrickslabs.github.io/brickster/LICENSE.html","id":"id_6-trademarks","dir":"","previous_headings":"Terms and Conditions for use, reproduction, and distribution","what":"6. Trademarks","title":"Apache License","text":"License grant permission use trade names, trademarks, service marks, product names Licensor, except required reasonable customary use describing origin Work reproducing content NOTICE file.","code":""},{"path":"https://databrickslabs.github.io/brickster/LICENSE.html","id":"id_7-disclaimer-of-warranty","dir":"","previous_headings":"Terms and Conditions for use, reproduction, and distribution","what":"7. Disclaimer of Warranty","title":"Apache License","text":"Unless required applicable law agreed writing, Licensor provides Work (Contributor provides Contributions) “” BASIS, WITHOUT WARRANTIES CONDITIONS KIND, either express implied, including, without limitation, warranties conditions TITLE, NON-INFRINGEMENT, MERCHANTABILITY, FITNESS PARTICULAR PURPOSE. solely responsible determining appropriateness using redistributing Work assume risks associated exercise permissions License.","code":""},{"path":"https://databrickslabs.github.io/brickster/LICENSE.html","id":"id_8-limitation-of-liability","dir":"","previous_headings":"Terms and Conditions for use, reproduction, and distribution","what":"8. Limitation of Liability","title":"Apache License","text":"event legal theory, whether tort (including negligence), contract, otherwise, unless required applicable law (deliberate grossly negligent acts) agreed writing, shall Contributor liable damages, including direct, indirect, special, incidental, consequential damages character arising result License use inability use Work (including limited damages loss goodwill, work stoppage, computer failure malfunction, commercial damages losses), even Contributor advised possibility damages.","code":""},{"path":"https://databrickslabs.github.io/brickster/LICENSE.html","id":"id_9-accepting-warranty-or-additional-liability","dir":"","previous_headings":"Terms and Conditions for use, reproduction, and distribution","what":"9. Accepting Warranty or Additional Liability","title":"Apache License","text":"redistributing Work Derivative Works thereof, may choose offer, charge fee , acceptance support, warranty, indemnity, liability obligations /rights consistent License. However, accepting obligations, may act behalf sole responsibility, behalf Contributor, agree indemnify, defend, hold Contributor harmless liability incurred , claims asserted , Contributor reason accepting warranty additional liability. END TERMS CONDITIONS","code":""},{"path":"https://databrickslabs.github.io/brickster/LICENSE.html","id":"appendix-how-to-apply-the-apache-license-to-your-work","dir":"","previous_headings":"","what":"APPENDIX: How to apply the Apache License to your work","title":"Apache License","text":"apply Apache License work, attach following boilerplate notice, fields enclosed brackets [] replaced identifying information. (Don’t include brackets!) text enclosed appropriate comment syntax file format. also recommend file class name description purpose included “printed page” copyright notice easier identification within third-party archives.","code":"Copyright [yyyy] [name of copyright owner]  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at    http://www.apache.org/licenses/LICENSE-2.0  Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."},{"path":"https://databrickslabs.github.io/brickster/articles/cluster-management.html","id":"cluster-creation","dir":"Articles","previous_headings":"","what":"Cluster Creation","title":"Cluster Management","text":"Clusters number parameters can configured match needs given workload. db_cluster_create() facilitates creation cluster Databricks workspace cloud platforms (AWS, Azure, GCP). Depending cloud need change node types cloud_attrs one ; aws_attributes(), azure_attributes(), gcp_attributes(). create cluster AWS step using supporting functions. Refer documentation details use parameters mentioned (e.g. spark_conf). creating cluster may want check supported values number parameters. functions assist : db_cluster_get() provide details cluster just created, including information state. can useful may wish wait cluster RUNNING , exactly get_and_start_cluster() uses internally wait cluster running completing.","code":"library(brickster)  # create a small cluster on AWS with DBR 9.1 LTS new_cluster <- db_cluster_create(   name = \"brickster-cluster\",   spark_version = \"9.1.x-scala2.12\",   num_workers = 2,   node_type_id = \"m5a.xlarge\",   cloud_attrs = aws_attributes(     ebs_volume_count = 3,     ebs_volume_size = 100   ) ) cluster_info <- db_cluster_get(cluster_id = new_cluster$cluster_id) cluster_info$state"},{"path":"https://databrickslabs.github.io/brickster/articles/cluster-management.html","id":"editing-clusters","dir":"Articles","previous_headings":"","what":"Editing Clusters","title":"Cluster Management","text":"can edit Databricks clusters change various parameters using db_cluster_edit(). example, may decide want cluster autoscale 2-8 nodes add tags. However, intention change size given cluster db_cluster_resize() function simpler alternative. can either adjust number workers change autoscale range. range workers adjusted via autoscale number workers active cluster increased/decreased outside bounds. ’s important note specifying num_workers instead autoscale cluster existing autoscale range become fixed number workers point onward. Databricks clusters can “pinned” stops removed 30 days termination. db_cluster_pin() db_cluster_unpin() functions used changing cluster “pinned” .","code":"# we are required to input all parameters db_cluster_edit(   cluster_id = new_cluster$cluster_id,   name = \"brickster-cluster\",   spark_version = \"9.1.x-scala2.12\",   node_type_id = \"m5a.xlarge\",   autoscale = cluster_autoscale(min_workers = 2, max_workers = 8),   cloud_attrs = aws_attributes(     ebs_volume_count = 3,     ebs_volume_size = 100   ),   custom_tags = list(     purpose = \"brickster_cluster_demo\"   ) ) # adjust number autoscale range to be between 4-6 workers db_cluster_resize(   cluster_id = new_cluster$cluster_id,   autoscale = cluster_autoscale(min_workers = 4, max_workers = 6) ) # pin the cluster db_cluster_pin(cluster_id = new_cluster$cluster_id)  # unpin the cluster # db_cluster_unpin(cluster_id = new_cluster$cluster_id)"},{"path":"https://databrickslabs.github.io/brickster/articles/cluster-management.html","id":"cluster-state","dir":"Articles","previous_headings":"","what":"Cluster State","title":"Cluster Management","text":"functions can used manage state existing cluster","code":""},{"path":"https://databrickslabs.github.io/brickster/articles/cluster-management.html","id":"cluster-libraries","dir":"Articles","previous_headings":"","what":"Cluster Libraries","title":"Cluster Management","text":"Databricks clusters can libraries installed number sources using db_libs_install() associated libs_*() functions: convenience wait_for_lib_installs() function block libraries specified cluster finished installing. Installation libraries asynchronous complete background. db_libs_cluster_status() used check installation status libraries given cluster, db_libs_all_cluster_statuses() used getting status libraries across clusters workspace. Libraries can uninstalled using db_libs_uninstall(). Using db_libs_cluster_status() shows library uninstalled upon restart (e.g. db_cluster_restart()).","code":"# installing a package from CRAN on cluster db_libs_install(   cluster_id = new_cluster$cluster_id,   libraries = libraries(     lib_cran(package = \"palmerpenguins\"),     lib_cran(package = \"dplyr\")   ) ) wait_for_lib_installs(cluster_id = new_cluster$cluster_id) db_libs_cluster_status(cluster_id = new_cluster$cluster_id) db_libs_uninstall(   cluster_id = new_cluster$cluster_id,   libraries = libraries(     lib_cran(package = \"palmerpenguins\")   ) ) db_libs_cluster_status(cluster_id = new_cluster$cluster_id)"},{"path":"https://databrickslabs.github.io/brickster/articles/cluster-management.html","id":"events","dir":"Articles","previous_headings":"","what":"Events","title":"Cluster Management","text":"list events regarding clusters activity can fetched via db_cluster_events(). many event types can occur, default 50 recent events returned.","code":"events <- db_cluster_events(cluster_id = new_cluster$cluster_id) head(events, 1)"},{"path":"https://databrickslabs.github.io/brickster/articles/managing-jobs.html","id":"managing-databricks-jobs","dir":"Articles","previous_headings":"","what":"Managing Databricks Jobs","title":"Job Management","text":"brickster provides coverage Jobs REST API’s, allowing creation multi-task jobs via R code.","code":""},{"path":"https://databrickslabs.github.io/brickster/articles/managing-jobs.html","id":"job-basics","dir":"Articles","previous_headings":"Managing Databricks Jobs","what":"Job Basics","title":"Job Management","text":"listing jobs default behaviour returns first 25 jobs, configure number jobs returned can specify limit offset. expand_tasks parameter return task cluster details job (default FALSE). return details specific job can use db_jobs_get(), requires knowledge job_id can found via user interface Databricks using db_jobs_list(). job one tasks may dependence upon , execution job flow tasks - known “run” job. Jobs can scheduled start run particular schedule, direct trigger db_jobs_run_now(). number functions specific job runs execution metadata: using functions db_jobs_run_get_output() db_jobs_runs_export() required run_id can refer id given task. task within given “run” also run_id can referenced.","code":"# list all jobs within Databricks workspace # can control limit, offset, and if cluster/jobs details are returned jobs <- db_jobs_list(limit = 10)  # list all runs within a specific job job_runs <- db_jobs_runs_list(job_id = jobs[[1]]$job_id) # details of a specific job job_details <- db_jobs_get(job_id = jobs[[1]]$job_id)"},{"path":"https://databrickslabs.github.io/brickster/articles/managing-jobs.html","id":"creating-jobs","dir":"Articles","previous_headings":"Managing Databricks Jobs","what":"Creating Jobs","title":"Job Management","text":"brickster enables creation jobs R, jobs can simple single task jobs complex multi-task jobs dependencies share re-use clusters.","code":""},{"path":"https://databrickslabs.github.io/brickster/articles/managing-jobs.html","id":"simple-job","dir":"Articles","previous_headings":"Managing Databricks Jobs > Creating Jobs","what":"Simple Job","title":"Job Management","text":"single-task job can created following components: job_task() defines singular task new_cluster() required job_task(), defines compute specifications *_task(), one task functions e.g. notebook_task() creation simple job runs notebook paused schedule:","code":"# define a job task simple_task <- job_task(   task_key = \"simple_task\",   description = \"a simple task that runs a notebook\",   # specify a cluster for the job   new_cluster = new_cluster(     spark_version = \"16.4.x-scala2.12\",     driver_node_type_id = \"m5a.large\",     node_type_id = \"m5a.large\",     num_workers = 2,     cloud_attr = aws_attributes(ebs_volume_size = 32)   ),   # this task will be a notebook   task = notebook_task(notebook_path = \"/brickster/simple-notebook\") ) # create job with simple task simple_task_job <- db_jobs_create(   name = \"brickster example: simple\",   tasks = job_tasks(simple_task),   # 9am every day, paused currently   schedule = cron_schedule(     quartz_cron_expression = \"0 0 9 * * ?\",     pause_status = \"PAUSED\"   ) )"},{"path":"https://databrickslabs.github.io/brickster/articles/managing-jobs.html","id":"multiple-tasks","dir":"Articles","previous_headings":"Managing Databricks Jobs > Creating Jobs","what":"Multiple Tasks","title":"Job Management","text":"Jobs can extended beyond singular task, next example extends simple job example now three tasks. subsequent task depends priors completion can proceed, task also define new_cluster. Whilst job runs notebook time sake example demonstrates build dependencies.","code":"# one cluster definition, repeatedly use for each task multitask_cluster <- new_cluster(   spark_version = \"16.4.x-scala2.12\",   driver_node_type_id = \"m5a.large\",   node_type_id = \"m5a.large\",   num_workers = 2,   cloud_attr = aws_attributes(ebs_volume_size = 32) )  # each task will run the same notebook (just for this example) multitask_task <- notebook_task(notebook_path = \"/brickster/simple-notebook\")  # create three simple tasks that will depend on each other # task_a -> task_b -> task_c task_a <- job_task(   task_key = \"task_a\",   description = \"First task in the sequence\",   new_cluster = multitask_cluster,   task = multitask_task )  task_b <- job_task(   task_key = \"task_b\",   description = \"Second task in the sequence\",   new_cluster = multitask_cluster,   task = multitask_task,   depends_on = \"task_a\" )  task_c <- job_task(   task_key = \"task_c\",   description = \"Third task in the sequence\",   new_cluster = multitask_cluster,   task = multitask_task,   depends_on = \"task_b\" ) # create job with multiple tasks multitask_job <- db_jobs_create(   name = \"brickster example: multi-task\",   tasks = job_tasks(task_a, task_b, task_c),   # 9am every day, paused currently   schedule = cron_schedule(     quartz_cron_expression = \"0 0 9 * * ?\",     pause_status = \"PAUSED\"   ) )"},{"path":"https://databrickslabs.github.io/brickster/articles/managing-jobs.html","id":"cluster-reuse","dir":"Articles","previous_headings":"Managing Databricks Jobs > Creating Jobs","what":"Cluster Reuse","title":"Job Management","text":"multiple tasks example one shortcoming - clusters reused tasks therefore task wait another cluster started computations can begin, typically add minutes task. can advantageous reuse clusters job tasks avoid overhead short lived tasks, share resources tasks computationally demanding. example previous multi-task job however now shares single cluster task. new_cluster replaced job_cluster_key task: job_clusters now defined db_jobs_create() call accepts named list new_clusters(), reuse example earlier: can multiple shared clusters may beneficial give demanding tasks larger cluster alone (share demanding tasks).","code":"# create three simple tasks that will depend on each other # this time we will use a shared cluster to reduce startup overhead # task_a -> task_b -> task_c task_a <- job_task(   task_key = \"task_a\",   description = \"First task in the sequence\",   job_cluster_key = \"shared_cluster\",   task = multitask_task )  task_b <- job_task(   task_key = \"task_b\",   description = \"Second task in the sequence\",   job_cluster_key = \"shared_cluster\",   task = multitask_task,   depends_on = \"task_a\" )  task_c <- job_task(   task_key = \"task_c\",   description = \"Third task in the sequence\",   job_cluster_key = \"shared_cluster\",   task = multitask_task,   depends_on = \"task_b\" ) # define job_clusters as a named list of new_cluster() multitask_job_with_reuse <- db_jobs_create(   name = \"brickster example: multi-task with reuse\",   job_clusters = list(\"shared_cluster\" = multitask_cluster),   tasks = job_tasks(task_a, task_b, task_c),   # 9am every day, paused currently   schedule = cron_schedule(     quartz_cron_expression = \"0 0 9 * * ?\",     pause_status = \"PAUSED\"   ) )"},{"path":"https://databrickslabs.github.io/brickster/articles/managing-jobs.html","id":"one-off-jobs","dir":"Articles","previous_headings":"Managing Databricks Jobs > Creating Jobs","what":"One-off Jobs","title":"Job Management","text":"’s possible create one-jobs schedule appears “jobs runs” tab, “jobs”. db_jobs_runs_submit() permits use idempotency_token can avoid re-submission run.","code":"# submit a one off job run # reuse the simple task from first example # idempotency_token guarentees no additional triggers oneoff_job <- db_jobs_runs_submit(   tasks = job_tasks(simple_task),   run_name = \"brickster example: one-off job\",   idempotency_token = \"my_job_run_token\" )"},{"path":"https://databrickslabs.github.io/brickster/articles/managing-jobs.html","id":"remote-git-repos","dir":"Articles","previous_headings":"Managing Databricks Jobs > Creating Jobs","what":"Remote Git Repos","title":"Job Management","text":"Instead using notebooks contained within Databricks workspace job can referenced external git repository reference specific branch, tag, commit. Let’s revisit simple job example adjust use git_source(). example repo folder example contains simple-notebook.py. Within call db_jobs_create() use git_source() point repo containing notebook.","code":"# define a job task # this time, the notebook_path is relative to the git root directory # omit file extensions like .py or .r simple_task <- job_task(   task_key = \"simple_task\",   description = \"a simple task that runs a notebook\",   # specify a cluster for the job   new_cluster = new_cluster(     spark_version = \"16.4.x-scala2.12\",     driver_node_type_id = \"m5a.large\",     node_type_id = \"m5a.large\",     num_workers = 2,     cloud_attr = aws_attributes(ebs_volume_size = 32)   ),   # this task will be a notebook   task = notebook_task(notebook_path = \"example/simple-notebook\") ) # create job with simple task simple_task_job <- db_jobs_create(   name = \"brickster example: simple\",   tasks = job_tasks(simple_task),   # git source points to repo   git_source = git_source(     git_url = \"www.github.com/<user>/<repo>\",     git_provider = \"github\",     reference = \"main\",     type = \"branch\"   ),   # 9am every day, paused currently   schedule = cron_schedule(     quartz_cron_expression = \"0 0 9 * * ?\",     pause_status = \"PAUSED\"   ) )"},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/articles/managing-jobs.html","id":"partial-update","dir":"Articles","previous_headings":"Managing Databricks Jobs > Updating Jobs","what":"Partial Update","title":"Job Management","text":"Jobs can partially updated, example rename existing job: Adding timeout adjusting maximum concurrent runs allowed: can add new tasks may involve extra steps, either: job_task() required job defined already within R adding new tasks Specifying job_task()’s Using db_jobs_get() parsing returned metadata job_task()’s","code":"# only change the job name db_jobs_update(   job_id = multitask_job_with_reuse$job_id,   name = \"brickster example: renamed job\" ) # adding timeout and increasing max concurrent runs db_jobs_update(   job_id = multitask_job_with_reuse$job_id,   timeout_seconds = 60 * 5,   max_concurrent_runs = 2 )"},{"path":"https://databrickslabs.github.io/brickster/articles/managing-jobs.html","id":"complete-update","dir":"Articles","previous_headings":"Managing Databricks Jobs > Updating Jobs","what":"Complete Update","title":"Job Management","text":"complete overwrite job can done db_jobs_reset() - allows job_id remain constant maintaining historical run information prior settings. db_jobs_reset() db_jobs_create() except takes one additional parameter (job_id).","code":""},{"path":"https://databrickslabs.github.io/brickster/articles/managing-jobs.html","id":"managing-jobs","dir":"Articles","previous_headings":"Managing Databricks Jobs","what":"Managing Jobs","title":"Job Management","text":"db_jobs_list() db_jobs_runs_list() covered job basics.","code":""},{"path":"https://databrickslabs.github.io/brickster/articles/managing-jobs.html","id":"invocation","dir":"Articles","previous_headings":"Managing Databricks Jobs > Managing Jobs","what":"Invocation","title":"Job Management","text":"Jobs can triggered : defined schedule (see: cron_schedule()) Using db_jobs_run_now() trigger run outside regular schedule paused job. Creating one job run via db_jobs_runs_submit() example yet covered db_jobs_run_now(), accepts parameters named lists: Runs can also cancelled:","code":"# invoke simple job example triggered_run <- db_jobs_run_now(job_id = simple_task_job$job_id) # cancel run whilst it is in progress db_jobs_runs_cancel(run_id = triggered_run$run_id)"},{"path":"https://databrickslabs.github.io/brickster/articles/managing-jobs.html","id":"deletion","dir":"Articles","previous_headings":"Managing Databricks Jobs > Managing Jobs","what":"Deletion","title":"Job Management","text":"Jobs runs can deleted db_jobs_delete() db_jobs_runs_delete() respectively. Cleaning jobs created documentation:","code":"db_jobs_delete(job_id = simple_task_job$job_id) db_jobs_delete(job_id = multitask_job$job_id) db_jobs_delete(job_id = multitask_job_with_reuse$job_id)"},{"path":"https://databrickslabs.github.io/brickster/articles/remote-repl.html","id":"running-code-remotely","dir":"Articles","previous_headings":"","what":"Running Code Remotely","title":"Databricks REPL","text":"brickster provides mechanisms run code Databricks, overview available package: Databricks REPL (db_repl()) focus article.","code":""},{"path":"https://databrickslabs.github.io/brickster/articles/remote-repl.html","id":"what-is-the-databricks-repl","dir":"Articles","previous_headings":"","what":"What is the Databricks REPL?","title":"Databricks REPL","text":"REPL temporarily connects existing R console Databricks cluster (via command execution APIs) allows code supported languages sent interactively - running locally.","code":""},{"path":"https://databrickslabs.github.io/brickster/articles/remote-repl.html","id":"getting-started","dir":"Articles","previous_headings":"What is the Databricks REPL?","what":"Getting Started","title":"Databricks REPL","text":"Using REPL simple, start just provide cluster_id: REPL check clusters state start cluster inactive. default language R. successfully connecting cluster can run commands remote compute local session.","code":"# start REPL db_repl(cluster_id = \"<insert cluster id>\")"},{"path":"https://databrickslabs.github.io/brickster/articles/remote-repl.html","id":"switching-languages","dir":"Articles","previous_headings":"What is the Databricks REPL?","what":"Switching Languages","title":"Databricks REPL","text":"REPL shortcut can enter :<language> change active language. can change following languages: change languages variables persist unless REPL exited.","code":""},{"path":"https://databrickslabs.github.io/brickster/articles/remote-repl.html","id":"limitations","dir":"Articles","previous_headings":"What is the Databricks REPL?","what":"Limitations","title":"Databricks REPL","text":"Development environments (e.g. RStudio, Positron) won’t display variables remote contexts environment pane HTML content render Python, htmlwidgets rendering restricted due notebook limitations require workaround currently designed work interactive serverless compute persist recover sessions Multi-line expressions supported R. Python, Scala, SQL limited single line expressions.","code":""},{"path":"https://databrickslabs.github.io/brickster/articles/setup-auth.html","id":"defining-credentials","dir":"Articles","previous_headings":"","what":"Defining Credentials","title":"Connect to a Databricks Workspace","text":"brickster package connects Databricks workspace two ways: OAuth user--machine (U2M) authentication Personal Access Tokens (PAT) ’s recommended use option (1) using brickster interactively, need run code via automated process option currently (2). brickster automatically detect session Posit Workbench managed Databricks OAuth credentials enabled. information authentication flow see section Posit Workbench Managed Databricks OAuth Credentials. Personal Access Tokens can generated steps, step--step breakdown refer documentation. token ’ll able store alongside workspace URL .Renviron file. .Renviron used storing variables, may sensitive (e.g. credentials) de-couple code additional reading. get started add following .Renviron: DATABRICKS_HOST: workspace URL DATABRICKS_TOKEN: Personal access token (required using OAuth U2M) DATABRICKS_WSID: workspace ID (docs) DATABRICKS_WSID required RStudio IDE integration connection pane. Example entries .Renviron: Note: Recommend creating .Renviron project. can create .Renviron within user home directory required. Restarting R session allow variable picked via brickster package.","code":"DATABRICKS_HOST=xxxxxxx.cloud.databricks.com DATABRICKS_TOKEN=dapi123456789012345678a9bc01234defg5 DATABRICKS_WSID=123123123123123"},{"path":"https://databrickslabs.github.io/brickster/articles/setup-auth.html","id":"using-credentials-with-brickster","dir":"Articles","previous_headings":"","what":"Using Credentials with {brickster}","title":"Connect to a Databricks Workspace","text":"Authentication now possible without specifying credentials R code. can load brickster list clusters within workspace using db_cluster_list(), access host/token use db_host()/db_token() respectively. brickster functions host/token parameters default calling db_host()/db_token() therefore can omit explicit calls functions. using OAuth U2M authentication don’t define token .Renviron therefore db_token() return NULL.","code":"library(brickster)  # using db_host() and db_token() to get credentials clusters <- db_cluster_list(host = db_host(), token = db_token()) # all host/token parameters default to db_host()/db_token() clusters <- db_cluster_list()"},{"path":"https://databrickslabs.github.io/brickster/articles/setup-auth.html","id":"managing-multiple-credentials","dir":"Articles","previous_headings":"","what":"Managing Multiple Credentials","title":"Connect to a Databricks Workspace","text":"two methods brickster supports simplify switching credentials within R project/session: Adding multiple credentials .Renviron, additional set credentials differentiated via suffix (e.g. DATABRICKS_TOKEN_DEV) Using .databrickscfg file (primary method Databricks CLI) differentiate (1) (2) option use_databrickscfg used, following example shows switch session use .databrickscfg. default behaviour read credentials .Renviron. wish change ’s recommended set option within .Rprofile ’s set initialization R session.","code":"# will use the `DEFAULT` profile in `.databrickscfg` options(use_databrickscfg = TRUE)  # values returned should be those in profile of `.databrickscfg` db_host() db_token()"},{"path":"https://databrickslabs.github.io/brickster/articles/setup-auth.html","id":"switching-between-credentials","dir":"Articles","previous_headings":"Managing Multiple Credentials","what":"Switching Between Credentials","title":"Connect to a Databricks Workspace","text":"db_profile option controls profiles credentials returned db_host()/db_token()/db_wsid(). Profiles enable switch contexts : Different workspaces (e.g. development production) Different permissions (e.g. admin restricted user) behaviour works using credentials specified either .Renviron .databrickscfg: expected profiles .Renviron adhere naming convention default add additional suffix. example .Renviron file three profiles (default, dev, prod):","code":"# using .Renviron db_host() # returns `DB_HOST` (.Renviron)  # switch profile to 'prod' options(db_profile = \"prod\") db_host() # returns `DB_HOST_PROD` (.Renviron)  # set back to default (NULL) options(db_profile = NULL) # use .databrickcfg options(use_databrickscfg = TRUE) db_host() # returns host from `DEFAULT` profile (.databrickscfg)  options(db_profile = \"prod\") db_host() # returns host from `prod` profile in (.datarickscfg) # default DATABRICKS_HOST=xxxxxxx.cloud.databricks.com DATABRICKS_TOKEN=dapixxxxxxxxxxxxxxxxxxxxxxxxx DATABRICKS_WSID=123123123123123 # dev DATABRICKS_HOST_DEV=xxxxxxx-dev.cloud.databricks.com DATABRICKS_TOKEN_DEV=dapixxxxxxxxxxxxxxxxxxxxxxxxx DATABRICKS_WSID_DEV=123123123123124 # prod DATABRICKS_HOST_PROD=xxxxxxx-prod.cloud.databricks.com DATABRICKS_TOKEN_PROD=dapixxxxxxxxxxxxxxxxxxxxxxxxx DATABRICKS_WSID_PROD=123123123123125"},{"path":"https://databrickslabs.github.io/brickster/articles/setup-auth.html","id":"configuring--databrickscfg","dir":"Articles","previous_headings":"Managing Multiple Credentials","what":"Configuring .databrickscfg","title":"Connect to a Databricks Workspace","text":"details configuring please refer documentation Databricks CLI. one brickster specific feature inclusion wsid alongside host/token. wsid used connections pane integration RStudio underlying API’s require .","code":""},{"path":"https://databrickslabs.github.io/brickster/articles/setup-auth.html","id":"posit-workbench-managed-databricks-oauth-credentials","dir":"Articles","previous_headings":"Managing Multiple Credentials","what":"Posit Workbench Managed Databricks OAuth Credentials","title":"Connect to a Databricks Workspace","text":"Posit Workbench managed Databricks OAuth credentials feature, allows users sign Databricks workspace home page Workbench launching session access Databricks resources identity. RStudio Pro session running Posit Workbench managed Databricks OAuth credentials selected, brickster functions using db_host()/db_token() respectively just work without needing specify credentials R code. See code example. brickster automatically detect session Workbench managed OAuth credentials use workbench profile defined .databrickscfg file DATABRICKS_CONFIG_FILE specified location. Workbench generates .databrickscfg file temporary directory modified directly. use alternative .databrickscfg file, different profile, alternative env variable DATABRICKS_HOST set env variable DATABRICKS_TOKEN, launch RStudio Pro session without Databricks managed credentials box selected.","code":"library(brickster) db_cluster_list()"},{"path":"https://databrickslabs.github.io/brickster/articles/sql-backend.html","id":"motivation","dir":"Articles","previous_headings":"","what":"Motivation","title":"`{DBI}`/`{dbplyr}` backend","text":"Connecting Databricks SQL Warehouses typically involves using one methods : Heavy install can difficult install corporate devices. slow larger write operations. Slower ODBC. Requires Java. slow larger write operations. leaves void something : Native R Easy install Acceptable performance (better) Handles bulk uploads efficiently Supports OAuth U2M (tokens!) brickster now provides DBI dbplyr backends working Databricks SQL warehouses. uses Statement Execution API combination Files API (latter specifically large data uploads).","code":""},{"path":"https://databrickslabs.github.io/brickster/articles/sql-backend.html","id":"connecting-to-a-warehouse","dir":"Articles","previous_headings":"","what":"Connecting to a Warehouse","title":"`{DBI}`/`{dbplyr}` backend","text":"Connecting Databricks SQL warehouse requires creating DBI connection:","code":"library(brickster) library(DBI) library(dplyr) library(dbplyr)  # Create connection to SQL warehouse con <- dbConnect(   drv = DatabricksSQL(),   warehouse_id = \"<your_warehouse_id>\",   catalog = \"samples\" )"},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/articles/sql-backend.html","id":"reading-data","dir":"Articles","previous_headings":"{DBI} Backend","what":"Reading Data","title":"`{DBI}`/`{dbplyr}` backend","text":"Execute SQL directly return results data frames: Tables can references either via Id(), (), using name -:","code":"trips <- dbGetQuery(con, \"SELECT * FROM samples.nyctaxi.trips LIMIT 10\") # List available tables tables <- dbListTables(con)  # Check if specific table exists dbExistsTable(con, \"samples.nyctaxi.trips\") dbExistsTable(con, I(\"samples.nyctaxi.trips\")) dbExistsTable(con, Id(\"samples\", \"nyctaxi\", \"trips\"))  # Get column information dbListFields(con, \"samples.nyctaxi.trips\") dbListFields(con, I(\"samples.nyctaxi.trips\")) dbListFields(con, Id(\"samples\", \"nyctaxi\", \"trips\"))"},{"path":"https://databrickslabs.github.io/brickster/articles/sql-backend.html","id":"writing-data","dir":"Articles","previous_headings":"{DBI} Backend","what":"Writing Data","title":"`{DBI}`/`{dbplyr}` backend","text":"writing data (append, overwrite, etc) two possible behaviours: -line SQL statement write Stage .parquet files Volume directory COPY CTAS data larger 50k rows brickster permit use method (2), requires staging_volume specified establishing connection, directly dbWriteQuery. Ensure staging_volume valid Volume path permission write files.","code":"# small data (150 rows) # generates an in-line CTAS dbWriteTable(   conn = con,   name = Id(catalog = \"<catalog>\", schema = \"<schema>\", table = \"<table>\"),   value = iris,   overwrite = TRUE )  # bigger data (4 million rows) # writes parquet files to volume then CTAS iris_big <- sample_n(iris, replace = TRUE, size = 4000000)  dbWriteTable(   conn = con,   name = Id(catalog = \"<catalog>\", schema = \"<schema>\", table = \"<table>\"),   value = iris_big,   overwrite = TRUE,   staging_volume = \"/Volumes/<catalog>/<schema>/<volume>/...\" # or inherited from connection   progress = TRUE )"},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/articles/sql-backend.html","id":"reading-data-1","dir":"Articles","previous_headings":"{dbplyr} Backend","what":"Reading Data","title":"`{DBI}`/`{dbplyr}` backend","text":"DBI backend tables can referenced either via Id(), (), using name -tbl(): Chain dplyr operations - execute remotely Databricks: general reminder, call collect() latest point possible analysis take reduce required computation locally.","code":"# Connect to existing tables tbl(con, \"samples.nyctaxi.trips\") tbl(con, I(\"samples.nyctaxi.trips\")) tbl(con, Id(\"samples\", \"nyctaxi\", \"trips\")) tbl(con, in_catalog(\"samples\", \"nyctaxi\", \"trips\")) # Filter and select (translated to SQL) long_trips <- tbl(con, \"samples.nyctaxi.trips\") |>   filter(trip_distance > 10) |>   select(     tpep_pickup_datetime,     tpep_dropoff_datetime,     trip_distance,     fare_amount   )  # View the generated SQL (without executing) show_query(long_trips)  # Execute and collect results long_trips |> collect() # Customer summary statistics trips_summary <- tbl(con, \"samples.nyctaxi.trips\") |>   group_by(pickup_zip) %>%   summarise(     trip_count = n(),     total_fare_amount = sum(fare_amount, na.rm = TRUE),     total_trip_distance = sum(trip_distance, na.rm = TRUE),     avg_fare_amount = mean(fare_amount, na.rm = TRUE)   ) |>   arrange(desc(avg_fare_amount))  # Execute to get the 20 most expensive pickip zip codes with more than 30 trips top_zipz <- trips_summary |>   filter(trip_count > 20) |>   head(20) |>   collect()"},{"path":"https://databrickslabs.github.io/brickster/articles/sql-backend.html","id":"writing-data-1","dir":"Articles","previous_headings":"","what":"Writing Data","title":"`{DBI}`/`{dbplyr}` backend","text":"key difference temporary tables supported - makes functions like copy_to usable specifying temporary FALSE use dbWriteTable create table.","code":"iris_remote <- copy_to(con, iris, \"iris_table\", temporary = FALSE, overwrite = TRUE)"},{"path":"https://databrickslabs.github.io/brickster/articles/sql-backend.html","id":"connection-management","dir":"Articles","previous_headings":"","what":"Connection Management","title":"`{DBI}`/`{dbplyr}` backend","text":"connection DatabricksSQL driver different DBI backends doesn’t persistent session (’s just API calls). means calling dbDisconnect serves purpose comes freeing resources SQL warehouse.","code":""},{"path":"https://databrickslabs.github.io/brickster/articles/sql-backend.html","id":"webr-support","dir":"Articles","previous_headings":"","what":"{WebR} Support","title":"`{DBI}`/`{dbplyr}` backend","text":"{bricksters} core dependencies {WebR} compatible. backend uses nanoarrow fallback arrow unavailable (currently arrow Suggests). ’s recommended always arrow installed improve performance data loading.","code":""},{"path":"https://databrickslabs.github.io/brickster/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Zac Davies. Author, maintainer. Rafi Kurlansik. Author. . Copyright holder, funder.","code":""},{"path":"https://databrickslabs.github.io/brickster/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Davies Z, Kurlansik R (2025). brickster: R Toolkit 'Databricks'. R package version 0.2.9, https://github.com/databrickslabs/brickster.","code":"@Manual{,   title = {brickster: R Toolkit for 'Databricks'},   author = {Zac Davies and Rafi Kurlansik},   year = {2025},   note = {R package version 0.2.9},   url = {https://github.com/databrickslabs/brickster}, }"},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/index.html","id":"overview","dir":"","previous_headings":"","what":"Overview","title":"R Toolkit for Databricks","text":"brickster R toolkit Databricks, includes: Wrappers Databricks API’s (e.g. db_cluster_list, db_volume_read) Browser workspace assets via RStudio Connections Pane (open_workspace()) DBI + dbplyr backend (ODBC installs!) Interactive Databricks REPL","code":""},{"path":"https://databrickslabs.github.io/brickster/index.html","id":"quick-start","dir":"","previous_headings":"","what":"Quick Start","title":"R Toolkit for Databricks","text":"Refer “Connect Databricks Workspace” article details getting authentication configured.","code":"library(brickster)  # only requires `DATABRICKS_HOST` if using OAuth U2M # first request will open browser window to login Sys.setenv(DATABRICKS_HOST = \"https://<workspace-prefix>.cloud.databricks.com\")  # open RStudio/Positron connection pane to view Databricks resources open_workspace()  # list all SQL warehouses warehouses <- db_sql_warehouse_list()"},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/index.html","id":"dbi-backend","dir":"","previous_headings":"Usage","what":"{DBI} Backend","title":"R Toolkit for Databricks","text":"","code":"library(brickster) library(DBI)  # Connect to Databricks using DBI (assumes you followed quickstart to authenticate) con <- dbConnect(   DatabricksSQL(),   warehouse_id = \"<warehouse-id>\" )  # Standard {DBI} operations tables <- dbListTables(con) dbGetQuery(con, \"SELECT * FROM samples.nyctaxi.trips LIMIT 5\")  # Use with {dbplyr} for {dplyr} syntax library(dplyr) library(dbplyr)  nyc_taxi <- tbl(con, I(\"samples.nyctaxi.trips\"))  result <- nyc_taxi |>   filter(year(tpep_pickup_datetime) == 2016) |>   group_by(pickup_zip) |>   summarise(     trip_count = n(),     avg_fare = mean(fare_amount, na.rm = TRUE),     avg_distance = mean(trip_distance, na.rm = TRUE)   ) |>   collect()"},{"path":"https://databrickslabs.github.io/brickster/index.html","id":"download--upload-to-volume","dir":"","previous_headings":"Usage","what":"Download & Upload to Volume","title":"R Toolkit for Databricks","text":"","code":"library(readr) library(brickster)  # upload `data.csv` to a volume local_file <- tempfile(fileext = \".csv\") write_csv(x = iris, file = local_file) db_volume_write(   path = \"/Volumes/<catalog>/<schema>/<volume>/data.csv\",   file = local_file )  # read `data.csv` from a volume and write to a file downloaded_file <- tempfile(fileext = \".csv\") file <- db_volume_read(   path = \"/Volumes/<catalog>/<schema>/<volume>/data.csv\",   destination = downloaded_file ) volume_csv <- read_csv(downloaded_file)"},{"path":"https://databrickslabs.github.io/brickster/index.html","id":"databricks-repl","dir":"","previous_headings":"Usage","what":"Databricks REPL","title":"R Toolkit for Databricks","text":"Run commands existing interactive Databricks cluster, read article details.","code":"library(brickster)  # commands after this will run on the interactive cluster # read the vignette for more details db_repl(cluster_id = \"<interactive_cluster_id>\")"},{"path":"https://databrickslabs.github.io/brickster/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"R Toolkit for Databricks","text":"","code":"install.packages(\"brickster\")"},{"path":"https://databrickslabs.github.io/brickster/index.html","id":"development-version","dir":"","previous_headings":"Installation","what":"Development Version","title":"R Toolkit for Databricks","text":"","code":"# install.packages(\"pak\") pak::pak(\"databrickslabs/brickster\")"},{"path":"https://databrickslabs.github.io/brickster/index.html","id":"api-coverage","dir":"","previous_headings":"","what":"API Coverage","title":"R Toolkit for Databricks","text":"brickster deliberate choosing API’s wrapped. brickster isn’t intended replace IaC tooling (e.g. Terraform) used account/workspace administration.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/DatabricksConnection-class.html","id":null,"dir":"Reference","previous_headings":"","what":"DBI Connection for Databricks — DatabricksConnection-class","title":"DBI Connection for Databricks — DatabricksConnection-class","text":"DBI Connection Databricks","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/DatabricksDriver-class.html","id":null,"dir":"Reference","previous_headings":"","what":"DBI Driver for Databricks — DatabricksDriver-class","title":"DBI Driver for Databricks — DatabricksDriver-class","text":"DBI Driver Databricks","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/DatabricksResult-class.html","id":null,"dir":"Reference","previous_headings":"","what":"DBI Result for Databricks — DatabricksResult-class","title":"DBI Result for Databricks — DatabricksResult-class","text":"DBI Result Databricks","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/DatabricksSQL.html","id":null,"dir":"Reference","previous_headings":"","what":"Create Databricks SQL Driver — DatabricksSQL","title":"Create Databricks SQL Driver — DatabricksSQL","text":"Create Databricks SQL Driver","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/DatabricksSQL.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create Databricks SQL Driver — DatabricksSQL","text":"","code":"DatabricksSQL()"},{"path":"https://databrickslabs.github.io/brickster/reference/DatabricksSQL.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create Databricks SQL Driver — DatabricksSQL","text":"DatabricksDriver object","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/DatabricksSQL.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create Databricks SQL Driver — DatabricksSQL","text":"","code":"if (FALSE) { # \\dontrun{ drv <- DatabricksSQL() con <- dbConnect(drv, warehouse_id = \"your_warehouse_id\") } # }"},{"path":"https://databrickslabs.github.io/brickster/reference/access_control_req_group.html","id":null,"dir":"Reference","previous_headings":"","what":"Access Control Request for Group — access_control_req_group","title":"Access Control Request for Group — access_control_req_group","text":"Access Control Request Group","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/access_control_req_group.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Access Control Request for Group — access_control_req_group","text":"","code":"access_control_req_group(   group,   permission_level = c(\"CAN_MANAGE\", \"CAN_MANAGE_RUN\", \"CAN_VIEW\") )"},{"path":"https://databrickslabs.github.io/brickster/reference/access_control_req_group.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Access Control Request for Group — access_control_req_group","text":"group Group name. two built-groups: users users, admins administrators. permission_level Permission level grant. One CAN_MANAGE, CAN_MANAGE_RUN, CAN_VIEW.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/access_control_req_user.html","id":null,"dir":"Reference","previous_headings":"","what":"Access Control Request For User — access_control_req_user","title":"Access Control Request For User — access_control_req_user","text":"Access Control Request User","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/access_control_req_user.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Access Control Request For User — access_control_req_user","text":"","code":"access_control_req_user(   user_name,   permission_level = c(\"CAN_MANAGE\", \"CAN_MANAGE_RUN\", \"CAN_VIEW\", \"IS_OWNER\") )"},{"path":"https://databrickslabs.github.io/brickster/reference/access_control_req_user.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Access Control Request For User — access_control_req_user","text":"user_name Email address user. permission_level Permission level grant. One CAN_MANAGE, CAN_MANAGE_RUN, CAN_VIEW, IS_OWNER.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/access_control_request.html","id":null,"dir":"Reference","previous_headings":"","what":"Access Control Request — access_control_request","title":"Access Control Request — access_control_request","text":"Access Control Request","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/access_control_request.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Access Control Request — access_control_request","text":"","code":"access_control_request(...)"},{"path":"https://databrickslabs.github.io/brickster/reference/access_control_request.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Access Control Request — access_control_request","text":"... Instances access_control_req_user() access_control_req_group().","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/add_lib_path.html","id":null,"dir":"Reference","previous_headings":"","what":"Add Library Path — add_lib_path","title":"Add Library Path — add_lib_path","text":"Add Library Path","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/add_lib_path.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Add Library Path — add_lib_path","text":"","code":"add_lib_path(path, after, version = FALSE)"},{"path":"https://databrickslabs.github.io/brickster/reference/add_lib_path.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Add Library Path — add_lib_path","text":"path Directory added location packages searched. Recursively creates directory exist. Databricks remember use /dbfs/ /Volumes/... prefix. Location append path value . version TRUE add R version string end path. recommended using different R versions sharing common path users.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/add_lib_path.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Add Library Path — add_lib_path","text":"functions primary use using Databricks notebooks hosted RStudio, however, works anywhere.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/aws_attributes.html","id":null,"dir":"Reference","previous_headings":"","what":"AWS Attributes — aws_attributes","title":"AWS Attributes — aws_attributes","text":"AWS Attributes","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/aws_attributes.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"AWS Attributes — aws_attributes","text":"","code":"aws_attributes(   first_on_demand = 1,   availability = c(\"SPOT_WITH_FALLBACK\", \"SPOT\", \"ON_DEMAND\"),   zone_id = NULL,   instance_profile_arn = NULL,   spot_bid_price_percent = 100,   ebs_volume_type = c(\"GENERAL_PURPOSE_SSD\", \"THROUGHPUT_OPTIMIZED_HDD\"),   ebs_volume_count = 1,   ebs_volume_size = NULL,   ebs_volume_iops = NULL,   ebs_volume_throughput = NULL )"},{"path":"https://databrickslabs.github.io/brickster/reference/aws_attributes.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"AWS Attributes — aws_attributes","text":"first_on_demand Number nodes cluster placed -demand instances. value greater 0, cluster driver node placed -demand instance. value greater equal current cluster size, nodes placed -demand instances. value less current cluster size, first_on_demand nodes placed -demand instances remainder placed availability instances. value affect cluster size mutated lifetime cluster. availability One SPOT_WITH_FALLBACK, SPOT, ON_DEMAND. Type used subsequent nodes past first_on_demand ones. first_on_demand zero, availability type used entire cluster. zone_id Identifier availability zone/datacenter cluster resides. three options: availability zone region Databricks deployment, auto selects based available IPs, NULL use default availability zone. instance_profile_arn Nodes cluster placed AWS instances instance profile. omitted, nodes placed instances without instance profile. instance profile must previously added Databricks environment account administrator. feature may available certain customer plans. spot_bid_price_percent max price AWS spot instances, percentage corresponding instance type’s -demand price. example, field set 50, cluster needs new i3.xlarge spot instance, max price half price -demand i3.xlarge instances. Similarly, field set 200, max price twice price -demand i3.xlarge instances. specified, default value 100. spot instances requested cluster, spot instances whose max price percentage matches field considered. safety, enforce field 10000. ebs_volume_type Either GENERAL_PURPOSE_SSD THROUGHPUT_OPTIMIZED_HDD ebs_volume_count number volumes launched instance. can choose 10 volumes. feature enabled supported node types. Legacy node types specify custom EBS volumes. node types instance store, least one EBS volume needs specified; otherwise, cluster creation fail. EBS volumes mounted /ebs0, /ebs1, etc. Instance store volumes mounted /local_disk0, /local_disk1, etc. EBS volumes attached, Databricks configure Spark use EBS volumes scratch storage heterogeneously sized scratch devices can lead inefficient disk utilization. EBS volumes attached, Databricks configure Spark use instance store volumes. EBS volumes specified, Spark configuration spark.local.dir overridden. ebs_volume_size size EBS volume (GiB) launched instance. general purpose SSD, value must within range 100 - 4096. throughput optimized HDD, value must within range 500 - 4096. Custom EBS volumes specified legacy node types (memory-optimized compute-optimized). ebs_volume_iops number IOPS per EBS gp3 volume. value must 3000 16000. value IOPS throughput calculated based AWS documentation match maximum performance gp2 volume volume size. ebs_volume_throughput throughput per EBS gp3 volume, MiB per second. value must 125 1000.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/aws_attributes.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"AWS Attributes — aws_attributes","text":"ebs_volume_iops, ebs_volume_throughput, specified, values inferred throughput IOPS gp2 volume disk size, using following calculation:","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/azure_attributes.html","id":null,"dir":"Reference","previous_headings":"","what":"Azure Attributes — azure_attributes","title":"Azure Attributes — azure_attributes","text":"Azure Attributes","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/azure_attributes.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Azure Attributes — azure_attributes","text":"","code":"azure_attributes(   first_on_demand = 1,   availability = c(\"SPOT_WITH_FALLBACK\", \"SPOT\", \"ON_DEMAND\"),   spot_bid_max_price = -1 )"},{"path":"https://databrickslabs.github.io/brickster/reference/azure_attributes.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Azure Attributes — azure_attributes","text":"first_on_demand Number nodes cluster placed -demand instances. value greater 0, cluster driver node placed -demand instance. value greater equal current cluster size, nodes placed -demand instances. value less current cluster size, first_on_demand nodes placed -demand instances remainder placed availability instances. value affect cluster size mutated lifetime cluster. availability One SPOT_WITH_FALLBACK, SPOT, ON_DEMAND. Type used subsequent nodes past first_on_demand ones. first_on_demand zero, availability type used entire cluster. spot_bid_max_price max bid price used Azure spot instances. can set greater equal current spot price. can also set -1 (default), specifies instance evicted basis price. price instance current price spot instances price standard instance. can view historical pricing eviction rates Azure portal.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/close_workspace.html","id":null,"dir":"Reference","previous_headings":"","what":"Close Databricks Workspace Connection — close_workspace","title":"Close Databricks Workspace Connection — close_workspace","text":"Close Databricks Workspace Connection","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/close_workspace.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Close Databricks Workspace Connection — close_workspace","text":"","code":"close_workspace(host = db_host())"},{"path":"https://databrickslabs.github.io/brickster/reference/close_workspace.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Close Databricks Workspace Connection — close_workspace","text":"host Databricks workspace URL, defaults calling db_host().","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/close_workspace.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Close Databricks Workspace Connection — close_workspace","text":"","code":"if (FALSE) { # \\dontrun{ close_workspace(host = db_host()) } # }"},{"path":"https://databrickslabs.github.io/brickster/reference/cluster_autoscale.html","id":null,"dir":"Reference","previous_headings":"","what":"Cluster Autoscale — cluster_autoscale","title":"Cluster Autoscale — cluster_autoscale","text":"Range defining min max number cluster workers.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/cluster_autoscale.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Cluster Autoscale — cluster_autoscale","text":"","code":"cluster_autoscale(min_workers, max_workers)"},{"path":"https://databrickslabs.github.io/brickster/reference/cluster_autoscale.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Cluster Autoscale — cluster_autoscale","text":"min_workers minimum number workers cluster can scale underutilized. also initial number workers cluster creation. max_workers maximum number workers cluster can scale overloaded. max_workers must strictly greater min_workers.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/cluster_log_conf.html","id":null,"dir":"Reference","previous_headings":"","what":"Cluster Log Configuration — cluster_log_conf","title":"Cluster Log Configuration — cluster_log_conf","text":"Path cluster log.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/cluster_log_conf.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Cluster Log Configuration — cluster_log_conf","text":"","code":"cluster_log_conf(dbfs = NULL, s3 = NULL)"},{"path":"https://databrickslabs.github.io/brickster/reference/cluster_log_conf.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Cluster Log Configuration — cluster_log_conf","text":"dbfs Instance dbfs_storage_info(). s3 Instance s3_storage_info().","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/cluster_log_conf.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Cluster Log Configuration — cluster_log_conf","text":"dbfs s3 mutually exclusive, logs can sent one destination.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/condition_task.html","id":null,"dir":"Reference","previous_headings":"","what":"Condition Task — condition_task","title":"Condition Task — condition_task","text":"Condition Task","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/condition_task.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Condition Task — condition_task","text":"","code":"condition_task(   left,   right,   op = c(\"EQUAL_TO\", \"GREATER_THAN\", \"GREATER_THAN_OR_EQUAL\", \"LESS_THAN\",     \"LESS_THAN_OR_EQUAL\", \"NOT_EQUAL\") )"},{"path":"https://databrickslabs.github.io/brickster/reference/condition_task.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Condition Task — condition_task","text":"left Left operand condition task. Either string value job state parameter reference. right Right operand condition task. Either string value job state parameter reference. op Operator, one \"EQUAL_TO\", \"GREATER_THAN\", \"GREATER_THAN_OR_EQUAL\", \"LESS_THAN\", \"LESS_THAN_OR_EQUAL\", \"NOT_EQUAL\"","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/condition_task.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Condition Task — condition_task","text":"task evaluates condition can used control execution tasks condition_task field present. condition task require cluster execute support retries notifications.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/copy_to.DatabricksConnection.html","id":null,"dir":"Reference","previous_headings":"","what":"Copy data frame to Databricks as table or view — copy_to.DatabricksConnection","title":"Copy data frame to Databricks as table or view — copy_to.DatabricksConnection","text":"Copy data frame Databricks table view","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/copy_to.DatabricksConnection.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Copy data frame to Databricks as table or view — copy_to.DatabricksConnection","text":"","code":"# S3 method for class 'DatabricksConnection' copy_to(   dest,   df,   name = deparse(substitute(df)),   overwrite = FALSE,   temporary = TRUE,   ... )"},{"path":"https://databrickslabs.github.io/brickster/reference/copy_to.DatabricksConnection.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Copy data frame to Databricks as table or view — copy_to.DatabricksConnection","text":"dest DatabricksConnection object df Data frame copy name Name table/view overwrite Whether overwrite existing table/view temporary Whether create temporary view (default: TRUE, SUPPORTED - error) ... Additional arguments passed dbWriteTable","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/copy_to.DatabricksConnection.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Copy data frame to Databricks as table or view — copy_to.DatabricksConnection","text":"dbplyr table reference","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/copy_to.DatabricksConnection.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Copy data frame to Databricks as table or view — copy_to.DatabricksConnection","text":"Note: temporary=TRUE result error temporary tables supported SQL Statement Execution API. Use temporary=FALSE create regular tables.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/cron_schedule.html","id":null,"dir":"Reference","previous_headings":"","what":"Cron Schedule — cron_schedule","title":"Cron Schedule — cron_schedule","text":"Cron Schedule","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/cron_schedule.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Cron Schedule — cron_schedule","text":"","code":"cron_schedule(   quartz_cron_expression,   timezone_id = \"Etc/UTC\",   pause_status = c(\"UNPAUSED\", \"PAUSED\") )"},{"path":"https://databrickslabs.github.io/brickster/reference/cron_schedule.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Cron Schedule — cron_schedule","text":"quartz_cron_expression Cron expression using Quartz syntax describes schedule job. See Cron Trigger details. timezone_id Java timezone ID. schedule job resolved respect timezone. See Java TimeZone details. pause_status Indicate whether schedule paused . Either UNPAUSED (default) PAUSED.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/databricks-dbi.html","id":null,"dir":"Reference","previous_headings":"","what":"DBI Interface for Databricks SQL Warehouses — databricks-dbi","title":"DBI Interface for Databricks SQL Warehouses — databricks-dbi","text":"file implements standard DBI interface Databricks SQL warehouses, built top existing db_sql_query() infrastructure.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/databricks-dbplyr.html","id":null,"dir":"Reference","previous_headings":"","what":"dbplyr Backend for Databricks SQL — databricks-dbplyr","title":"dbplyr Backend for Databricks SQL — databricks-dbplyr","text":"file implements dbplyr backend support Databricks SQL warehouses, enabling dplyr syntax translated Databricks SQL.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/dbAppendTable-DatabricksConnection-Id-data.frame-method.html","id":null,"dir":"Reference","previous_headings":"","what":"Append rows to an existing Databricks table (Id method) — dbAppendTable,DatabricksConnection,Id,data.frame-method","title":"Append rows to an existing Databricks table (Id method) — dbAppendTable,DatabricksConnection,Id,data.frame-method","text":"Append rows existing Databricks table (Id method)","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/dbAppendTable-DatabricksConnection-Id-data.frame-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Append rows to an existing Databricks table (Id method) — dbAppendTable,DatabricksConnection,Id,data.frame-method","text":"","code":"# S4 method for class 'DatabricksConnection,Id,data.frame' dbAppendTable(conn, name, value, ..., row.names = FALSE)"},{"path":"https://databrickslabs.github.io/brickster/reference/dbAppendTable-DatabricksConnection-Id-data.frame-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Append rows to an existing Databricks table (Id method) — dbAppendTable,DatabricksConnection,Id,data.frame-method","text":"conn DatabricksConnection object name Table name Id object value Data frame append ... Additional arguments row.names TRUE, preserve row names column","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/dbAppendTable-DatabricksConnection-Id-data.frame-method.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Append rows to an existing Databricks table (Id method) — dbAppendTable,DatabricksConnection,Id,data.frame-method","text":"TRUE invisibly success","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/dbAppendTable-DatabricksConnection-character-data.frame-method.html","id":null,"dir":"Reference","previous_headings":"","what":"Append rows to an existing Databricks table — dbAppendTable,DatabricksConnection,character,data.frame-method","title":"Append rows to an existing Databricks table — dbAppendTable,DatabricksConnection,character,data.frame-method","text":"Append rows existing Databricks table","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/dbAppendTable-DatabricksConnection-character-data.frame-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Append rows to an existing Databricks table — dbAppendTable,DatabricksConnection,character,data.frame-method","text":"","code":"# S4 method for class 'DatabricksConnection,character,data.frame' dbAppendTable(conn, name, value, ..., row.names = FALSE)"},{"path":"https://databrickslabs.github.io/brickster/reference/dbAppendTable-DatabricksConnection-character-data.frame-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Append rows to an existing Databricks table — dbAppendTable,DatabricksConnection,character,data.frame-method","text":"conn DatabricksConnection object name Table name (character, Id, SQL) value Data frame append ... Additional arguments row.names TRUE, preserve row names column","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/dbAppendTable-DatabricksConnection-character-data.frame-method.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Append rows to an existing Databricks table — dbAppendTable,DatabricksConnection,character,data.frame-method","text":"TRUE invisibly success","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/dbBegin-DatabricksConnection-method.html","id":null,"dir":"Reference","previous_headings":"","what":"Begin transaction (not supported) — dbBegin,DatabricksConnection-method","title":"Begin transaction (not supported) — dbBegin,DatabricksConnection-method","text":"Begin transaction (supported)","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/dbBegin-DatabricksConnection-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Begin transaction (not supported) — dbBegin,DatabricksConnection-method","text":"","code":"# S4 method for class 'DatabricksConnection' dbBegin(conn, ...)"},{"path":"https://databrickslabs.github.io/brickster/reference/dbBegin-DatabricksConnection-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Begin transaction (not supported) — dbBegin,DatabricksConnection-method","text":"conn DatabricksConnection object ... Additional arguments (ignored)","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/dbBegin-DatabricksConnection-method.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Begin transaction (not supported) — dbBegin,DatabricksConnection-method","text":"Always throws error (transactions supported)","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/dbClearResult-DatabricksResult-method.html","id":null,"dir":"Reference","previous_headings":"","what":"Clear result set — dbClearResult,DatabricksResult-method","title":"Clear result set — dbClearResult,DatabricksResult-method","text":"Clear result set","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/dbClearResult-DatabricksResult-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Clear result set — dbClearResult,DatabricksResult-method","text":"","code":"# S4 method for class 'DatabricksResult' dbClearResult(res, ...)"},{"path":"https://databrickslabs.github.io/brickster/reference/dbClearResult-DatabricksResult-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Clear result set — dbClearResult,DatabricksResult-method","text":"res DatabricksResult object ... Additional arguments (ignored)","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/dbClearResult-DatabricksResult-method.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Clear result set — dbClearResult,DatabricksResult-method","text":"TRUE (invisibly)","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/dbColumnInfo-DatabricksResult-method.html","id":null,"dir":"Reference","previous_headings":"","what":"Get column information from result — dbColumnInfo,DatabricksResult-method","title":"Get column information from result — dbColumnInfo,DatabricksResult-method","text":"Get column information result","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/dbColumnInfo-DatabricksResult-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get column information from result — dbColumnInfo,DatabricksResult-method","text":"","code":"# S4 method for class 'DatabricksResult' dbColumnInfo(res, ...)"},{"path":"https://databrickslabs.github.io/brickster/reference/dbColumnInfo-DatabricksResult-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get column information from result — dbColumnInfo,DatabricksResult-method","text":"res DatabricksResult object ... Additional arguments (ignored)","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/dbColumnInfo-DatabricksResult-method.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get column information from result — dbColumnInfo,DatabricksResult-method","text":"data.frame column names types","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/dbCommit-DatabricksConnection-method.html","id":null,"dir":"Reference","previous_headings":"","what":"Commit transaction (not supported) — dbCommit,DatabricksConnection-method","title":"Commit transaction (not supported) — dbCommit,DatabricksConnection-method","text":"Commit transaction (supported)","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/dbCommit-DatabricksConnection-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Commit transaction (not supported) — dbCommit,DatabricksConnection-method","text":"","code":"# S4 method for class 'DatabricksConnection' dbCommit(conn, ...)"},{"path":"https://databrickslabs.github.io/brickster/reference/dbCommit-DatabricksConnection-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Commit transaction (not supported) — dbCommit,DatabricksConnection-method","text":"conn DatabricksConnection object ... Additional arguments (ignored)","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/dbCommit-DatabricksConnection-method.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Commit transaction (not supported) — dbCommit,DatabricksConnection-method","text":"Always throws error (transactions supported)","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/dbConnect-DatabricksDriver-method.html","id":null,"dir":"Reference","previous_headings":"","what":"Connect to Databricks SQL Warehouse — dbConnect,DatabricksDriver-method","title":"Connect to Databricks SQL Warehouse — dbConnect,DatabricksDriver-method","text":"Connect Databricks SQL Warehouse","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/dbConnect-DatabricksDriver-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Connect to Databricks SQL Warehouse — dbConnect,DatabricksDriver-method","text":"","code":"# S4 method for class 'DatabricksDriver' dbConnect(   drv,   warehouse_id,   catalog = NULL,   schema = NULL,   staging_volume = NULL,   token = db_token(),   host = db_host(),   ... )"},{"path":"https://databrickslabs.github.io/brickster/reference/dbConnect-DatabricksDriver-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Connect to Databricks SQL Warehouse — dbConnect,DatabricksDriver-method","text":"drv DatabricksDriver object warehouse_id ID SQL warehouse connect catalog Optional catalog name use default schema Optional schema name use default staging_volume Optional volume path large dataset staging token Authentication token (defaults db_token()) host Databricks workspace host (defaults db_host()) ... Additional arguments (ignored)","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/dbConnect-DatabricksDriver-method.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Connect to Databricks SQL Warehouse — dbConnect,DatabricksDriver-method","text":"DatabricksConnection object","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/dbDataType-DatabricksConnection-method.html","id":null,"dir":"Reference","previous_headings":"","what":"Map R data types to Databricks SQL types — dbDataType,DatabricksConnection-method","title":"Map R data types to Databricks SQL types — dbDataType,DatabricksConnection-method","text":"Map R data types Databricks SQL types","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/dbDataType-DatabricksConnection-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Map R data types to Databricks SQL types — dbDataType,DatabricksConnection-method","text":"","code":"# S4 method for class 'DatabricksConnection' dbDataType(dbObj, obj, ...)"},{"path":"https://databrickslabs.github.io/brickster/reference/dbDataType-DatabricksConnection-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Map R data types to Databricks SQL types — dbDataType,DatabricksConnection-method","text":"dbObj DatabricksConnection object obj R object(s) get SQL types ... Additional arguments (ignored)","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/dbDataType-DatabricksConnection-method.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Map R data types to Databricks SQL types — dbDataType,DatabricksConnection-method","text":"Character vector SQL type names","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/dbDisconnect-DatabricksConnection-method.html","id":null,"dir":"Reference","previous_headings":"","what":"Disconnect from Databricks — dbDisconnect,DatabricksConnection-method","title":"Disconnect from Databricks — dbDisconnect,DatabricksConnection-method","text":"Disconnect Databricks","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/dbDisconnect-DatabricksConnection-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Disconnect from Databricks — dbDisconnect,DatabricksConnection-method","text":"","code":"# S4 method for class 'DatabricksConnection' dbDisconnect(conn, ...)"},{"path":"https://databrickslabs.github.io/brickster/reference/dbDisconnect-DatabricksConnection-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Disconnect from Databricks — dbDisconnect,DatabricksConnection-method","text":"conn DatabricksConnection object ... Additional arguments (ignored)","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/dbDisconnect-DatabricksConnection-method.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Disconnect from Databricks — dbDisconnect,DatabricksConnection-method","text":"TRUE (invisibly)","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/dbExecute-DatabricksConnection-character-method.html","id":null,"dir":"Reference","previous_headings":"","what":"Execute statement on Databricks — dbExecute,DatabricksConnection,character-method","title":"Execute statement on Databricks — dbExecute,DatabricksConnection,character-method","text":"Execute statement Databricks","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/dbExecute-DatabricksConnection-character-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Execute statement on Databricks — dbExecute,DatabricksConnection,character-method","text":"","code":"# S4 method for class 'DatabricksConnection,character' dbExecute(conn, statement, ...)"},{"path":"https://databrickslabs.github.io/brickster/reference/dbExecute-DatabricksConnection-character-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Execute statement on Databricks — dbExecute,DatabricksConnection,character-method","text":"conn DatabricksConnection object statement SQL statement ... Additional arguments (ignored)","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/dbExecute-DatabricksConnection-character-method.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Execute statement on Databricks — dbExecute,DatabricksConnection,character-method","text":"Number rows result set (metadata, without loading data)","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/dbExistsTable-DatabricksConnection-AsIs-method.html","id":null,"dir":"Reference","previous_headings":"","what":"Check if table exists (AsIs method) — dbExistsTable,DatabricksConnection,AsIs-method","title":"Check if table exists (AsIs method) — dbExistsTable,DatabricksConnection,AsIs-method","text":"Check table exists (AsIs method)","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/dbExistsTable-DatabricksConnection-AsIs-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check if table exists (AsIs method) — dbExistsTable,DatabricksConnection,AsIs-method","text":"","code":"# S4 method for class 'DatabricksConnection,AsIs' dbExistsTable(conn, name, ...)"},{"path":"https://databrickslabs.github.io/brickster/reference/dbExistsTable-DatabricksConnection-AsIs-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check if table exists (AsIs method) — dbExistsTable,DatabricksConnection,AsIs-method","text":"conn DatabricksConnection object name Table name AsIs object (()) ... Additional arguments (ignored)","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/dbExistsTable-DatabricksConnection-AsIs-method.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check if table exists (AsIs method) — dbExistsTable,DatabricksConnection,AsIs-method","text":"TRUE table exists, FALSE otherwise","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/dbExistsTable-DatabricksConnection-Id-method.html","id":null,"dir":"Reference","previous_headings":"","what":"Check if table exists (Id method) — dbExistsTable,DatabricksConnection,Id-method","title":"Check if table exists (Id method) — dbExistsTable,DatabricksConnection,Id-method","text":"Check table exists (Id method)","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/dbExistsTable-DatabricksConnection-Id-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check if table exists (Id method) — dbExistsTable,DatabricksConnection,Id-method","text":"","code":"# S4 method for class 'DatabricksConnection,Id' dbExistsTable(conn, name, ...)"},{"path":"https://databrickslabs.github.io/brickster/reference/dbExistsTable-DatabricksConnection-Id-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check if table exists (Id method) — dbExistsTable,DatabricksConnection,Id-method","text":"conn DatabricksConnection object name Table name Id object ... Additional arguments (ignored)","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/dbExistsTable-DatabricksConnection-Id-method.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check if table exists (Id method) — dbExistsTable,DatabricksConnection,Id-method","text":"TRUE table exists, FALSE otherwise","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/dbExistsTable-DatabricksConnection-character-method.html","id":null,"dir":"Reference","previous_headings":"","what":"Check if table exists in Databricks — dbExistsTable,DatabricksConnection,character-method","title":"Check if table exists in Databricks — dbExistsTable,DatabricksConnection,character-method","text":"Check table exists Databricks","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/dbExistsTable-DatabricksConnection-character-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check if table exists in Databricks — dbExistsTable,DatabricksConnection,character-method","text":"","code":"# S4 method for class 'DatabricksConnection,character' dbExistsTable(conn, name, ...)"},{"path":"https://databrickslabs.github.io/brickster/reference/dbExistsTable-DatabricksConnection-character-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check if table exists in Databricks — dbExistsTable,DatabricksConnection,character-method","text":"conn DatabricksConnection object name Table name check ... Additional arguments (ignored)","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/dbExistsTable-DatabricksConnection-character-method.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check if table exists in Databricks — dbExistsTable,DatabricksConnection,character-method","text":"TRUE table exists, FALSE otherwise","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/dbFetch-DatabricksResult-method.html","id":null,"dir":"Reference","previous_headings":"","what":"Fetch results from Databricks query — dbFetch,DatabricksResult-method","title":"Fetch results from Databricks query — dbFetch,DatabricksResult-method","text":"Fetch results Databricks query","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/dbFetch-DatabricksResult-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fetch results from Databricks query — dbFetch,DatabricksResult-method","text":"","code":"# S4 method for class 'DatabricksResult' dbFetch(res, n = -1, ...)"},{"path":"https://databrickslabs.github.io/brickster/reference/dbFetch-DatabricksResult-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fetch results from Databricks query — dbFetch,DatabricksResult-method","text":"res DatabricksResult object n Maximum number rows fetch (-1 rows) ... Additional arguments (ignored)","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/dbFetch-DatabricksResult-method.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fetch results from Databricks query — dbFetch,DatabricksResult-method","text":"data.frame query results","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/dbGetInfo-DatabricksConnection-method.html","id":null,"dir":"Reference","previous_headings":"","what":"Get connection information — dbGetInfo,DatabricksConnection-method","title":"Get connection information — dbGetInfo,DatabricksConnection-method","text":"Get connection information","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/dbGetInfo-DatabricksConnection-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get connection information — dbGetInfo,DatabricksConnection-method","text":"","code":"# S4 method for class 'DatabricksConnection' dbGetInfo(dbObj, ...)"},{"path":"https://databrickslabs.github.io/brickster/reference/dbGetInfo-DatabricksConnection-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get connection information — dbGetInfo,DatabricksConnection-method","text":"dbObj DatabricksConnection object ... Additional arguments (ignored)","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/dbGetInfo-DatabricksConnection-method.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get connection information — dbGetInfo,DatabricksConnection-method","text":"list connection details","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/dbGetQuery-DatabricksConnection-character-method.html","id":null,"dir":"Reference","previous_headings":"","what":"Execute SQL query and return results — dbGetQuery,DatabricksConnection,character-method","title":"Execute SQL query and return results — dbGetQuery,DatabricksConnection,character-method","text":"Execute SQL query return results","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/dbGetQuery-DatabricksConnection-character-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Execute SQL query and return results — dbGetQuery,DatabricksConnection,character-method","text":"","code":"# S4 method for class 'DatabricksConnection,character' dbGetQuery(   conn,   statement,   disposition = \"EXTERNAL_LINKS\",   show_progress = TRUE,   ... )"},{"path":"https://databrickslabs.github.io/brickster/reference/dbGetQuery-DatabricksConnection-character-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Execute SQL query and return results — dbGetQuery,DatabricksConnection,character-method","text":"conn DatabricksConnection object statement SQL statement execute disposition Query disposition mode: \"EXTERNAL_LINKS\" (default) large results, \"INLINE\" small metadata queries (automatically chooses appropriate format) show_progress TRUE, show progress updates query execution (default: TRUE) ... Additional arguments passed underlying query execution","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/dbGetQuery-DatabricksConnection-character-method.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Execute SQL query and return results — dbGetQuery,DatabricksConnection,character-method","text":"data.frame query results","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/dbGetRowCount-DatabricksResult-method.html","id":null,"dir":"Reference","previous_headings":"","what":"Get number of rows fetched — dbGetRowCount,DatabricksResult-method","title":"Get number of rows fetched — dbGetRowCount,DatabricksResult-method","text":"Get number rows fetched","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/dbGetRowCount-DatabricksResult-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get number of rows fetched — dbGetRowCount,DatabricksResult-method","text":"","code":"# S4 method for class 'DatabricksResult' dbGetRowCount(res, ...)"},{"path":"https://databrickslabs.github.io/brickster/reference/dbGetRowCount-DatabricksResult-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get number of rows fetched — dbGetRowCount,DatabricksResult-method","text":"res DatabricksResult object ... Additional arguments (ignored)","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/dbGetRowCount-DatabricksResult-method.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get number of rows fetched — dbGetRowCount,DatabricksResult-method","text":"Number rows fetched far","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/dbGetRowsAffected-DatabricksResult-method.html","id":null,"dir":"Reference","previous_headings":"","what":"Get number of rows affected (not applicable for SELECT) — dbGetRowsAffected,DatabricksResult-method","title":"Get number of rows affected (not applicable for SELECT) — dbGetRowsAffected,DatabricksResult-method","text":"Get number rows affected (applicable SELECT)","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/dbGetRowsAffected-DatabricksResult-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get number of rows affected (not applicable for SELECT) — dbGetRowsAffected,DatabricksResult-method","text":"","code":"# S4 method for class 'DatabricksResult' dbGetRowsAffected(res, ...)"},{"path":"https://databrickslabs.github.io/brickster/reference/dbGetRowsAffected-DatabricksResult-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get number of rows affected (not applicable for SELECT) — dbGetRowsAffected,DatabricksResult-method","text":"res DatabricksResult object ... Additional arguments (ignored)","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/dbGetRowsAffected-DatabricksResult-method.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get number of rows affected (not applicable for SELECT) — dbGetRowsAffected,DatabricksResult-method","text":"-1 (applicable SELECT queries)","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/dbGetStatement-DatabricksResult-method.html","id":null,"dir":"Reference","previous_headings":"","what":"Get SQL statement from result — dbGetStatement,DatabricksResult-method","title":"Get SQL statement from result — dbGetStatement,DatabricksResult-method","text":"Get SQL statement result","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/dbGetStatement-DatabricksResult-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get SQL statement from result — dbGetStatement,DatabricksResult-method","text":"","code":"# S4 method for class 'DatabricksResult' dbGetStatement(res, ...)"},{"path":"https://databrickslabs.github.io/brickster/reference/dbGetStatement-DatabricksResult-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get SQL statement from result — dbGetStatement,DatabricksResult-method","text":"res DatabricksResult object ... Additional arguments (ignored)","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/dbGetStatement-DatabricksResult-method.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get SQL statement from result — dbGetStatement,DatabricksResult-method","text":"SQL statement character","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/dbHasCompleted-DatabricksResult-method.html","id":null,"dir":"Reference","previous_headings":"","what":"Check if query has completed — dbHasCompleted,DatabricksResult-method","title":"Check if query has completed — dbHasCompleted,DatabricksResult-method","text":"Check query completed","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/dbHasCompleted-DatabricksResult-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check if query has completed — dbHasCompleted,DatabricksResult-method","text":"","code":"# S4 method for class 'DatabricksResult' dbHasCompleted(res, ...)"},{"path":"https://databrickslabs.github.io/brickster/reference/dbHasCompleted-DatabricksResult-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check if query has completed — dbHasCompleted,DatabricksResult-method","text":"res DatabricksResult object ... Additional arguments (ignored)","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/dbHasCompleted-DatabricksResult-method.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check if query has completed — dbHasCompleted,DatabricksResult-method","text":"TRUE query complete, FALSE otherwise","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/dbIsValid-DatabricksConnection-method.html","id":null,"dir":"Reference","previous_headings":"","what":"Check if connection is valid — dbIsValid,DatabricksConnection-method","title":"Check if connection is valid — dbIsValid,DatabricksConnection-method","text":"Check connection valid","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/dbIsValid-DatabricksConnection-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check if connection is valid — dbIsValid,DatabricksConnection-method","text":"","code":"# S4 method for class 'DatabricksConnection' dbIsValid(dbObj, ...)"},{"path":"https://databrickslabs.github.io/brickster/reference/dbIsValid-DatabricksConnection-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check if connection is valid — dbIsValid,DatabricksConnection-method","text":"dbObj DatabricksConnection object ... Additional arguments (ignored)","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/dbIsValid-DatabricksConnection-method.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check if connection is valid — dbIsValid,DatabricksConnection-method","text":"TRUE connection valid, FALSE otherwise","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/dbListFields-DatabricksConnection-AsIs-method.html","id":null,"dir":"Reference","previous_headings":"","what":"List column names of a Databricks table (AsIs method) — dbListFields,DatabricksConnection,AsIs-method","title":"List column names of a Databricks table (AsIs method) — dbListFields,DatabricksConnection,AsIs-method","text":"List column names Databricks table (AsIs method)","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/dbListFields-DatabricksConnection-AsIs-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List column names of a Databricks table (AsIs method) — dbListFields,DatabricksConnection,AsIs-method","text":"","code":"# S4 method for class 'DatabricksConnection,AsIs' dbListFields(conn, name, ...)"},{"path":"https://databrickslabs.github.io/brickster/reference/dbListFields-DatabricksConnection-AsIs-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List column names of a Databricks table (AsIs method) — dbListFields,DatabricksConnection,AsIs-method","text":"conn DatabricksConnection object name Table name AsIs object (()) ... Additional arguments (ignored)","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/dbListFields-DatabricksConnection-AsIs-method.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"List column names of a Databricks table (AsIs method) — dbListFields,DatabricksConnection,AsIs-method","text":"Character vector column names","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/dbListFields-DatabricksConnection-character-method.html","id":null,"dir":"Reference","previous_headings":"","what":"List column names of a Databricks table — dbListFields,DatabricksConnection,character-method","title":"List column names of a Databricks table — dbListFields,DatabricksConnection,character-method","text":"List column names Databricks table","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/dbListFields-DatabricksConnection-character-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List column names of a Databricks table — dbListFields,DatabricksConnection,character-method","text":"","code":"# S4 method for class 'DatabricksConnection,character' dbListFields(conn, name, ...)"},{"path":"https://databrickslabs.github.io/brickster/reference/dbListFields-DatabricksConnection-character-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List column names of a Databricks table — dbListFields,DatabricksConnection,character-method","text":"conn DatabricksConnection object name Table name describe ... Additional arguments (ignored)","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/dbListFields-DatabricksConnection-character-method.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"List column names of a Databricks table — dbListFields,DatabricksConnection,character-method","text":"Character vector column names","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/dbListTables-DatabricksConnection-method.html","id":null,"dir":"Reference","previous_headings":"","what":"List tables in Databricks catalog/schema — dbListTables,DatabricksConnection-method","title":"List tables in Databricks catalog/schema — dbListTables,DatabricksConnection-method","text":"List tables Databricks catalog/schema","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/dbListTables-DatabricksConnection-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List tables in Databricks catalog/schema — dbListTables,DatabricksConnection-method","text":"","code":"# S4 method for class 'DatabricksConnection' dbListTables(conn, ...)"},{"path":"https://databrickslabs.github.io/brickster/reference/dbListTables-DatabricksConnection-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List tables in Databricks catalog/schema — dbListTables,DatabricksConnection-method","text":"conn DatabricksConnection object ... Additional arguments (ignored)","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/dbListTables-DatabricksConnection-method.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"List tables in Databricks catalog/schema — dbListTables,DatabricksConnection-method","text":"Character vector table names","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/dbQuoteIdentifier-DatabricksConnection-Id-method.html","id":null,"dir":"Reference","previous_headings":"","what":"Quote complex identifiers (schema.table) — dbQuoteIdentifier,DatabricksConnection,Id-method","title":"Quote complex identifiers (schema.table) — dbQuoteIdentifier,DatabricksConnection,Id-method","text":"Quote complex identifiers (schema.table)","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/dbQuoteIdentifier-DatabricksConnection-Id-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Quote complex identifiers (schema.table) — dbQuoteIdentifier,DatabricksConnection,Id-method","text":"","code":"# S4 method for class 'DatabricksConnection,Id' dbQuoteIdentifier(conn, x, ...)"},{"path":"https://databrickslabs.github.io/brickster/reference/dbQuoteIdentifier-DatabricksConnection-Id-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Quote complex identifiers (schema.table) — dbQuoteIdentifier,DatabricksConnection,Id-method","text":"conn DatabricksConnection object x Id object catalog/schema/table components ... Additional arguments (ignored)","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/dbQuoteIdentifier-DatabricksConnection-Id-method.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Quote complex identifiers (schema.table) — dbQuoteIdentifier,DatabricksConnection,Id-method","text":"SQL object quoted identifier components","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/dbQuoteIdentifier-DatabricksConnection-SQL-method.html","id":null,"dir":"Reference","previous_headings":"","what":"Quote SQL objects (passthrough) — dbQuoteIdentifier,DatabricksConnection,SQL-method","title":"Quote SQL objects (passthrough) — dbQuoteIdentifier,DatabricksConnection,SQL-method","text":"Quote SQL objects (passthrough)","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/dbQuoteIdentifier-DatabricksConnection-SQL-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Quote SQL objects (passthrough) — dbQuoteIdentifier,DatabricksConnection,SQL-method","text":"","code":"# S4 method for class 'DatabricksConnection,SQL' dbQuoteIdentifier(conn, x, ...)"},{"path":"https://databrickslabs.github.io/brickster/reference/dbQuoteIdentifier-DatabricksConnection-SQL-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Quote SQL objects (passthrough) — dbQuoteIdentifier,DatabricksConnection,SQL-method","text":"conn DatabricksConnection object x SQL object (already quoted) ... Additional arguments (ignored)","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/dbQuoteIdentifier-DatabricksConnection-SQL-method.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Quote SQL objects (passthrough) — dbQuoteIdentifier,DatabricksConnection,SQL-method","text":"SQL object unchanged","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/dbQuoteIdentifier-DatabricksConnection-character-method.html","id":null,"dir":"Reference","previous_headings":"","what":"Quote identifiers for Databricks SQL — dbQuoteIdentifier,DatabricksConnection,character-method","title":"Quote identifiers for Databricks SQL — dbQuoteIdentifier,DatabricksConnection,character-method","text":"Quote identifiers Databricks SQL","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/dbQuoteIdentifier-DatabricksConnection-character-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Quote identifiers for Databricks SQL — dbQuoteIdentifier,DatabricksConnection,character-method","text":"","code":"# S4 method for class 'DatabricksConnection,character' dbQuoteIdentifier(conn, x, ...)"},{"path":"https://databrickslabs.github.io/brickster/reference/dbQuoteIdentifier-DatabricksConnection-character-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Quote identifiers for Databricks SQL — dbQuoteIdentifier,DatabricksConnection,character-method","text":"conn DatabricksConnection object x Character vector identifiers quote ... Additional arguments (ignored)","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/dbQuoteIdentifier-DatabricksConnection-character-method.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Quote identifiers for Databricks SQL — dbQuoteIdentifier,DatabricksConnection,character-method","text":"SQL object quoted identifiers","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/dbRollback-DatabricksConnection-method.html","id":null,"dir":"Reference","previous_headings":"","what":"Rollback transaction (not supported) — dbRollback,DatabricksConnection-method","title":"Rollback transaction (not supported) — dbRollback,DatabricksConnection-method","text":"Rollback transaction (supported)","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/dbRollback-DatabricksConnection-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Rollback transaction (not supported) — dbRollback,DatabricksConnection-method","text":"","code":"# S4 method for class 'DatabricksConnection' dbRollback(conn, ...)"},{"path":"https://databrickslabs.github.io/brickster/reference/dbRollback-DatabricksConnection-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Rollback transaction (not supported) — dbRollback,DatabricksConnection-method","text":"conn DatabricksConnection object ... Additional arguments (ignored)","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/dbRollback-DatabricksConnection-method.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Rollback transaction (not supported) — dbRollback,DatabricksConnection-method","text":"Always throws error (transactions supported)","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/dbSendQuery-DatabricksConnection-character-method.html","id":null,"dir":"Reference","previous_headings":"","what":"Send query to Databricks (asynchronous) — dbSendQuery,DatabricksConnection,character-method","title":"Send query to Databricks (asynchronous) — dbSendQuery,DatabricksConnection,character-method","text":"Send query Databricks (asynchronous)","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/dbSendQuery-DatabricksConnection-character-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Send query to Databricks (asynchronous) — dbSendQuery,DatabricksConnection,character-method","text":"","code":"# S4 method for class 'DatabricksConnection,character' dbSendQuery(conn, statement, ...)"},{"path":"https://databrickslabs.github.io/brickster/reference/dbSendQuery-DatabricksConnection-character-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Send query to Databricks (asynchronous) — dbSendQuery,DatabricksConnection,character-method","text":"conn DatabricksConnection object statement SQL statement execute ... Additional arguments (ignored)","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/dbSendQuery-DatabricksConnection-character-method.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Send query to Databricks (asynchronous) — dbSendQuery,DatabricksConnection,character-method","text":"DatabricksResult object","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/dbSendStatement-DatabricksConnection-character-method.html","id":null,"dir":"Reference","previous_headings":"","what":"Send statement to Databricks — dbSendStatement,DatabricksConnection,character-method","title":"Send statement to Databricks — dbSendStatement,DatabricksConnection,character-method","text":"Send statement Databricks","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/dbSendStatement-DatabricksConnection-character-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Send statement to Databricks — dbSendStatement,DatabricksConnection,character-method","text":"","code":"# S4 method for class 'DatabricksConnection,character' dbSendStatement(conn, statement, ...)"},{"path":"https://databrickslabs.github.io/brickster/reference/dbSendStatement-DatabricksConnection-character-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Send statement to Databricks — dbSendStatement,DatabricksConnection,character-method","text":"conn DatabricksConnection object statement SQL statement ... Additional arguments (ignored)","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/dbSendStatement-DatabricksConnection-character-method.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Send statement to Databricks — dbSendStatement,DatabricksConnection,character-method","text":"DatabricksResult object","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/dbWriteTable-DatabricksConnection-AsIs-data.frame-method.html","id":null,"dir":"Reference","previous_headings":"","what":"Write table to Databricks (AsIs name signature) — dbWriteTable,DatabricksConnection,AsIs,data.frame-method","title":"Write table to Databricks (AsIs name signature) — dbWriteTable,DatabricksConnection,AsIs,data.frame-method","text":"Write table Databricks (AsIs name signature)","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/dbWriteTable-DatabricksConnection-AsIs-data.frame-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Write table to Databricks (AsIs name signature) — dbWriteTable,DatabricksConnection,AsIs,data.frame-method","text":"","code":"# S4 method for class 'DatabricksConnection,AsIs,data.frame' dbWriteTable(   conn,   name,   value,   overwrite = FALSE,   append = FALSE,   row.names = FALSE,   temporary = FALSE,   field.types = NULL,   staging_volume = NULL,   progress = TRUE,   ... )"},{"path":"https://databrickslabs.github.io/brickster/reference/dbWriteTable-DatabricksConnection-AsIs-data.frame-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Write table to Databricks (AsIs name signature) — dbWriteTable,DatabricksConnection,AsIs,data.frame-method","text":"conn DatabricksConnection object name Table name AsIs object (()) value Data frame write overwrite TRUE, overwrite existing table append TRUE, append existing table row.names TRUE, preserve row names column temporary TRUE, create temporary table (SUPPORTED - error) field.types Named character vector SQL types columns staging_volume Optional volume path large dataset staging progress TRUE, show progress bar file uploads (default: TRUE) ... Additional arguments","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/dbWriteTable-DatabricksConnection-AsIs-data.frame-method.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Write table to Databricks (AsIs name signature) — dbWriteTable,DatabricksConnection,AsIs,data.frame-method","text":"TRUE invisibly success","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/dbWriteTable-DatabricksConnection-Id-data.frame-method.html","id":null,"dir":"Reference","previous_headings":"","what":"Write a data frame to Databricks table (Id method) — dbWriteTable,DatabricksConnection,Id,data.frame-method","title":"Write a data frame to Databricks table (Id method) — dbWriteTable,DatabricksConnection,Id,data.frame-method","text":"Write data frame Databricks table (Id method)","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/dbWriteTable-DatabricksConnection-Id-data.frame-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Write a data frame to Databricks table (Id method) — dbWriteTable,DatabricksConnection,Id,data.frame-method","text":"","code":"# S4 method for class 'DatabricksConnection,Id,data.frame' dbWriteTable(   conn,   name,   value,   overwrite = FALSE,   append = FALSE,   row.names = FALSE,   temporary = FALSE,   field.types = NULL,   staging_volume = NULL,   progress = TRUE,   ... )"},{"path":"https://databrickslabs.github.io/brickster/reference/dbWriteTable-DatabricksConnection-Id-data.frame-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Write a data frame to Databricks table (Id method) — dbWriteTable,DatabricksConnection,Id,data.frame-method","text":"conn DatabricksConnection object name Table name Id object value Data frame write overwrite TRUE, overwrite existing table append TRUE, append existing table row.names TRUE, preserve row names column temporary TRUE, create temporary table (SUPPORTED - error) field.types Named character vector SQL types columns staging_volume Optional volume path large dataset staging progress TRUE, show progress bar file uploads (default: TRUE) ... Additional arguments","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/dbWriteTable-DatabricksConnection-Id-data.frame-method.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Write a data frame to Databricks table (Id method) — dbWriteTable,DatabricksConnection,Id,data.frame-method","text":"TRUE invisibly success","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/dbWriteTable-DatabricksConnection-character-data.frame-method.html","id":null,"dir":"Reference","previous_headings":"","what":"Write a data frame to Databricks table — dbWriteTable,DatabricksConnection,character,data.frame-method","title":"Write a data frame to Databricks table — dbWriteTable,DatabricksConnection,character,data.frame-method","text":"Write data frame Databricks table","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/dbWriteTable-DatabricksConnection-character-data.frame-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Write a data frame to Databricks table — dbWriteTable,DatabricksConnection,character,data.frame-method","text":"","code":"# S4 method for class 'DatabricksConnection,character,data.frame' dbWriteTable(   conn,   name,   value,   overwrite = FALSE,   append = FALSE,   row.names = FALSE,   temporary = FALSE,   field.types = NULL,   staging_volume = NULL,   progress = TRUE,   ... )"},{"path":"https://databrickslabs.github.io/brickster/reference/dbWriteTable-DatabricksConnection-character-data.frame-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Write a data frame to Databricks table — dbWriteTable,DatabricksConnection,character,data.frame-method","text":"conn DatabricksConnection object name Table name (character, Id, SQL) value Data frame write overwrite TRUE, overwrite existing table append TRUE, append existing table row.names TRUE, preserve row names column temporary TRUE, create temporary table (SUPPORTED - error) field.types Named character vector SQL types columns staging_volume Optional volume path large dataset staging progress TRUE, show progress bar file uploads (default: TRUE) ... Additional arguments","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/dbWriteTable-DatabricksConnection-character-data.frame-method.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Write a data frame to Databricks table — dbWriteTable,DatabricksConnection,character,data.frame-method","text":"TRUE invisibly success","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_append_with_select_values.html","id":null,"dir":"Reference","previous_headings":"","what":"Append data using atomic INSERT INTO with SELECT VALUES — db_append_with_select_values","title":"Append data using atomic INSERT INTO with SELECT VALUES — db_append_with_select_values","text":"Append data using atomic INSERT SELECT VALUES","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_append_with_select_values.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Append data using atomic INSERT INTO with SELECT VALUES — db_append_with_select_values","text":"","code":"db_append_with_select_values(conn, quoted_name, value)"},{"path":"https://databrickslabs.github.io/brickster/reference/db_cluster_action.html","id":null,"dir":"Reference","previous_headings":"","what":"Cluster Action Helper Function — db_cluster_action","title":"Cluster Action Helper Function — db_cluster_action","text":"Cluster Action Helper Function","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_cluster_action.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Cluster Action Helper Function — db_cluster_action","text":"","code":"db_cluster_action(   cluster_id,   action = c(\"start\", \"restart\", \"delete\", \"permanent-delete\", \"pin\", \"unpin\"),   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_cluster_action.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Cluster Action Helper Function — db_cluster_action","text":"cluster_id Canonical identifier cluster. action One start, restart, delete, permanent-delete, pin, unpin. host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_cluster_create.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a Cluster — db_cluster_create","title":"Create a Cluster — db_cluster_create","text":"Create Cluster","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_cluster_create.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a Cluster — db_cluster_create","text":"","code":"db_cluster_create(   name,   spark_version,   node_type_id,   num_workers = NULL,   autoscale = NULL,   spark_conf = list(),   cloud_attrs = aws_attributes(),   driver_node_type_id = NULL,   custom_tags = list(),   init_scripts = list(),   spark_env_vars = list(),   autotermination_minutes = 120,   log_conf = NULL,   ssh_public_keys = NULL,   driver_instance_pool_id = NULL,   instance_pool_id = NULL,   idempotency_token = NULL,   enable_elastic_disk = TRUE,   apply_policy_default_values = TRUE,   enable_local_disk_encryption = TRUE,   docker_image = NULL,   policy_id = NULL,   kind = c(\"CLASSIC_PREVIEW\"),   data_security_mode = c(\"NONE\", \"SINGLE_USER\", \"USER_ISOLATION\", \"LEGACY_TABLE_ACL\",     \"LEGACY_PASSTHROUGH\", \"LEGACY_SINGLE_USER\", \"LEGACY_SINGLE_USER_STANDARD\",     \"DATA_SECURITY_MODE_STANDARD\", \"DATA_SECURITY_MODE_DEDICATED\",     \"DATA_SECURITY_MODE_AUTO\"),   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_cluster_create.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a Cluster — db_cluster_create","text":"name Cluster name requested user. doesn’t unique. specified creation, cluster name empty string. spark_version runtime version cluster. can retrieve list available runtime versions using db_cluster_runtime_versions(). node_type_id node type worker nodes. db_cluster_list_node_types() can used see available node types. num_workers Number worker nodes cluster . cluster one Spark driver num_workers executors total num_workers + 1 Spark nodes. autoscale Instance cluster_autoscale(). spark_conf Named list. object containing set optional, user-specified Spark configuration key-value pairs. can also pass string extra JVM options driver executors via spark.driver.extraJavaOptions spark.executor.extraJavaOptions respectively. E.g. list(\"spark.speculation\" = true, \"spark.streaming.ui.retainedBatches\" = 5). cloud_attrs Attributes related clusters running specific cloud provider. Defaults aws_attributes(). Must one aws_attributes(), azure_attributes(), gcp_attributes(). driver_node_type_id node type Spark driver. field optional; unset, driver node type set value node_type_id defined . db_cluster_list_node_types() can used see available node types. custom_tags Named list. object containing set tags cluster resources. Databricks tags cluster resources tags addition default_tags. Databricks allows 45 custom tags. init_scripts Instance init_script_info(). spark_env_vars Named list. User-specified environment variable key-value pairs. order specify additional set SPARK_DAEMON_JAVA_OPTS, recommend appending $SPARK_DAEMON_JAVA_OPTS shown following example. ensures default Databricks managed environmental variables included well. E.g. {\"SPARK_DAEMON_JAVA_OPTS\": \"$SPARK_DAEMON_JAVA_OPTS -Dspark.shuffle.service.enabled=true\"} autotermination_minutes Automatically terminates cluster inactive time minutes. set, cluster automatically terminated. specified, threshold must 10 10000 minutes. can also set value 0 explicitly disable automatic termination. Defaults 120. log_conf Instance cluster_log_conf(). ssh_public_keys List. SSH public key contents added Spark node cluster. corresponding private keys can used login user name ubuntu port 2200. 10 keys can specified. driver_instance_pool_id ID instance pool use driver node. must also specify instance_pool_id. Optional. instance_pool_id ID instance pool use cluster nodes. driver_instance_pool_id present, instance_pool_id used worker nodes . Otherwise, used driver worker nodes. Optional. idempotency_token optional token can used guarantee idempotency cluster creation requests. active cluster provided token already exists, request create new cluster, return ID existing cluster instead. existence cluster token checked terminated clusters. specify idempotency token, upon failure can retry request succeeds. Databricks guarantees exactly one cluster launched idempotency token. token 64 characters. enable_elastic_disk enabled, cluster dynamically acquire additional disk space Spark workers running low disk space. apply_policy_default_values Boolean (Default: TRUE), whether use policy default values missing cluster attributes. enable_local_disk_encryption Boolean (Default: TRUE), whether encryption disks locally attached cluster enabled. docker_image Instance docker_image(). policy_id String, ID cluster policy. kind kind compute described compute specification. data_security_mode Data security mode decides data governance model use accessing data cluster. host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_cluster_create.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create a Cluster — db_cluster_create","text":"Create new Apache Spark cluster. method acquires new instances cloud provider necessary. method asynchronous; returned cluster_id can used poll cluster state (db_cluster_get()). method returns, cluster PENDING state. cluster usable enters RUNNING state. Databricks may able acquire requested nodes, due cloud provider limitations transient network issues. Databricks acquires least 85% requested -demand nodes, cluster creation succeed. Otherwise cluster terminate informative error message. specify autoscale num_workers, must choose one. Documentation.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_cluster_delete.html","id":null,"dir":"Reference","previous_headings":"","what":"Delete/Terminate a Cluster — db_cluster_delete","title":"Delete/Terminate a Cluster — db_cluster_delete","text":"Delete/Terminate Cluster","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_cluster_delete.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Delete/Terminate a Cluster — db_cluster_delete","text":"","code":"db_cluster_delete(   cluster_id,   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_cluster_delete.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Delete/Terminate a Cluster — db_cluster_delete","text":"cluster_id Canonical identifier cluster. host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_cluster_delete.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Delete/Terminate a Cluster — db_cluster_delete","text":"cluster must RUNNING state.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_cluster_edit.html","id":null,"dir":"Reference","previous_headings":"","what":"Edit a Cluster — db_cluster_edit","title":"Edit a Cluster — db_cluster_edit","text":"Edit configuration cluster match provided attributes size.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_cluster_edit.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Edit a Cluster — db_cluster_edit","text":"","code":"db_cluster_edit(   cluster_id,   spark_version,   node_type_id,   num_workers = NULL,   autoscale = NULL,   name = NULL,   spark_conf = NULL,   cloud_attrs = NULL,   driver_node_type_id = NULL,   custom_tags = NULL,   init_scripts = NULL,   spark_env_vars = NULL,   autotermination_minutes = NULL,   log_conf = NULL,   ssh_public_keys = NULL,   driver_instance_pool_id = NULL,   instance_pool_id = NULL,   idempotency_token = NULL,   enable_elastic_disk = NULL,   apply_policy_default_values = NULL,   enable_local_disk_encryption = NULL,   docker_image = NULL,   policy_id = NULL,   kind = c(\"CLASSIC_PREVIEW\"),   data_security_mode = c(\"NONE\", \"SINGLE_USER\", \"USER_ISOLATION\", \"LEGACY_TABLE_ACL\",     \"LEGACY_PASSTHROUGH\", \"LEGACY_SINGLE_USER\", \"LEGACY_SINGLE_USER_STANDARD\",     \"DATA_SECURITY_MODE_STANDARD\", \"DATA_SECURITY_MODE_DEDICATED\",     \"DATA_SECURITY_MODE_AUTO\"),   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_cluster_edit.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Edit a Cluster — db_cluster_edit","text":"cluster_id Canonical identifier cluster. spark_version runtime version cluster. can retrieve list available runtime versions using db_cluster_runtime_versions(). node_type_id node type worker nodes. db_cluster_list_node_types() can used see available node types. num_workers Number worker nodes cluster . cluster one Spark driver num_workers executors total num_workers + 1 Spark nodes. autoscale Instance cluster_autoscale(). name Cluster name requested user. doesn’t unique. specified creation, cluster name empty string. spark_conf Named list. object containing set optional, user-specified Spark configuration key-value pairs. can also pass string extra JVM options driver executors via spark.driver.extraJavaOptions spark.executor.extraJavaOptions respectively. E.g. list(\"spark.speculation\" = true, \"spark.streaming.ui.retainedBatches\" = 5). cloud_attrs Attributes related clusters running specific cloud provider. Defaults aws_attributes(). Must one aws_attributes(), azure_attributes(), gcp_attributes(). driver_node_type_id node type Spark driver. field optional; unset, driver node type set value node_type_id defined . db_cluster_list_node_types() can used see available node types. custom_tags Named list. object containing set tags cluster resources. Databricks tags cluster resources tags addition default_tags. Databricks allows 45 custom tags. init_scripts Instance init_script_info(). spark_env_vars Named list. User-specified environment variable key-value pairs. order specify additional set SPARK_DAEMON_JAVA_OPTS, recommend appending $SPARK_DAEMON_JAVA_OPTS shown following example. ensures default Databricks managed environmental variables included well. E.g. {\"SPARK_DAEMON_JAVA_OPTS\": \"$SPARK_DAEMON_JAVA_OPTS -Dspark.shuffle.service.enabled=true\"} autotermination_minutes Automatically terminates cluster inactive time minutes. set, cluster automatically terminated. specified, threshold must 10 10000 minutes. can also set value 0 explicitly disable automatic termination. Defaults 120. log_conf Instance cluster_log_conf(). ssh_public_keys List. SSH public key contents added Spark node cluster. corresponding private keys can used login user name ubuntu port 2200. 10 keys can specified. driver_instance_pool_id ID instance pool use driver node. must also specify instance_pool_id. Optional. instance_pool_id ID instance pool use cluster nodes. driver_instance_pool_id present, instance_pool_id used worker nodes . Otherwise, used driver worker nodes. Optional. idempotency_token optional token can used guarantee idempotency cluster creation requests. active cluster provided token already exists, request create new cluster, return ID existing cluster instead. existence cluster token checked terminated clusters. specify idempotency token, upon failure can retry request succeeds. Databricks guarantees exactly one cluster launched idempotency token. token 64 characters. enable_elastic_disk enabled, cluster dynamically acquire additional disk space Spark workers running low disk space. apply_policy_default_values Boolean (Default: TRUE), whether use policy default values missing cluster attributes. enable_local_disk_encryption Boolean (Default: TRUE), whether encryption disks locally attached cluster enabled. docker_image Instance docker_image(). policy_id String, ID cluster policy. kind kind compute described compute specification. data_security_mode Data security mode decides data governance model use accessing data cluster. host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_cluster_edit.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Edit a Cluster — db_cluster_edit","text":"can edit cluster RUNNING TERMINATED state. edit cluster RUNNING state, restarted new attributes can take effect. edit cluster TERMINATED state, remain TERMINATED. next time started using clusters/start API, new attributes take effect. attempt edit cluster state rejected INVALID_STATE error code. Clusters created Databricks Jobs service edited.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_cluster_events.html","id":null,"dir":"Reference","previous_headings":"","what":"List Cluster Activity Events — db_cluster_events","title":"List Cluster Activity Events — db_cluster_events","text":"List Cluster Activity Events","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_cluster_events.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List Cluster Activity Events — db_cluster_events","text":"","code":"db_cluster_events(   cluster_id,   start_time = NULL,   end_time = NULL,   event_types = NULL,   order = c(\"DESC\", \"ASC\"),   offset = 0,   limit = 50,   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_cluster_events.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List Cluster Activity Events — db_cluster_events","text":"cluster_id ID cluster retrieve events . start_time start time epoch milliseconds. empty, returns events starting beginning time. end_time end time epoch milliseconds. empty, returns events current time. event_types List. Optional set event types filter . Default return events. Event Types. order Either DESC (default) ASC. offset offset result set. Defaults 0 (offset). offset specified results requested descending order, end_time field required. limit Maximum number events include page events. Defaults 50, maximum allowed value 500. host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_cluster_events.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"List Cluster Activity Events — db_cluster_events","text":"Retrieve list events activity cluster. can retrieve events active clusters (running, pending, reconfiguring) terminated clusters within 30 days last termination. API paginated. events read, response includes parameters necessary request next page events.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_cluster_get.html","id":null,"dir":"Reference","previous_headings":"","what":"Get Details of a Cluster — db_cluster_get","title":"Get Details of a Cluster — db_cluster_get","text":"Get Details Cluster","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_cluster_get.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get Details of a Cluster — db_cluster_get","text":"","code":"db_cluster_get(   cluster_id,   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_cluster_get.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get Details of a Cluster — db_cluster_get","text":"cluster_id Canonical identifier cluster. host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_cluster_get.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get Details of a Cluster — db_cluster_get","text":"Retrieve information cluster given identifier. Clusters can described running 30 days terminated.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_cluster_list.html","id":null,"dir":"Reference","previous_headings":"","what":"List Clusters — db_cluster_list","title":"List Clusters — db_cluster_list","text":"List Clusters","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_cluster_list.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List Clusters — db_cluster_list","text":"","code":"db_cluster_list(host = db_host(), token = db_token(), perform_request = TRUE)"},{"path":"https://databrickslabs.github.io/brickster/reference/db_cluster_list.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List Clusters — db_cluster_list","text":"host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_cluster_list.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"List Clusters — db_cluster_list","text":"Return information pinned clusters, active clusters, 150 recently terminated -purpose clusters past 30 days, 30 recently terminated job clusters past 30 days. example, 1 pinned cluster, 4 active clusters, 45 terminated -purpose clusters past 30 days, 50 terminated job clusters past 30 days, API returns: 1 pinned cluster 4 active clusters 45 terminated -purpose clusters 30 recently terminated job clusters","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_cluster_list_node_types.html","id":null,"dir":"Reference","previous_headings":"","what":"List Available Cluster Node Types — db_cluster_list_node_types","title":"List Available Cluster Node Types — db_cluster_list_node_types","text":"List Available Cluster Node Types","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_cluster_list_node_types.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List Available Cluster Node Types — db_cluster_list_node_types","text":"","code":"db_cluster_list_node_types(   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_cluster_list_node_types.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List Available Cluster Node Types — db_cluster_list_node_types","text":"host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_cluster_list_node_types.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"List Available Cluster Node Types — db_cluster_list_node_types","text":"Return list supported Spark node types. node types can used launch cluster.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_cluster_list_zones.html","id":null,"dir":"Reference","previous_headings":"","what":"List Availability Zones (AWS Only) — db_cluster_list_zones","title":"List Availability Zones (AWS Only) — db_cluster_list_zones","text":"List Availability Zones (AWS )","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_cluster_list_zones.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List Availability Zones (AWS Only) — db_cluster_list_zones","text":"","code":"db_cluster_list_zones(   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_cluster_list_zones.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List Availability Zones (AWS Only) — db_cluster_list_zones","text":"host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_cluster_list_zones.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"List Availability Zones (AWS Only) — db_cluster_list_zones","text":"Amazon Web Services (AWS) ! Return list availability zones clusters can created (ex: us-west-2a). zones can used launch cluster.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_cluster_perm_delete.html","id":null,"dir":"Reference","previous_headings":"","what":"Permanently Delete a Cluster — db_cluster_perm_delete","title":"Permanently Delete a Cluster — db_cluster_perm_delete","text":"Permanently Delete Cluster","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_cluster_perm_delete.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Permanently Delete a Cluster — db_cluster_perm_delete","text":"","code":"db_cluster_perm_delete(   cluster_id,   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_cluster_perm_delete.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Permanently Delete a Cluster — db_cluster_perm_delete","text":"cluster_id Canonical identifier cluster. host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_cluster_perm_delete.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Permanently Delete a Cluster — db_cluster_perm_delete","text":"cluster running, terminated resources asynchronously removed. cluster terminated, immediately removed. perform *action, including retrieve cluster’s permissions, permanently deleted cluster. permanently deleted cluster also longer returned cluster list.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_cluster_pin.html","id":null,"dir":"Reference","previous_headings":"","what":"Pin a Cluster — db_cluster_pin","title":"Pin a Cluster — db_cluster_pin","text":"Pin Cluster","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_cluster_pin.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Pin a Cluster — db_cluster_pin","text":"","code":"db_cluster_pin(   cluster_id,   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_cluster_pin.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Pin a Cluster — db_cluster_pin","text":"cluster_id Canonical identifier cluster. host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_cluster_pin.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Pin a Cluster — db_cluster_pin","text":"Ensure -purpose cluster configuration retained even cluster terminated 30 days. Pinning ensures cluster always returned db_cluster_list(). Pinning cluster already pinned effect.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_cluster_resize.html","id":null,"dir":"Reference","previous_headings":"","what":"Resize a Cluster — db_cluster_resize","title":"Resize a Cluster — db_cluster_resize","text":"Resize Cluster","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_cluster_resize.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Resize a Cluster — db_cluster_resize","text":"","code":"db_cluster_resize(   cluster_id,   num_workers = NULL,   autoscale = NULL,   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_cluster_resize.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Resize a Cluster — db_cluster_resize","text":"cluster_id Canonical identifier cluster. num_workers Number worker nodes cluster . cluster one Spark driver num_workers executors total num_workers + 1 Spark nodes. autoscale Instance cluster_autoscale(). host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_cluster_resize.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Resize a Cluster — db_cluster_resize","text":"cluster must RUNNING state.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_cluster_restart.html","id":null,"dir":"Reference","previous_headings":"","what":"Restart a Cluster — db_cluster_restart","title":"Restart a Cluster — db_cluster_restart","text":"Restart Cluster","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_cluster_restart.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Restart a Cluster — db_cluster_restart","text":"","code":"db_cluster_restart(   cluster_id,   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_cluster_restart.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Restart a Cluster — db_cluster_restart","text":"cluster_id Canonical identifier cluster. host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_cluster_restart.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Restart a Cluster — db_cluster_restart","text":"cluster must RUNNING state.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_cluster_runtime_versions.html","id":null,"dir":"Reference","previous_headings":"","what":"List Available Databricks Runtime Versions — db_cluster_runtime_versions","title":"List Available Databricks Runtime Versions — db_cluster_runtime_versions","text":"List Available Databricks Runtime Versions","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_cluster_runtime_versions.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List Available Databricks Runtime Versions — db_cluster_runtime_versions","text":"","code":"db_cluster_runtime_versions(   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_cluster_runtime_versions.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List Available Databricks Runtime Versions — db_cluster_runtime_versions","text":"host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_cluster_runtime_versions.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"List Available Databricks Runtime Versions — db_cluster_runtime_versions","text":"Return list available runtime versions. versions can used launch cluster.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_cluster_start.html","id":null,"dir":"Reference","previous_headings":"","what":"Start a Cluster — db_cluster_start","title":"Start a Cluster — db_cluster_start","text":"Start Cluster","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_cluster_start.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Start a Cluster — db_cluster_start","text":"","code":"db_cluster_start(   cluster_id,   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_cluster_start.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Start a Cluster — db_cluster_start","text":"cluster_id Canonical identifier cluster. host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_cluster_start.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Start a Cluster — db_cluster_start","text":"Start terminated cluster given ID. similar db_cluster_create(), except: terminated cluster ID attributes preserved. cluster starts last specified cluster size. terminated cluster autoscaling cluster, cluster starts minimum number nodes. cluster RESTARTING state, 400 error returned. start cluster launched run job.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_cluster_terminate.html","id":null,"dir":"Reference","previous_headings":"","what":"Delete/Terminate a Cluster — db_cluster_terminate","title":"Delete/Terminate a Cluster — db_cluster_terminate","text":"Delete/Terminate Cluster","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_cluster_terminate.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Delete/Terminate a Cluster — db_cluster_terminate","text":"","code":"db_cluster_terminate(   cluster_id,   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_cluster_terminate.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Delete/Terminate a Cluster — db_cluster_terminate","text":"cluster_id Canonical identifier cluster. host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_cluster_terminate.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Delete/Terminate a Cluster — db_cluster_terminate","text":"cluster removed asynchronously. termination completed, cluster TERMINATED state. cluster already TERMINATING TERMINATED state, nothing happen. Unless cluster pinned, 30 days cluster terminated, permanently deleted.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_cluster_unpin.html","id":null,"dir":"Reference","previous_headings":"","what":"Unpin a Cluster — db_cluster_unpin","title":"Unpin a Cluster — db_cluster_unpin","text":"Unpin Cluster","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_cluster_unpin.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Unpin a Cluster — db_cluster_unpin","text":"","code":"db_cluster_unpin(   cluster_id,   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_cluster_unpin.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Unpin a Cluster — db_cluster_unpin","text":"cluster_id Canonical identifier cluster. host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_cluster_unpin.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Unpin a Cluster — db_cluster_unpin","text":"Allows cluster eventually removed list returned db_cluster_list(). Unpinning cluster pinned effect.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_collect.DatabricksConnection.html","id":null,"dir":"Reference","previous_headings":"","what":"Collect query results with proper progress timing for Databricks — db_collect.DatabricksConnection","title":"Collect query results with proper progress timing for Databricks — db_collect.DatabricksConnection","text":"Collect query results proper progress timing Databricks","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_collect.DatabricksConnection.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Collect query results with proper progress timing for Databricks — db_collect.DatabricksConnection","text":"","code":"# S3 method for class 'DatabricksConnection' db_collect(con, sql, n = -1, warn_incomplete = TRUE, ...)"},{"path":"https://databrickslabs.github.io/brickster/reference/db_collect.DatabricksConnection.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Collect query results with proper progress timing for Databricks — db_collect.DatabricksConnection","text":"con DatabricksConnection object sql SQL query execute n Maximum number rows collect (-1 ) warn_incomplete Whether warn results truncated ... Additional arguments","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_collect.DatabricksConnection.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Collect query results with proper progress timing for Databricks — db_collect.DatabricksConnection","text":"data frame query results","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_context_command_cancel.html","id":null,"dir":"Reference","previous_headings":"","what":"Cancel a Command — db_context_command_cancel","title":"Cancel a Command — db_context_command_cancel","text":"Cancel Command","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_context_command_cancel.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Cancel a Command — db_context_command_cancel","text":"","code":"db_context_command_cancel(   cluster_id,   context_id,   command_id,   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_context_command_cancel.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Cancel a Command — db_context_command_cancel","text":"cluster_id ID cluster create context . context_id ID execution context. command_id ID command get information . host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_context_command_parse.html","id":null,"dir":"Reference","previous_headings":"","what":"Parse Command Results — db_context_command_parse","title":"Parse Command Results — db_context_command_parse","text":"Parse Command Results","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_context_command_parse.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Parse Command Results — db_context_command_parse","text":"","code":"db_context_command_parse(x, language = c(\"r\", \"py\", \"scala\", \"sql\"))"},{"path":"https://databrickslabs.github.io/brickster/reference/db_context_command_parse.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Parse Command Results — db_context_command_parse","text":"x command output db_context_command_status db_context_manager's cmd_run language","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_context_command_parse.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Parse Command Results — db_context_command_parse","text":"command results","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_context_command_run.html","id":null,"dir":"Reference","previous_headings":"","what":"Run a Command — db_context_command_run","title":"Run a Command — db_context_command_run","text":"Run Command","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_context_command_run.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Run a Command — db_context_command_run","text":"","code":"db_context_command_run(   cluster_id,   context_id,   language = c(\"python\", \"sql\", \"scala\", \"r\"),   command = NULL,   command_file = NULL,   options = list(),   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_context_command_run.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Run a Command — db_context_command_run","text":"cluster_id ID cluster create context . context_id ID execution context. language language context. One python, sql, scala, r. command command string run. command_file path file containing command run. options Named list values used downstream. example, 'displayRowLimit' override (used testing). host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_context_command_run_and_wait.html","id":null,"dir":"Reference","previous_headings":"","what":"Run a Command and Wait For Results — db_context_command_run_and_wait","title":"Run a Command and Wait For Results — db_context_command_run_and_wait","text":"Run Command Wait Results","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_context_command_run_and_wait.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Run a Command and Wait For Results — db_context_command_run_and_wait","text":"","code":"db_context_command_run_and_wait(   cluster_id,   context_id,   language = c(\"python\", \"sql\", \"scala\", \"r\"),   command = NULL,   command_file = NULL,   options = list(),   parse_result = TRUE,   host = db_host(),   token = db_token() )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_context_command_run_and_wait.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Run a Command and Wait For Results — db_context_command_run_and_wait","text":"cluster_id ID cluster create context . context_id ID execution context. language language context. One python, sql, scala, r. command command string run. command_file path file containing command run. options Named list values used downstream. example, 'displayRowLimit' override (used testing). parse_result Boolean, determines results parsed automatically. host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token().","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_context_command_status.html","id":null,"dir":"Reference","previous_headings":"","what":"Get Information About a Command — db_context_command_status","title":"Get Information About a Command — db_context_command_status","text":"Get Information Command","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_context_command_status.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get Information About a Command — db_context_command_status","text":"","code":"db_context_command_status(   cluster_id,   context_id,   command_id,   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_context_command_status.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get Information About a Command — db_context_command_status","text":"cluster_id ID cluster create context . context_id ID execution context. command_id ID command get information . host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_context_create.html","id":null,"dir":"Reference","previous_headings":"","what":"Create an Execution Context — db_context_create","title":"Create an Execution Context — db_context_create","text":"Create Execution Context","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_context_create.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create an Execution Context — db_context_create","text":"","code":"db_context_create(   cluster_id,   language = c(\"python\", \"sql\", \"scala\", \"r\"),   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_context_create.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create an Execution Context — db_context_create","text":"cluster_id ID cluster create context . language language context. One python, sql, scala, r. host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_context_destroy.html","id":null,"dir":"Reference","previous_headings":"","what":"Delete an Execution Context — db_context_destroy","title":"Delete an Execution Context — db_context_destroy","text":"Delete Execution Context","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_context_destroy.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Delete an Execution Context — db_context_destroy","text":"","code":"db_context_destroy(   cluster_id,   context_id,   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_context_destroy.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Delete an Execution Context — db_context_destroy","text":"cluster_id ID cluster create context . context_id ID execution context. host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_context_manager.html","id":null,"dir":"Reference","previous_headings":"","what":"Databricks Execution Context Manager (R6 Class) — db_context_manager","title":"Databricks Execution Context Manager (R6 Class) — db_context_manager","text":"Databricks Execution Context Manager (R6 Class) Databricks Execution Context Manager (R6 Class)","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_context_manager.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Databricks Execution Context Manager (R6 Class) — db_context_manager","text":"db_context_manager() provides simple interface send commands Databricks cluster return results.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_context_manager.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Databricks Execution Context Manager (R6 Class) — db_context_manager","text":"db_context_manager$new() db_context_manager$close() db_context_manager$cmd_run() db_context_manager$clone()","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_context_manager.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Databricks Execution Context Manager (R6 Class) — db_context_manager","text":"Create new context manager object.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_context_manager.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Databricks Execution Context Manager (R6 Class) — db_context_manager","text":"","code":"db_context_manager$new(   cluster_id,   language = c(\"r\", \"py\", \"scala\", \"sql\", \"sh\"),   host = db_host(),   token = db_token() )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_context_manager.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Databricks Execution Context Manager (R6 Class) — db_context_manager","text":"cluster_id ID cluster execute command . language One r, py, scala, sql, sh. host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token().","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_context_manager.html","id":"returns","dir":"Reference","previous_headings":"","what":"Returns","title":"Databricks Execution Context Manager (R6 Class) — db_context_manager","text":"new databricks_context_manager object.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_context_manager.html","id":"method-close-","dir":"Reference","previous_headings":"","what":"Method close()","title":"Databricks Execution Context Manager (R6 Class) — db_context_manager","text":"Destroy execution context","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_context_manager.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Databricks Execution Context Manager (R6 Class) — db_context_manager","text":"","code":"db_context_manager$close()"},{"path":"https://databrickslabs.github.io/brickster/reference/db_context_manager.html","id":"method-cmd-run-","dir":"Reference","previous_headings":"","what":"Method cmd_run()","title":"Databricks Execution Context Manager (R6 Class) — db_context_manager","text":"Execute command Databricks cluster","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_context_manager.html","id":"usage-2","dir":"Reference","previous_headings":"","what":"Usage","title":"Databricks Execution Context Manager (R6 Class) — db_context_manager","text":"","code":"db_context_manager$cmd_run(cmd, language = c(\"r\", \"py\", \"scala\", \"sql\", \"sh\"))"},{"path":"https://databrickslabs.github.io/brickster/reference/db_context_manager.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"Databricks Execution Context Manager (R6 Class) — db_context_manager","text":"cmd code execute Databricks cluster language One r, py, scala, sql, sh.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_context_manager.html","id":"returns-1","dir":"Reference","previous_headings":"","what":"Returns","title":"Databricks Execution Context Manager (R6 Class) — db_context_manager","text":"Command results","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_context_manager.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"Databricks Execution Context Manager (R6 Class) — db_context_manager","text":"objects class cloneable method.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_context_manager.html","id":"usage-3","dir":"Reference","previous_headings":"","what":"Usage","title":"Databricks Execution Context Manager (R6 Class) — db_context_manager","text":"","code":"db_context_manager$clone(deep = FALSE)"},{"path":"https://databrickslabs.github.io/brickster/reference/db_context_manager.html","id":"arguments-2","dir":"Reference","previous_headings":"","what":"Arguments","title":"Databricks Execution Context Manager (R6 Class) — db_context_manager","text":"deep Whether make deep clone.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_context_status.html","id":null,"dir":"Reference","previous_headings":"","what":"Get Information About an Execution Context — db_context_status","title":"Get Information About an Execution Context — db_context_status","text":"Get Information Execution Context","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_context_status.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get Information About an Execution Context — db_context_status","text":"","code":"db_context_status(   cluster_id,   context_id,   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_context_status.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get Information About an Execution Context — db_context_status","text":"cluster_id ID cluster create context . context_id ID execution context. host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_create_table_as_select_values.html","id":null,"dir":"Reference","previous_headings":"","what":"Create table using atomic CTAS with VALUES — db_create_table_as_select_values","title":"Create table using atomic CTAS with VALUES — db_create_table_as_select_values","text":"Create table using atomic CTAS VALUES","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_create_table_as_select_values.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create table using atomic CTAS with VALUES — db_create_table_as_select_values","text":"","code":"db_create_table_as_select_values(   conn,   quoted_name,   value,   field.types,   temporary = FALSE,   overwrite = FALSE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_create_table_from_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Create table from data frame structure — db_create_table_from_data","title":"Create table from data frame structure — db_create_table_from_data","text":"Create table data frame structure","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_create_table_from_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create table from data frame structure — db_create_table_from_data","text":"","code":"db_create_table_from_data(   conn,   quoted_name,   value,   field.types,   temporary = FALSE,   overwrite = FALSE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_current_cloud.html","id":null,"dir":"Reference","previous_headings":"","what":"Detect Current Workspaces Cloud — db_current_cloud","title":"Detect Current Workspaces Cloud — db_current_cloud","text":"Detect Current Workspaces Cloud","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_current_cloud.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Detect Current Workspaces Cloud — db_current_cloud","text":"","code":"db_current_cloud(host = db_host(), token = db_token(), perform_request = TRUE)"},{"path":"https://databrickslabs.github.io/brickster/reference/db_current_cloud.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Detect Current Workspaces Cloud — db_current_cloud","text":"host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_current_cloud.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Detect Current Workspaces Cloud — db_current_cloud","text":"String","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_current_user.html","id":null,"dir":"Reference","previous_headings":"","what":"Get Current User Info — db_current_user","title":"Get Current User Info — db_current_user","text":"Get Current User Info","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_current_user.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get Current User Info — db_current_user","text":"","code":"db_current_user(host = db_host(), token = db_token(), perform_request = TRUE)"},{"path":"https://databrickslabs.github.io/brickster/reference/db_current_user.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get Current User Info — db_current_user","text":"host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_current_user.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get Current User Info — db_current_user","text":"list user metadata","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_current_workspace_id.html","id":null,"dir":"Reference","previous_headings":"","what":"Detect Current Workspace ID — db_current_workspace_id","title":"Detect Current Workspace ID — db_current_workspace_id","text":"Detect Current Workspace ID","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_current_workspace_id.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Detect Current Workspace ID — db_current_workspace_id","text":"","code":"db_current_workspace_id(   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_current_workspace_id.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Detect Current Workspace ID — db_current_workspace_id","text":"host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_current_workspace_id.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Detect Current Workspace ID — db_current_workspace_id","text":"String","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_dbfs_add_block.html","id":null,"dir":"Reference","previous_headings":"","what":"DBFS Add Block — db_dbfs_add_block","title":"DBFS Add Block — db_dbfs_add_block","text":"Append block data stream specified input handle.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_dbfs_add_block.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"DBFS Add Block — db_dbfs_add_block","text":"","code":"db_dbfs_add_block(   handle,   data,   convert_to_raw = FALSE,   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_dbfs_add_block.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"DBFS Add Block — db_dbfs_add_block","text":"handle Handle open stream. data Either path file local system character/raw vector base64-encoded. limit 1 MB. convert_to_raw Boolean (Default: FALSE), TRUE convert character vector raw via base::.raw(). host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_dbfs_add_block.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"DBFS Add Block — db_dbfs_add_block","text":"handle exist, call throw exception RESOURCE_DOES_NOT_EXIST. block data exceeds 1 MB, call throw exception MAX_BLOCK_SIZE_EXCEEDED.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_dbfs_add_block.html","id":"typical-file-upload-flow","dir":"Reference","previous_headings":"","what":"Typical File Upload Flow","title":"DBFS Add Block — db_dbfs_add_block","text":"Call create get handle via db_dbfs_create() Make one db_dbfs_add_block() calls handle Call db_dbfs_close() handle ","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_dbfs_close.html","id":null,"dir":"Reference","previous_headings":"","what":"DBFS Close — db_dbfs_close","title":"DBFS Close — db_dbfs_close","text":"Close stream specified input handle.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_dbfs_close.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"DBFS Close — db_dbfs_close","text":"","code":"db_dbfs_close(   handle,   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_dbfs_close.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"DBFS Close — db_dbfs_close","text":"handle handle open stream. field required. host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_dbfs_close.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"DBFS Close — db_dbfs_close","text":"HTTP Response","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_dbfs_close.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"DBFS Close — db_dbfs_close","text":"handle exist, call throws exception RESOURCE_DOES_NOT_EXIST.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_dbfs_close.html","id":"typical-file-upload-flow","dir":"Reference","previous_headings":"","what":"Typical File Upload Flow","title":"DBFS Close — db_dbfs_close","text":"Call create get handle via db_dbfs_create() Make one db_dbfs_add_block() calls handle Call db_dbfs_close() handle ","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_dbfs_create.html","id":null,"dir":"Reference","previous_headings":"","what":"DBFS Create — db_dbfs_create","title":"DBFS Create — db_dbfs_create","text":"Open stream write file returns handle stream.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_dbfs_create.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"DBFS Create — db_dbfs_create","text":"","code":"db_dbfs_create(   path,   overwrite = FALSE,   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_dbfs_create.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"DBFS Create — db_dbfs_create","text":"path path new file. path absolute DBFS path (example /mnt/-file.txt). overwrite Boolean, specifies whether overwrite existing file files. host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_dbfs_create.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"DBFS Create — db_dbfs_create","text":"Handle subsequently passed db_dbfs_add_block() db_dbfs_close() writing file stream.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_dbfs_create.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"DBFS Create — db_dbfs_create","text":"10 minute idle timeout handle. file directory already exists given path overwrite set FALSE, call throws exception RESOURCE_ALREADY_EXISTS.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_dbfs_create.html","id":"typical-file-upload-flow","dir":"Reference","previous_headings":"","what":"Typical File Upload Flow","title":"DBFS Create — db_dbfs_create","text":"Call create get handle via db_dbfs_create() Make one db_dbfs_add_block() calls handle Call db_dbfs_close() handle ","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_dbfs_delete.html","id":null,"dir":"Reference","previous_headings":"","what":"DBFS Delete — db_dbfs_delete","title":"DBFS Delete — db_dbfs_delete","text":"DBFS Delete","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_dbfs_delete.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"DBFS Delete — db_dbfs_delete","text":"","code":"db_dbfs_delete(   path,   recursive = FALSE,   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_dbfs_delete.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"DBFS Delete — db_dbfs_delete","text":"path path new file. path absolute DBFS path (example /mnt/-file.txt). recursive Whether recursively delete directory’s contents. Deleting empty directories can done without providing recursive flag. host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_dbfs_get_status.html","id":null,"dir":"Reference","previous_headings":"","what":"DBFS Get Status — db_dbfs_get_status","title":"DBFS Get Status — db_dbfs_get_status","text":"Get file information file directory.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_dbfs_get_status.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"DBFS Get Status — db_dbfs_get_status","text":"","code":"db_dbfs_get_status(   path,   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_dbfs_get_status.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"DBFS Get Status — db_dbfs_get_status","text":"path path new file. path absolute DBFS path (example /mnt/-file.txt). host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_dbfs_get_status.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"DBFS Get Status — db_dbfs_get_status","text":"file directory exist, call throws exception RESOURCE_DOES_NOT_EXIST.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_dbfs_list.html","id":null,"dir":"Reference","previous_headings":"","what":"DBFS List — db_dbfs_list","title":"DBFS List — db_dbfs_list","text":"List contents directory, details file.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_dbfs_list.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"DBFS List — db_dbfs_list","text":"","code":"db_dbfs_list(   path,   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_dbfs_list.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"DBFS List — db_dbfs_list","text":"path path new file. path absolute DBFS path (example /mnt/-file.txt). host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_dbfs_list.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"DBFS List — db_dbfs_list","text":"data.frame","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_dbfs_list.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"DBFS List — db_dbfs_list","text":"calling list large directory, list operation time approximately 60 seconds. strongly recommend using list directories containing less 10K files discourage using DBFS REST API operations list 10K files. Instead, recommend perform operations context cluster, using File system utility (dbutils.fs), provides functionality without timing . file directory exist, call throws exception RESOURCE_DOES_NOT_EXIST.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_dbfs_mkdirs.html","id":null,"dir":"Reference","previous_headings":"","what":"DBFS mkdirs — db_dbfs_mkdirs","title":"DBFS mkdirs — db_dbfs_mkdirs","text":"Create given directory necessary parent directories exist.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_dbfs_mkdirs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"DBFS mkdirs — db_dbfs_mkdirs","text":"","code":"db_dbfs_mkdirs(   path,   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_dbfs_mkdirs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"DBFS mkdirs — db_dbfs_mkdirs","text":"path path new file. path absolute DBFS path (example /mnt/-file.txt). host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_dbfs_mkdirs.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"DBFS mkdirs — db_dbfs_mkdirs","text":"exists file (directory) prefix input path, call throws exception RESOURCE_ALREADY_EXISTS. operation fails may succeeded creating necessary parent directories.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_dbfs_move.html","id":null,"dir":"Reference","previous_headings":"","what":"DBFS Move — db_dbfs_move","title":"DBFS Move — db_dbfs_move","text":"Move file one location another location within DBFS.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_dbfs_move.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"DBFS Move — db_dbfs_move","text":"","code":"db_dbfs_move(   source_path,   destination_path,   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_dbfs_move.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"DBFS Move — db_dbfs_move","text":"source_path source path file directory. path absolute DBFS path (example, /mnt/-source-folder/). destination_path destination path file directory. path absolute DBFS path (example, /mnt/-destination-folder/). host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_dbfs_move.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"DBFS Move — db_dbfs_move","text":"given source path directory, call always recursively moves files. moving large number files, API call time approximately 60 seconds, potentially resulting partially moved data. Therefore, operations move 10K files, strongly discourage using DBFS REST API. Instead, recommend perform operations context cluster, using File system utility (dbutils.fs) notebook, provides functionality without timing . source file exist, call throws exception RESOURCE_DOES_NOT_EXIST. already exists file destination path, call throws exception RESOURCE_ALREADY_EXISTS.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_dbfs_put.html","id":null,"dir":"Reference","previous_headings":"","what":"DBFS Put — db_dbfs_put","title":"DBFS Put — db_dbfs_put","text":"Upload file use multipart form post.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_dbfs_put.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"DBFS Put — db_dbfs_put","text":"","code":"db_dbfs_put(   path,   file = NULL,   contents = NULL,   overwrite = FALSE,   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_dbfs_put.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"DBFS Put — db_dbfs_put","text":"path path new file. path absolute DBFS path (example /mnt/-file.txt). file Path file local system, takes precedent path. contents String base64 encoded. overwrite Flag (Default: FALSE) specifies whether overwrite existing files. host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_dbfs_put.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"DBFS Put — db_dbfs_put","text":"Either contents file must specified. file takes precedent contents specified. Mainly used streaming uploads, can also used convenient single call data upload. amount data can passed using contents parameter limited 1 MB specified string (MAX_BLOCK_SIZE_EXCEEDED thrown exceeded) 2 GB file.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_dbfs_read.html","id":null,"dir":"Reference","previous_headings":"","what":"DBFS Read — db_dbfs_read","title":"DBFS Read — db_dbfs_read","text":"Return contents file.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_dbfs_read.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"DBFS Read — db_dbfs_read","text":"","code":"db_dbfs_read(   path,   offset = 0,   length = NULL,   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_dbfs_read.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"DBFS Read — db_dbfs_read","text":"path path new file. path absolute DBFS path (example /mnt/-file.txt). offset Offset read bytes. length Number bytes read starting offset. limit 1 MB, default value 0.5 MB. host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_dbfs_read.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"DBFS Read — db_dbfs_read","text":"offset + length exceeds number bytes file, reads contents end file. file exist, call throws exception RESOURCE_DOES_NOT_EXIST. path directory, read length negative, offset negative, call throws exception INVALID_PARAMETER_VALUE. read length exceeds 1 MB, call throws exception MAX_READ_SIZE_EXCEEDED.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_generate_typed_values_sql.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate type-aware VALUES SQL from data frame — db_generate_typed_values_sql","title":"Generate type-aware VALUES SQL from data frame — db_generate_typed_values_sql","text":"Generate type-aware VALUES SQL data frame","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_generate_typed_values_sql.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate type-aware VALUES SQL from data frame — db_generate_typed_values_sql","text":"","code":"db_generate_typed_values_sql(conn, data)"},{"path":"https://databrickslabs.github.io/brickster/reference/db_generate_typed_values_sql_for_view.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate typed VALUES SQL for temporary views (helper) — db_generate_typed_values_sql_for_view","title":"Generate typed VALUES SQL for temporary views (helper) — db_generate_typed_values_sql_for_view","text":"Generate typed VALUES SQL temporary views (helper)","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_generate_typed_values_sql_for_view.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate typed VALUES SQL for temporary views (helper) — db_generate_typed_values_sql_for_view","text":"","code":"db_generate_typed_values_sql_for_view(con, data)"},{"path":"https://databrickslabs.github.io/brickster/reference/db_generate_typed_values_sql_for_view.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate typed VALUES SQL for temporary views (helper) — db_generate_typed_values_sql_for_view","text":"con DatabricksConnection object data Data frame","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_generate_typed_values_sql_for_view.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate typed VALUES SQL for temporary views (helper) — db_generate_typed_values_sql_for_view","text":"SQL VALUES clause","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_generate_values_sql.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate VALUES SQL from data frame — db_generate_values_sql","title":"Generate VALUES SQL from data frame — db_generate_values_sql","text":"Generate VALUES SQL data frame","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_generate_values_sql.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate VALUES SQL from data frame — db_generate_values_sql","text":"","code":"db_generate_values_sql(conn, data)"},{"path":"https://databrickslabs.github.io/brickster/reference/db_host.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate/Fetch Databricks Host — db_host","title":"Generate/Fetch Databricks Host — db_host","text":"id prefix NULL function check DATABRICKS_HOST environment variable. .databrickscfg searched db_profile use_databrickscfg set Posit Workbench managed OAuth credentials detected. defining id prefix need specify whole URL. E.g. https://<prefix>.<id>.cloud.databricks.com/ form follow.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_host.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate/Fetch Databricks Host — db_host","text":"","code":"db_host(id = NULL, prefix = NULL, profile = default_config_profile())"},{"path":"https://databrickslabs.github.io/brickster/reference/db_host.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate/Fetch Databricks Host — db_host","text":"id workspace string prefix Workspace prefix profile Profile use fetching environment variable (e.g. .Renviron) .databricksfg file","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_host.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate/Fetch Databricks Host — db_host","text":"workspace URL","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_host.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Generate/Fetch Databricks Host — db_host","text":"behaviour subject change depending db_profile use_databrickscfg options set. use_databrickscfg: Boolean (default: FALSE), determines credentials fetched profile .databrickscfg .Renviron db_profile: String (default: NULL), determines profile used. .databrickscfg automatically used Posit Workbench managed OAuth credentials detected. See vignette authentication details.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_jobs_create.html","id":null,"dir":"Reference","previous_headings":"","what":"Create Job — db_jobs_create","title":"Create Job — db_jobs_create","text":"Create Job","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_jobs_create.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create Job — db_jobs_create","text":"","code":"db_jobs_create(   name,   tasks,   schedule = NULL,   job_clusters = NULL,   email_notifications = NULL,   timeout_seconds = NULL,   max_concurrent_runs = 1,   access_control_list = NULL,   git_source = NULL,   queue = TRUE,   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_jobs_create.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create Job — db_jobs_create","text":"name Name job. tasks Task specifications executed job. Use job_tasks(). schedule Instance cron_schedule(). job_clusters Named list job cluster specifications (using new_cluster()) can shared reused tasks job. Libraries declared shared job cluster. must declare dependent libraries task settings. email_notifications Instance email_notifications(). timeout_seconds optional timeout applied run job. default behavior timeout. max_concurrent_runs Maximum allowed number concurrent runs job. Set value want able execute multiple runs job concurrently. setting affects new runs. value exceed 1000. Setting value 0 causes new runs skipped. default behavior allow 1 concurrent run. access_control_list Instance access_control_request(). git_source Optional specification remote repository containing notebooks used job's notebook tasks. Instance git_source(). queue true, enable queueing job. host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_jobs_create.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create Job — db_jobs_create","text":"Full Documentation","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_jobs_delete.html","id":null,"dir":"Reference","previous_headings":"","what":"Delete a Job — db_jobs_delete","title":"Delete a Job — db_jobs_delete","text":"Delete Job","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_jobs_delete.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Delete a Job — db_jobs_delete","text":"","code":"db_jobs_delete(   job_id,   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_jobs_delete.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Delete a Job — db_jobs_delete","text":"job_id canonical identifier job. host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_jobs_get.html","id":null,"dir":"Reference","previous_headings":"","what":"Get Job Details — db_jobs_get","title":"Get Job Details — db_jobs_get","text":"Get Job Details","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_jobs_get.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get Job Details — db_jobs_get","text":"","code":"db_jobs_get(   job_id,   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_jobs_get.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get Job Details — db_jobs_get","text":"job_id canonical identifier job. host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_jobs_list.html","id":null,"dir":"Reference","previous_headings":"","what":"List Jobs — db_jobs_list","title":"List Jobs — db_jobs_list","text":"List Jobs","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_jobs_list.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List Jobs — db_jobs_list","text":"","code":"db_jobs_list(   limit = 25,   offset = 0,   expand_tasks = FALSE,   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_jobs_list.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List Jobs — db_jobs_list","text":"limit Number jobs return. value must greater 0 less equal 25. default value 25. request specifies limit 0, service instead uses maximum limit. offset offset first job return, relative recently created job. expand_tasks Whether include task cluster details response. host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_jobs_reset.html","id":null,"dir":"Reference","previous_headings":"","what":"Overwrite All Settings For A Job — db_jobs_reset","title":"Overwrite All Settings For A Job — db_jobs_reset","text":"Overwrite Settings Job","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_jobs_reset.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Overwrite All Settings For A Job — db_jobs_reset","text":"","code":"db_jobs_reset(   job_id,   name,   schedule,   tasks,   job_clusters = NULL,   email_notifications = NULL,   timeout_seconds = NULL,   max_concurrent_runs = 1,   access_control_list = NULL,   git_source = NULL,   queue = NULL,   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_jobs_reset.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Overwrite All Settings For A Job — db_jobs_reset","text":"job_id canonical identifier job. name Name job. schedule Instance cron_schedule(). tasks Task specifications executed job. Use job_tasks(). job_clusters Named list job cluster specifications (using new_cluster()) can shared reused tasks job. Libraries declared shared job cluster. must declare dependent libraries task settings. email_notifications Instance email_notifications(). timeout_seconds optional timeout applied run job. default behavior timeout. max_concurrent_runs Maximum allowed number concurrent runs job. Set value want able execute multiple runs job concurrently. setting affects new runs. value exceed 1000. Setting value 0 causes new runs skipped. default behavior allow 1 concurrent run. access_control_list Instance access_control_request(). git_source Optional specification remote repository containing notebooks used job's notebook tasks. Instance git_source(). queue true, enable queueing job. host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_jobs_run_now.html","id":null,"dir":"Reference","previous_headings":"","what":"Trigger A New Job Run — db_jobs_run_now","title":"Trigger A New Job Run — db_jobs_run_now","text":"Trigger New Job Run","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_jobs_run_now.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Trigger A New Job Run — db_jobs_run_now","text":"","code":"db_jobs_run_now(   job_id,   jar_params = list(),   notebook_params = list(),   python_params = list(),   spark_submit_params = list(),   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_jobs_run_now.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Trigger A New Job Run — db_jobs_run_now","text":"job_id canonical identifier job. jar_params Named list. Parameters used invoke main function main class specified Spark JAR task. specified upon run-now, defaults empty list. jar_params specified conjunction notebook_params. notebook_params Named list. Parameters passed notebook accessible dbutils.widgets.get function. specified upon run-now, triggered run uses job’s base parameters. python_params Named list. Parameters passed Python file command-line parameters. specified upon run-now, overwrite parameters specified job setting. spark_submit_params Named list. Parameters passed spark-submit script command-line parameters. specified upon run-now, overwrite parameters specified job setting. host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_jobs_run_now.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Trigger A New Job Run — db_jobs_run_now","text":"*_params parameters exceed 10,000 bytes serialized JSON. jar_params notebook_params mutually exclusive.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_jobs_runs_cancel.html","id":null,"dir":"Reference","previous_headings":"","what":"Cancel Job Run — db_jobs_runs_cancel","title":"Cancel Job Run — db_jobs_runs_cancel","text":"Cancels run.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_jobs_runs_cancel.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Cancel Job Run — db_jobs_runs_cancel","text":"","code":"db_jobs_runs_cancel(   run_id,   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_jobs_runs_cancel.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Cancel Job Run — db_jobs_runs_cancel","text":"run_id canonical identifier run. host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_jobs_runs_cancel.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Cancel Job Run — db_jobs_runs_cancel","text":"run canceled asynchronously, request completes, run may still running. run terminated shortly. run already terminal life_cycle_state, method -op.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_jobs_runs_delete.html","id":null,"dir":"Reference","previous_headings":"","what":"Delete Job Run — db_jobs_runs_delete","title":"Delete Job Run — db_jobs_runs_delete","text":"Delete Job Run","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_jobs_runs_delete.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Delete Job Run — db_jobs_runs_delete","text":"","code":"db_jobs_runs_delete(   run_id,   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_jobs_runs_delete.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Delete Job Run — db_jobs_runs_delete","text":"run_id canonical identifier run. host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_jobs_runs_export.html","id":null,"dir":"Reference","previous_headings":"","what":"Export Job Run Output — db_jobs_runs_export","title":"Export Job Run Output — db_jobs_runs_export","text":"Export retrieve job run task.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_jobs_runs_export.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Export Job Run Output — db_jobs_runs_export","text":"","code":"db_jobs_runs_export(   run_id,   views_to_export = c(\"CODE\", \"DASHBOARDS\", \"ALL\"),   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_jobs_runs_export.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Export Job Run Output — db_jobs_runs_export","text":"run_id canonical identifier run. views_to_export views export. One CODE, DASHBOARDS, . Defaults CODE. host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_jobs_runs_get.html","id":null,"dir":"Reference","previous_headings":"","what":"Get Job Run Details — db_jobs_runs_get","title":"Get Job Run Details — db_jobs_runs_get","text":"Retrieve metadata run.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_jobs_runs_get.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get Job Run Details — db_jobs_runs_get","text":"","code":"db_jobs_runs_get(   run_id,   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_jobs_runs_get.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get Job Run Details — db_jobs_runs_get","text":"run_id canonical identifier run. host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_jobs_runs_get_output.html","id":null,"dir":"Reference","previous_headings":"","what":"Get Job Run Output — db_jobs_runs_get_output","title":"Get Job Run Output — db_jobs_runs_get_output","text":"Get Job Run Output","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_jobs_runs_get_output.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get Job Run Output — db_jobs_runs_get_output","text":"","code":"db_jobs_runs_get_output(   run_id,   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_jobs_runs_get_output.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get Job Run Output — db_jobs_runs_get_output","text":"run_id canonical identifier run. host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_jobs_runs_list.html","id":null,"dir":"Reference","previous_headings":"","what":"List Job Runs — db_jobs_runs_list","title":"List Job Runs — db_jobs_runs_list","text":"List runs descending order start time.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_jobs_runs_list.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List Job Runs — db_jobs_runs_list","text":"","code":"db_jobs_runs_list(   job_id,   active_only = FALSE,   completed_only = FALSE,   offset = 0,   limit = 25,   run_type = c(\"JOB_RUN\", \"WORKFLOW_RUN\", \"SUBMIT_RUN\"),   expand_tasks = FALSE,   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_jobs_runs_list.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List Job Runs — db_jobs_runs_list","text":"job_id canonical identifier job. active_only Boolean (Default: FALSE). TRUE active runs included results; otherwise, lists active completed runs. active run run PENDING, RUNNING, TERMINATING. field true completed_only TRUE. completed_only Boolean (Default: FALSE). TRUE, completed runs included results; otherwise, lists active completed runs. field true active_only TRUE. offset offset first job return, relative recently created job. limit Number jobs return. value must greater 0 less equal 25. default value 25. request specifies limit 0, service instead uses maximum limit. run_type type runs return. One JOB_RUN, WORKFLOW_RUN, SUBMIT_RUN. expand_tasks Whether include task cluster details response. host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_jobs_runs_submit.html","id":null,"dir":"Reference","previous_headings":"","what":"Create And Trigger A One-Time Run — db_jobs_runs_submit","title":"Create And Trigger A One-Time Run — db_jobs_runs_submit","text":"Create Trigger One-Time Run","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_jobs_runs_submit.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create And Trigger A One-Time Run — db_jobs_runs_submit","text":"","code":"db_jobs_runs_submit(   tasks,   run_name,   timeout_seconds = NULL,   idempotency_token = NULL,   access_control_list = NULL,   git_source = NULL,   job_clusters = NULL,   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_jobs_runs_submit.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create And Trigger A One-Time Run — db_jobs_runs_submit","text":"tasks Task specifications executed job. Use job_tasks(). run_name Name run. timeout_seconds optional timeout applied run job. default behavior timeout. idempotency_token optional token can used guarantee idempotency job run requests. active run provided token already exists, request create new run, returns ID existing run instead. specify idempotency token, upon failure can retry request succeeds. Databricks guarantees exactly one run launched idempotency token. token must 64 characters. access_control_list Instance access_control_request(). git_source Optional specification remote repository containing notebooks used job's notebook tasks. Instance git_source(). job_clusters Named list job cluster specifications (using new_cluster()) can shared reused tasks job. Libraries declared shared job cluster. must declare dependent libraries task settings. host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_jobs_update.html","id":null,"dir":"Reference","previous_headings":"","what":"Partially Update A Job — db_jobs_update","title":"Partially Update A Job — db_jobs_update","text":"Partially Update Job","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_jobs_update.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Partially Update A Job — db_jobs_update","text":"","code":"db_jobs_update(   job_id,   fields_to_remove = list(),   name = NULL,   schedule = NULL,   tasks = NULL,   job_clusters = NULL,   email_notifications = NULL,   timeout_seconds = NULL,   max_concurrent_runs = NULL,   access_control_list = NULL,   git_source = NULL,   queue = NULL,   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_jobs_update.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Partially Update A Job — db_jobs_update","text":"job_id canonical identifier job. fields_to_remove Remove top-level fields job settings. Removing nested fields supported. field optional. Must list(). name Name job. schedule Instance cron_schedule(). tasks Task specifications executed job. Use job_tasks(). job_clusters Named list job cluster specifications (using new_cluster()) can shared reused tasks job. Libraries declared shared job cluster. must declare dependent libraries task settings. email_notifications Instance email_notifications(). timeout_seconds optional timeout applied run job. default behavior timeout. max_concurrent_runs Maximum allowed number concurrent runs job. Set value want able execute multiple runs job concurrently. setting affects new runs. value exceed 1000. Setting value 0 causes new runs skipped. default behavior allow 1 concurrent run. access_control_list Instance access_control_request(). git_source Optional specification remote repository containing notebooks used job's notebook tasks. Instance git_source(). queue true, enable queueing job. host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_jobs_update.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Partially Update A Job — db_jobs_update","text":"Parameters shared db_jobs_create() optional, specify changing.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_libs_all_cluster_statuses.html","id":null,"dir":"Reference","previous_headings":"","what":"Get Status of All Libraries on All Clusters — db_libs_all_cluster_statuses","title":"Get Status of All Libraries on All Clusters — db_libs_all_cluster_statuses","text":"Get Status Libraries Clusters","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_libs_all_cluster_statuses.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get Status of All Libraries on All Clusters — db_libs_all_cluster_statuses","text":"","code":"db_libs_all_cluster_statuses(   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_libs_all_cluster_statuses.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get Status of All Libraries on All Clusters — db_libs_all_cluster_statuses","text":"host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_libs_all_cluster_statuses.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get Status of All Libraries on All Clusters — db_libs_all_cluster_statuses","text":"status available libraries installed clusters via API libraries UI well libraries set installed clusters via libraries UI. library set installed clusters, is_library_for_all_clusters true, even library also installed specific cluster.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_libs_cluster_status.html","id":null,"dir":"Reference","previous_headings":"","what":"Get Status of Libraries on Cluster — db_libs_cluster_status","title":"Get Status of Libraries on Cluster — db_libs_cluster_status","text":"Get Status Libraries Cluster","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_libs_cluster_status.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get Status of Libraries on Cluster — db_libs_cluster_status","text":"","code":"db_libs_cluster_status(   cluster_id,   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_libs_cluster_status.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get Status of Libraries on Cluster — db_libs_cluster_status","text":"cluster_id Unique identifier Databricks cluster. host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_libs_install.html","id":null,"dir":"Reference","previous_headings":"","what":"Install Library on Cluster — db_libs_install","title":"Install Library on Cluster — db_libs_install","text":"Install Library Cluster","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_libs_install.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Install Library on Cluster — db_libs_install","text":"","code":"db_libs_install(   cluster_id,   libraries,   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_libs_install.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Install Library on Cluster — db_libs_install","text":"cluster_id Unique identifier Databricks cluster. libraries object created libraries() appropriate lib_*() functions. host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_libs_install.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Install Library on Cluster — db_libs_install","text":"Installation asynchronous - completes background request. call fail cluster terminated. Installing wheel library cluster like running pip command wheel file directly driver executors. Installing wheel library cluster like running pip command wheel file directly driver executors. dependencies specified library setup.py file installed requires library name satisfy wheel file name convention. installation executors happens new task launched. Databricks Runtime 7.1 , installation order libraries nondeterministic. wheel libraries, can ensure deterministic installation order creating zip file suffix .wheelhouse.zip includes wheel files.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_libs_uninstall.html","id":null,"dir":"Reference","previous_headings":"","what":"Uninstall Library on Cluster — db_libs_uninstall","title":"Uninstall Library on Cluster — db_libs_uninstall","text":"Uninstall Library Cluster","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_libs_uninstall.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Uninstall Library on Cluster — db_libs_uninstall","text":"","code":"db_libs_uninstall(   cluster_id,   libraries,   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_libs_uninstall.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Uninstall Library on Cluster — db_libs_uninstall","text":"cluster_id Unique identifier Databricks cluster. libraries object created libraries() appropriate lib_*() functions. host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_libs_uninstall.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Uninstall Library on Cluster — db_libs_uninstall","text":"libraries aren’t uninstalled cluster restarted. Uninstalling libraries installed cluster impact error.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_mlflow_model_approve_transition_req.html","id":null,"dir":"Reference","previous_headings":"","what":"Approve Model Version Stage Transition Request — db_mlflow_model_approve_transition_req","title":"Approve Model Version Stage Transition Request — db_mlflow_model_approve_transition_req","text":"Approve Model Version Stage Transition Request","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_mlflow_model_approve_transition_req.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Approve Model Version Stage Transition Request — db_mlflow_model_approve_transition_req","text":"","code":"db_mlflow_model_approve_transition_req(   name,   version,   stage = c(\"None\", \"Staging\", \"Production\", \"Archived\"),   archive_existing_versions = TRUE,   comment = NULL,   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_mlflow_model_approve_transition_req.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Approve Model Version Stage Transition Request — db_mlflow_model_approve_transition_req","text":"name Name model. version Version model. stage Target stage transition. Valid values : None, Staging, Production, Archived. archive_existing_versions Boolean (Default: TRUE). Specifies whether archive current model versions target stage. comment User-provided comment action. host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_mlflow_model_delete_transition_req.html","id":null,"dir":"Reference","previous_headings":"","what":"Delete a Model Version Stage Transition Request — db_mlflow_model_delete_transition_req","title":"Delete a Model Version Stage Transition Request — db_mlflow_model_delete_transition_req","text":"Delete Model Version Stage Transition Request","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_mlflow_model_delete_transition_req.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Delete a Model Version Stage Transition Request — db_mlflow_model_delete_transition_req","text":"","code":"db_mlflow_model_delete_transition_req(   name,   version,   stage = c(\"None\", \"Staging\", \"Production\", \"Archived\"),   creator,   comment = NULL,   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_mlflow_model_delete_transition_req.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Delete a Model Version Stage Transition Request — db_mlflow_model_delete_transition_req","text":"name Name model. version Version model. stage Target stage transition. Valid values : None, Staging, Production, Archived. creator Username user created request. transition requests matching specified details, one transition created user deleted. comment User-provided comment action. host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_mlflow_model_open_transition_reqs.html","id":null,"dir":"Reference","previous_headings":"","what":"Get All Open Stage Transition Requests for the Model Version — db_mlflow_model_open_transition_reqs","title":"Get All Open Stage Transition Requests for the Model Version — db_mlflow_model_open_transition_reqs","text":"Get Open Stage Transition Requests Model Version","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_mlflow_model_open_transition_reqs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get All Open Stage Transition Requests for the Model Version — db_mlflow_model_open_transition_reqs","text":"","code":"db_mlflow_model_open_transition_reqs(   name,   version,   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_mlflow_model_open_transition_reqs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get All Open Stage Transition Requests for the Model Version — db_mlflow_model_open_transition_reqs","text":"name Name model. version Version model. host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_mlflow_model_reject_transition_req.html","id":null,"dir":"Reference","previous_headings":"","what":"Reject Model Version Stage Transition Request — db_mlflow_model_reject_transition_req","title":"Reject Model Version Stage Transition Request — db_mlflow_model_reject_transition_req","text":"Reject Model Version Stage Transition Request","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_mlflow_model_reject_transition_req.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Reject Model Version Stage Transition Request — db_mlflow_model_reject_transition_req","text":"","code":"db_mlflow_model_reject_transition_req(   name,   version,   stage = c(\"None\", \"Staging\", \"Production\", \"Archived\"),   comment = NULL,   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_mlflow_model_reject_transition_req.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Reject Model Version Stage Transition Request — db_mlflow_model_reject_transition_req","text":"name Name model. version Version model. stage Target stage transition. Valid values : None, Staging, Production, Archived. comment User-provided comment action. host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_mlflow_model_transition_req.html","id":null,"dir":"Reference","previous_headings":"","what":"Make a Model Version Stage Transition Request — db_mlflow_model_transition_req","title":"Make a Model Version Stage Transition Request — db_mlflow_model_transition_req","text":"Make Model Version Stage Transition Request","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_mlflow_model_transition_req.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Make a Model Version Stage Transition Request — db_mlflow_model_transition_req","text":"","code":"db_mlflow_model_transition_req(   name,   version,   stage = c(\"None\", \"Staging\", \"Production\", \"Archived\"),   comment = NULL,   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_mlflow_model_transition_req.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Make a Model Version Stage Transition Request — db_mlflow_model_transition_req","text":"name Name model. version Version model. stage Target stage transition. Valid values : None, Staging, Production, Archived. comment User-provided comment action. host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_mlflow_model_transition_stage.html","id":null,"dir":"Reference","previous_headings":"","what":"Transition a Model Version's Stage — db_mlflow_model_transition_stage","title":"Transition a Model Version's Stage — db_mlflow_model_transition_stage","text":"Transition Model Version's Stage","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_mlflow_model_transition_stage.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Transition a Model Version's Stage — db_mlflow_model_transition_stage","text":"","code":"db_mlflow_model_transition_stage(   name,   version,   stage = c(\"None\", \"Staging\", \"Production\", \"Archived\"),   archive_existing_versions = TRUE,   comment = NULL,   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_mlflow_model_transition_stage.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Transition a Model Version's Stage — db_mlflow_model_transition_stage","text":"name Name model. version Version model. stage Target stage transition. Valid values : None, Staging, Production, Archived. archive_existing_versions Boolean (Default: TRUE). Specifies whether archive current model versions target stage. comment User-provided comment action. host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_mlflow_model_transition_stage.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Transition a Model Version's Stage — db_mlflow_model_transition_stage","text":"Databricks version MLflow endpoint also accepts comment associated transition recorded.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_mlflow_model_version_comment.html","id":null,"dir":"Reference","previous_headings":"","what":"Make a Comment on a Model Version — db_mlflow_model_version_comment","title":"Make a Comment on a Model Version — db_mlflow_model_version_comment","text":"Make Comment Model Version","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_mlflow_model_version_comment.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Make a Comment on a Model Version — db_mlflow_model_version_comment","text":"","code":"db_mlflow_model_version_comment(   name,   version,   comment,   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_mlflow_model_version_comment.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Make a Comment on a Model Version — db_mlflow_model_version_comment","text":"name Name model. version Version model. comment User-provided comment action. host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_mlflow_model_version_comment_delete.html","id":null,"dir":"Reference","previous_headings":"","what":"Delete a Comment on a Model Version — db_mlflow_model_version_comment_delete","title":"Delete a Comment on a Model Version — db_mlflow_model_version_comment_delete","text":"Delete Comment Model Version","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_mlflow_model_version_comment_delete.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Delete a Comment on a Model Version — db_mlflow_model_version_comment_delete","text":"","code":"db_mlflow_model_version_comment_delete(   id,   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_mlflow_model_version_comment_delete.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Delete a Comment on a Model Version — db_mlflow_model_version_comment_delete","text":"id Unique identifier activity. host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_mlflow_model_version_comment_edit.html","id":null,"dir":"Reference","previous_headings":"","what":"Edit a Comment on a Model Version — db_mlflow_model_version_comment_edit","title":"Edit a Comment on a Model Version — db_mlflow_model_version_comment_edit","text":"Edit Comment Model Version","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_mlflow_model_version_comment_edit.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Edit a Comment on a Model Version — db_mlflow_model_version_comment_edit","text":"","code":"db_mlflow_model_version_comment_edit(   id,   comment,   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_mlflow_model_version_comment_edit.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Edit a Comment on a Model Version — db_mlflow_model_version_comment_edit","text":"id Unique identifier activity. comment User-provided comment action. host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_mlflow_registered_model_details.html","id":null,"dir":"Reference","previous_headings":"","what":"Get Registered Model Details — db_mlflow_registered_model_details","title":"Get Registered Model Details — db_mlflow_registered_model_details","text":"Get Registered Model Details","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_mlflow_registered_model_details.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get Registered Model Details — db_mlflow_registered_model_details","text":"","code":"db_mlflow_registered_model_details(   name,   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_mlflow_registered_model_details.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get Registered Model Details — db_mlflow_registered_model_details","text":"name Name model. host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_oauth_client.html","id":null,"dir":"Reference","previous_headings":"","what":"Create OAuth 2.0 Client — db_oauth_client","title":"Create OAuth 2.0 Client — db_oauth_client","text":"Create OAuth 2.0 Client","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_oauth_client.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create OAuth 2.0 Client — db_oauth_client","text":"","code":"db_oauth_client(host = db_host())"},{"path":"https://databrickslabs.github.io/brickster/reference/db_oauth_client.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create OAuth 2.0 Client — db_oauth_client","text":"host Databricks workspace URL, defaults calling db_host().","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_oauth_client.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create OAuth 2.0 Client — db_oauth_client","text":"List contains httr2_oauth_client relevant auth url","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_oauth_client.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create OAuth 2.0 Client — db_oauth_client","text":"Creates OAuth 2.0 Client, support U2M flows . May later extended account U2M M2M flows.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_perform_request.html","id":null,"dir":"Reference","previous_headings":"","what":"Perform Databricks API Request — db_perform_request","title":"Perform Databricks API Request — db_perform_request","text":"Perform Databricks API Request","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_perform_request.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Perform Databricks API Request — db_perform_request","text":"","code":"db_perform_request(req, ...)"},{"path":"https://databrickslabs.github.io/brickster/reference/db_perform_request.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Perform Databricks API Request — db_perform_request","text":"req {httr2} request. ... Parameters passed httr2::resp_body_json()","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_query_create.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a SQL Query — db_query_create","title":"Create a SQL Query — db_query_create","text":"Create SQL Query","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_query_create.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a SQL Query — db_query_create","text":"","code":"db_query_create(   warehouse_id,   query_text,   display_name,   description = NULL,   catalog = NULL,   schema = NULL,   parent_path = NULL,   run_as_mode = c(\"OWNER\", \"VIEWER\"),   apply_auto_limit = FALSE,   auto_resolve_display_name = TRUE,   tags = list(),   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_query_create.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a SQL Query — db_query_create","text":"warehouse_id description query_text Text query run. display_name Display name query appears list views, widget headings, query page. description General description conveys additional information query usage notes. catalog Name catalog query executed. schema Name schema query executed. parent_path Workspace path workspace folder containing object. run_as_mode Sets \"Run \" role object. apply_auto_limit Whether apply 1000 row limit query result. auto_resolve_display_name Automatically resolve query display name conflicts. Otherwise, fail request query's display name conflicts existing query's display name. tags Named list describes warehouse. Databricks tags warehouse resources tags. host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_query_delete.html","id":null,"dir":"Reference","previous_headings":"","what":"Delete a SQL Query — db_query_delete","title":"Delete a SQL Query — db_query_delete","text":"Delete SQL Query","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_query_delete.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Delete a SQL Query — db_query_delete","text":"","code":"db_query_delete(   id,   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_query_delete.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Delete a SQL Query — db_query_delete","text":"id String, ID query. host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_query_delete.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Delete a SQL Query — db_query_delete","text":"Moves query trash. Trashed queries immediately disappear searches list views, used alerts. can restore trashed query UI. trashed query permanently deleted 30 days.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_query_get.html","id":null,"dir":"Reference","previous_headings":"","what":"Get a SQL Query — db_query_get","title":"Get a SQL Query — db_query_get","text":"Returns repo given repo ID.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_query_get.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get a SQL Query — db_query_get","text":"","code":"db_query_get(id, host = db_host(), token = db_token(), perform_request = TRUE)"},{"path":"https://databrickslabs.github.io/brickster/reference/db_query_get.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get a SQL Query — db_query_get","text":"id String, ID query. host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_query_list.html","id":null,"dir":"Reference","previous_headings":"","what":"List SQL Queries — db_query_list","title":"List SQL Queries — db_query_list","text":"List SQL Queries","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_query_list.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List SQL Queries — db_query_list","text":"","code":"db_query_list(   page_size = 20,   page_token = NULL,   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_query_list.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List SQL Queries — db_query_list","text":"page_size Integer, number results return request. page_token Token used get next page results. specified, returns first page results well next page token results. host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_query_list.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"List SQL Queries — db_query_list","text":"Gets list queries accessible user, ordered creation time. Warning: Calling API concurrently 10 times result throttling, service degradation, temporary ban.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_query_update.html","id":null,"dir":"Reference","previous_headings":"","what":"Update a SQL Query — db_query_update","title":"Update a SQL Query — db_query_update","text":"Update SQL Query","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_query_update.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Update a SQL Query — db_query_update","text":"","code":"db_query_update(   id,   warehouse_id = NULL,   query_text = NULL,   display_name = NULL,   description = NULL,   catalog = NULL,   schema = NULL,   parent_path = NULL,   run_as_mode = NULL,   apply_auto_limit = NULL,   auto_resolve_display_name = NULL,   tags = NULL,   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_query_update.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Update a SQL Query — db_query_update","text":"id Query id warehouse_id description query_text Text query run. display_name Display name query appears list views, widget headings, query page. description General description conveys additional information query usage notes. catalog Name catalog query executed. schema Name schema query executed. parent_path Workspace path workspace folder containing object. run_as_mode Sets \"Run \" role object. apply_auto_limit Whether apply 1000 row limit query result. auto_resolve_display_name Automatically resolve query display name conflicts. Otherwise, fail request query's display name conflicts existing query's display name. tags Named list describes warehouse. Databricks tags warehouse resources tags. host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_read_netrc.html","id":null,"dir":"Reference","previous_headings":"","what":"Read .netrc File — db_read_netrc","title":"Read .netrc File — db_read_netrc","text":"Read .netrc File","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_read_netrc.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Read .netrc File — db_read_netrc","text":"","code":"db_read_netrc(path = \"~/.netrc\")"},{"path":"https://databrickslabs.github.io/brickster/reference/db_read_netrc.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Read .netrc File — db_read_netrc","text":"path path .netrc file, default ~/.netrc.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_read_netrc.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Read .netrc File — db_read_netrc","text":"named list .netrc entries","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_repl.html","id":null,"dir":"Reference","previous_headings":"","what":"Remote REPL to Databricks Cluster — db_repl","title":"Remote REPL to Databricks Cluster — db_repl","text":"Remote REPL Databricks Cluster","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_repl.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Remote REPL to Databricks Cluster — db_repl","text":"","code":"db_repl(   cluster_id,   language = c(\"r\", \"py\", \"scala\", \"sql\", \"sh\"),   host = db_host(),   token = db_token() )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_repl.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Remote REPL to Databricks Cluster — db_repl","text":"cluster_id Cluster Id create REPL context . language REPL ('r', 'py', 'scala', 'sql', 'sh') supported. host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token().","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_repl.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Remote REPL to Databricks Cluster — db_repl","text":"db_repl() take existing console allow execution commands Databricks cluster. RStudio users Addins can bound keyboard shortcuts improve usability.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_repo_create.html","id":null,"dir":"Reference","previous_headings":"","what":"Create Repo — db_repo_create","title":"Create Repo — db_repo_create","text":"Creates repo workspace links remote Git repo specified.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_repo_create.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create Repo — db_repo_create","text":"","code":"db_repo_create(   url,   provider,   path,   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_repo_create.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create Repo — db_repo_create","text":"url URL Git repository linked. provider Git provider. field case-insensitive. available Git providers gitHub, bitbucketCloud, gitLab, azureDevOpsServices, gitHubEnterprise, bitbucketServer gitLabEnterpriseEdition. path Desired path repo workspace. Must format /Repos/{folder}/{repo-name}. host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_repo_delete.html","id":null,"dir":"Reference","previous_headings":"","what":"Delete Repo — db_repo_delete","title":"Delete Repo — db_repo_delete","text":"Deletes specified repo","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_repo_delete.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Delete Repo — db_repo_delete","text":"","code":"db_repo_delete(   repo_id,   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_repo_delete.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Delete Repo — db_repo_delete","text":"repo_id ID corresponding repo access. host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_repo_get.html","id":null,"dir":"Reference","previous_headings":"","what":"Get Repo — db_repo_get","title":"Get Repo — db_repo_get","text":"Returns repo given repo ID.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_repo_get.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get Repo — db_repo_get","text":"","code":"db_repo_get(   repo_id,   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_repo_get.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get Repo — db_repo_get","text":"repo_id ID corresponding repo access. host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_repo_get_all.html","id":null,"dir":"Reference","previous_headings":"","what":"Get All Repos — db_repo_get_all","title":"Get All Repos — db_repo_get_all","text":"Get Repos","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_repo_get_all.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get All Repos — db_repo_get_all","text":"","code":"db_repo_get_all(   path_prefix,   next_page_token = NULL,   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_repo_get_all.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get All Repos — db_repo_get_all","text":"path_prefix Filters repos paths starting given path prefix. next_page_token Token used get next page results. specified, returns first page results well next page token results. host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_repo_get_all.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get All Repos — db_repo_get_all","text":"Returns repos calling user Manage permissions . Results paginated page containing twenty repos.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_repo_update.html","id":null,"dir":"Reference","previous_headings":"","what":"Update Repo — db_repo_update","title":"Update Repo — db_repo_update","text":"Updates repo given branch tag.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_repo_update.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Update Repo — db_repo_update","text":"","code":"db_repo_update(   repo_id,   branch = NULL,   tag = NULL,   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_repo_update.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Update Repo — db_repo_update","text":"repo_id ID corresponding repo access. branch Branch local version repo checked . tag Tag local version repo checked . host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_repo_update.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Update Repo — db_repo_update","text":"Specify either branch tag, . Updating repo tag puts repo detached HEAD state. committing new changes, must update repo branch instead detached HEAD.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_req_error_body.html","id":null,"dir":"Reference","previous_headings":"","what":"Propagate Databricks API Errors — db_req_error_body","title":"Propagate Databricks API Errors — db_req_error_body","text":"Propagate Databricks API Errors","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_req_error_body.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Propagate Databricks API Errors — db_req_error_body","text":"","code":"db_req_error_body(resp)"},{"path":"https://databrickslabs.github.io/brickster/reference/db_req_error_body.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Propagate Databricks API Errors — db_req_error_body","text":"resp Object class httr2_response.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_request.html","id":null,"dir":"Reference","previous_headings":"","what":"Databricks Request Helper — db_request","title":"Databricks Request Helper — db_request","text":"Databricks Request Helper","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_request.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Databricks Request Helper — db_request","text":"","code":"db_request(endpoint, method, version = NULL, body = NULL, host, token, ...)"},{"path":"https://databrickslabs.github.io/brickster/reference/db_request.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Databricks Request Helper — db_request","text":"endpoint Databricks REST API Endpoint method Passed httr2::req_method() version String, API version endpoint. E.g. 2.0. body Named list, passed httr2::req_body_json(). host Databricks host, defaults db_host(). token Databricks token, defaults db_token(). ... Parameters passed httr2::req_body_json() body NULL.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_request.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Databricks Request Helper — db_request","text":"request","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_request_json.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate Request JSON — db_request_json","title":"Generate Request JSON — db_request_json","text":"Generate Request JSON","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_request_json.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate Request JSON — db_request_json","text":"","code":"db_request_json(req)"},{"path":"https://databrickslabs.github.io/brickster/reference/db_request_json.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate Request JSON — db_request_json","text":"req httr2 request, ideally db_request().","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_request_json.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate Request JSON — db_request_json","text":"JSON string","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_secrets_delete.html","id":null,"dir":"Reference","previous_headings":"","what":"Delete Secret in Secret Scope — db_secrets_delete","title":"Delete Secret in Secret Scope — db_secrets_delete","text":"Delete Secret Secret Scope","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_secrets_delete.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Delete Secret in Secret Scope — db_secrets_delete","text":"","code":"db_secrets_delete(   scope,   key,   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_secrets_delete.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Delete Secret in Secret Scope — db_secrets_delete","text":"scope Name scope contains secret delete. key Name secret delete. host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_secrets_delete.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Delete Secret in Secret Scope — db_secrets_delete","text":"must WRITE MANAGE permission secret scope. Throws RESOURCE_DOES_NOT_EXIST secret scope secret exists. Throws PERMISSION_DENIED permission make API call.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_secrets_list.html","id":null,"dir":"Reference","previous_headings":"","what":"List Secrets in Secret Scope — db_secrets_list","title":"List Secrets in Secret Scope — db_secrets_list","text":"List Secrets Secret Scope","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_secrets_list.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List Secrets in Secret Scope — db_secrets_list","text":"","code":"db_secrets_list(   scope,   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_secrets_list.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List Secrets in Secret Scope — db_secrets_list","text":"scope Name scope whose secrets want list host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_secrets_list.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"List Secrets in Secret Scope — db_secrets_list","text":"metadata-operation; retrieve secret data using API. must READ permission make call. last_updated_timestamp returned milliseconds since epoch. Throws RESOURCE_DOES_NOT_EXIST secret scope exists. Throws PERMISSION_DENIED permission make API call.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_secrets_put.html","id":null,"dir":"Reference","previous_headings":"","what":"Put Secret in Secret Scope — db_secrets_put","title":"Put Secret in Secret Scope — db_secrets_put","text":"Insert secret provided scope given name.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_secrets_put.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Put Secret in Secret Scope — db_secrets_put","text":"","code":"db_secrets_put(   scope,   key,   value,   as_bytes = FALSE,   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_secrets_put.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Put Secret in Secret Scope — db_secrets_put","text":"scope Name scope secret associated key Unique name identify secret. value Contents secret store, must string. as_bytes Boolean (default: FALSE). Determines value stored bytes. host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_secrets_put.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Put Secret in Secret Scope — db_secrets_put","text":"secret already exists name, command overwrites existing secret’s value. server encrypts secret using secret scope’s encryption settings storing . must WRITE MANAGE permission secret scope. secret key must consist alphanumeric characters, dashes, underscores, periods, exceed 128 characters. maximum allowed secret value size 128 KB. maximum number secrets given scope 1000. can read secret value within command cluster (example, notebook); API read secret value outside cluster. permission applied based invoking command must least READ permission. input fields string_value bytes_value specify type secret, determine value returned secret value requested. Exactly one must specified, function interfaces parameters via as_bytes defaults FALSE. Throws RESOURCE_DOES_NOT_EXIST secret scope exists. Throws RESOURCE_LIMIT_EXCEEDED maximum number secrets scope exceeded. Throws INVALID_PARAMETER_VALUE key name value length invalid. Throws PERMISSION_DENIED user permission make API call.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_secrets_scope_acl_delete.html","id":null,"dir":"Reference","previous_headings":"","what":"Delete Secret Scope ACL — db_secrets_scope_acl_delete","title":"Delete Secret Scope ACL — db_secrets_scope_acl_delete","text":"Delete given ACL given scope.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_secrets_scope_acl_delete.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Delete Secret Scope ACL — db_secrets_scope_acl_delete","text":"","code":"db_secrets_scope_acl_delete(   scope,   principal,   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_secrets_scope_acl_delete.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Delete Secret Scope ACL — db_secrets_scope_acl_delete","text":"scope Name scope remove permissions. principal Principal remove existing ACL. host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_secrets_scope_acl_delete.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Delete Secret Scope ACL — db_secrets_scope_acl_delete","text":"must MANAGE permission invoke API. Throws RESOURCE_DOES_NOT_EXIST secret scope, principal, ACL exists. Throws PERMISSION_DENIED permission make API call.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_secrets_scope_acl_get.html","id":null,"dir":"Reference","previous_headings":"","what":"Get Secret Scope ACL — db_secrets_scope_acl_get","title":"Get Secret Scope ACL — db_secrets_scope_acl_get","text":"Get Secret Scope ACL","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_secrets_scope_acl_get.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get Secret Scope ACL — db_secrets_scope_acl_get","text":"","code":"db_secrets_scope_acl_get(   scope,   principal,   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_secrets_scope_acl_get.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get Secret Scope ACL — db_secrets_scope_acl_get","text":"scope Name scope fetch ACL information . principal Principal fetch ACL information . host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_secrets_scope_acl_get.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get Secret Scope ACL — db_secrets_scope_acl_get","text":"must MANAGE permission invoke Throws RESOURCE_DOES_NOT_EXIST secret scope exists. Throws PERMISSION_DENIED permission make API call.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_secrets_scope_acl_list.html","id":null,"dir":"Reference","previous_headings":"","what":"List Secret Scope ACL's — db_secrets_scope_acl_list","title":"List Secret Scope ACL's — db_secrets_scope_acl_list","text":"List Secret Scope ACL's","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_secrets_scope_acl_list.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List Secret Scope ACL's — db_secrets_scope_acl_list","text":"","code":"db_secrets_scope_acl_list(   scope,   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_secrets_scope_acl_list.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List Secret Scope ACL's — db_secrets_scope_acl_list","text":"scope Name scope fetch ACL information . host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_secrets_scope_acl_list.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"List Secret Scope ACL's — db_secrets_scope_acl_list","text":"must MANAGE permission invoke API. Throws RESOURCE_DOES_NOT_EXIST secret scope exists. Throws PERMISSION_DENIED permission make API call.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_secrets_scope_acl_put.html","id":null,"dir":"Reference","previous_headings":"","what":"Put ACL on Secret Scope — db_secrets_scope_acl_put","title":"Put ACL on Secret Scope — db_secrets_scope_acl_put","text":"Put ACL Secret Scope","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_secrets_scope_acl_put.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Put ACL on Secret Scope — db_secrets_scope_acl_put","text":"","code":"db_secrets_scope_acl_put(   scope,   principal,   permission = c(\"READ\", \"WRITE\", \"MANAGE\"),   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_secrets_scope_acl_put.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Put ACL on Secret Scope — db_secrets_scope_acl_put","text":"scope Name scope apply permissions. principal Principal permission applied permission Permission level applied principal. One READ, WRITE, MANAGE. host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_secrets_scope_acl_put.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Put ACL on Secret Scope — db_secrets_scope_acl_put","text":"Create overwrite ACL associated given principal (user group) specified scope point. general, user group use powerful permission available , permissions ordered follows: MANAGE - Allowed change ACLs, read write secret scope. WRITE - Allowed read write secret scope. READ - Allowed read secret scope list secrets available. must MANAGE permission invoke API. principal user group name corresponding existing Databricks principal granted revoked access. Throws RESOURCE_DOES_NOT_EXIST secret scope exists. Throws RESOURCE_ALREADY_EXISTS permission principal already exists. Throws INVALID_PARAMETER_VALUE permission invalid. Throws PERMISSION_DENIED permission make API call.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_secrets_scope_create.html","id":null,"dir":"Reference","previous_headings":"","what":"Create Secret Scope — db_secrets_scope_create","title":"Create Secret Scope — db_secrets_scope_create","text":"Create Secret Scope","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_secrets_scope_create.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create Secret Scope — db_secrets_scope_create","text":"","code":"db_secrets_scope_create(   scope,   initial_manage_principal = NULL,   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_secrets_scope_create.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create Secret Scope — db_secrets_scope_create","text":"scope Scope name requested user. Scope names unique. initial_manage_principal principal initially granted MANAGE permission created scope. host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_secrets_scope_create.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create Secret Scope — db_secrets_scope_create","text":"Create Databricks-backed secret scope secrets stored Databricks-managed storage encrypted cloud-based specific encryption key. scope name: Must unique within workspace. Must consist alphanumeric characters, dashes, underscores, periods, may exceed 128 characters. names considered non-sensitive readable users workspace. workspace limited maximum 100 secret scopes. initial_manage_principal specified, initial ACL applied scope applied supplied principal (user group) MANAGE permissions. supported principal option group users, contains users workspace. initial_manage_principal specified, initial ACL MANAGE permission applied scope assigned API request issuer’s user identity. Throws RESOURCE_ALREADY_EXISTS scope given name already exists. Throws RESOURCE_LIMIT_EXCEEDED maximum number scopes workspace exceeded. Throws INVALID_PARAMETER_VALUE scope name invalid.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_secrets_scope_delete.html","id":null,"dir":"Reference","previous_headings":"","what":"Delete Secret Scope — db_secrets_scope_delete","title":"Delete Secret Scope — db_secrets_scope_delete","text":"Delete Secret Scope","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_secrets_scope_delete.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Delete Secret Scope — db_secrets_scope_delete","text":"","code":"db_secrets_scope_delete(   scope,   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_secrets_scope_delete.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Delete Secret Scope — db_secrets_scope_delete","text":"scope Name scope delete. host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_secrets_scope_delete.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Delete Secret Scope — db_secrets_scope_delete","text":"Throws RESOURCE_DOES_NOT_EXIST scope exist. Throws PERMISSION_DENIED user permission make API call.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_secrets_scope_list_all.html","id":null,"dir":"Reference","previous_headings":"","what":"List Secret Scopes — db_secrets_scope_list_all","title":"List Secret Scopes — db_secrets_scope_list_all","text":"List Secret Scopes","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_secrets_scope_list_all.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List Secret Scopes — db_secrets_scope_list_all","text":"","code":"db_secrets_scope_list_all(   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_secrets_scope_list_all.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List Secret Scopes — db_secrets_scope_list_all","text":"host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_secrets_scope_list_all.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"List Secret Scopes — db_secrets_scope_list_all","text":"Throws PERMISSION_DENIED permission make API call.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_should_use_volume_method.html","id":null,"dir":"Reference","previous_headings":"","what":"Check if volume method should be used — db_should_use_volume_method","title":"Check if volume method should be used — db_should_use_volume_method","text":"Check volume method used","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_should_use_volume_method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check if volume method should be used — db_should_use_volume_method","text":"","code":"db_should_use_volume_method(value, staging_volume, temporary = FALSE)"},{"path":"https://databrickslabs.github.io/brickster/reference/db_sql_create_empty_result.html","id":null,"dir":"Reference","previous_headings":"","what":"Create Empty Data Frame from Query Manifest — db_sql_create_empty_result","title":"Create Empty Data Frame from Query Manifest — db_sql_create_empty_result","text":"Helper function creates empty data frame proper column types based query result manifest schema. Used query returns zero rows.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_sql_create_empty_result.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create Empty Data Frame from Query Manifest — db_sql_create_empty_result","text":"","code":"db_sql_create_empty_result(manifest)"},{"path":"https://databrickslabs.github.io/brickster/reference/db_sql_create_empty_result.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create Empty Data Frame from Query Manifest — db_sql_create_empty_result","text":"manifest Query result manifest containing schema information","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_sql_create_empty_result.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create Empty Data Frame from Query Manifest — db_sql_create_empty_result","text":"tibble zero rows correct column types","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_sql_exec_and_wait.html","id":null,"dir":"Reference","previous_headings":"","what":"Execute SQL Query and Wait for Completion — db_sql_exec_and_wait","title":"Execute SQL Query and Wait for Completion — db_sql_exec_and_wait","text":"Internal helper executes query waits completion. separates execution/polling logic result fetching.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_sql_exec_and_wait.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Execute SQL Query and Wait for Completion — db_sql_exec_and_wait","text":"","code":"db_sql_exec_and_wait(   warehouse_id,   statement,   catalog = NULL,   schema = NULL,   parameters = NULL,   row_limit = NULL,   byte_limit = NULL,   wait_timeout = \"0s\",   disposition = c(\"EXTERNAL_LINKS\", \"INLINE\"),   format = c(\"ARROW_STREAM\", \"JSON_ARRAY\"),   host = db_host(),   token = db_token(),   show_progress = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_sql_exec_and_wait.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Execute SQL Query and Wait for Completion — db_sql_exec_and_wait","text":"warehouse_id String, ID warehouse upon execute statement. statement String, SQL statement execute. statement can optionally parameterized, see parameters. catalog String, sets default catalog statement execution, similar USE CATALOG SQL. schema String, sets default schema statement execution, similar USE SCHEMA SQL. parameters List Named Lists, parameters pass SQL statement containing parameter markers. parameter consists name, value, optionally type. represent NULL value, value field may omitted set NULL explicitly. See docs details. row_limit Integer, applies given row limit statement's result set, unlike LIMIT clause SQL, also sets truncated field response indicate whether result trimmed due limit . byte_limit Integer, applies given byte limit statement's result size. Byte counts based internal data representations might match final size requested format. result truncated due byte limit, truncated response set true. using EXTERNAL_LINKS disposition, default byte_limit 100 GiB applied byte_limit explicitly set. wait_timeout Initial wait timeout (default \"30s\") disposition One \"INLINE\" (default) \"EXTERNAL_LINKS\". See docs details. format One \"JSON_ARRAY\" (default), \"ARROW_STREAM\", \"CSV\". See docs details. host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token().","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_sql_exec_and_wait.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Execute SQL Query and Wait for Completion — db_sql_exec_and_wait","text":"Status response manifest query completes successfully","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_sql_exec_cancel.html","id":null,"dir":"Reference","previous_headings":"","what":"Cancel SQL Query — db_sql_exec_cancel","title":"Cancel SQL Query — db_sql_exec_cancel","text":"Cancel SQL Query","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_sql_exec_cancel.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Cancel SQL Query — db_sql_exec_cancel","text":"","code":"db_sql_exec_cancel(   statement_id,   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_sql_exec_cancel.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Cancel SQL Query — db_sql_exec_cancel","text":"statement_id String, query execution statement_id host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_sql_exec_cancel.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Cancel SQL Query — db_sql_exec_cancel","text":"Requests executing statement canceled. Callers must poll status see terminal state. Read Databricks API docs","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_sql_exec_poll_for_success.html","id":null,"dir":"Reference","previous_headings":"","what":"Poll a Query Until Successful — db_sql_exec_poll_for_success","title":"Poll a Query Until Successful — db_sql_exec_poll_for_success","text":"Poll Query Successful","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_sql_exec_poll_for_success.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Poll a Query Until Successful — db_sql_exec_poll_for_success","text":"","code":"db_sql_exec_poll_for_success(statement_id, interval = 1, show_progress = TRUE)"},{"path":"https://databrickslabs.github.io/brickster/reference/db_sql_exec_poll_for_success.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Poll a Query Until Successful — db_sql_exec_poll_for_success","text":"statement_id String, query execution statement_id interval Number seconds status checks. show_progress TRUE, show progress updates polling (default: TRUE)","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_sql_exec_query.html","id":null,"dir":"Reference","previous_headings":"","what":"Execute SQL Query — db_sql_exec_query","title":"Execute SQL Query — db_sql_exec_query","text":"Execute SQL Query","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_sql_exec_query.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Execute SQL Query — db_sql_exec_query","text":"","code":"db_sql_exec_query(   statement,   warehouse_id,   catalog = NULL,   schema = NULL,   parameters = NULL,   row_limit = NULL,   byte_limit = NULL,   disposition = c(\"INLINE\", \"EXTERNAL_LINKS\"),   format = c(\"JSON_ARRAY\", \"ARROW_STREAM\", \"CSV\"),   wait_timeout = \"0s\",   on_wait_timeout = c(\"CONTINUE\", \"CANCEL\"),   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_sql_exec_query.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Execute SQL Query — db_sql_exec_query","text":"statement String, SQL statement execute. statement can optionally parameterized, see parameters. warehouse_id String, ID warehouse upon execute statement. catalog String, sets default catalog statement execution, similar USE CATALOG SQL. schema String, sets default schema statement execution, similar USE SCHEMA SQL. parameters List Named Lists, parameters pass SQL statement containing parameter markers. parameter consists name, value, optionally type. represent NULL value, value field may omitted set NULL explicitly. See docs details. row_limit Integer, applies given row limit statement's result set, unlike LIMIT clause SQL, also sets truncated field response indicate whether result trimmed due limit . byte_limit Integer, applies given byte limit statement's result size. Byte counts based internal data representations might match final size requested format. result truncated due byte limit, truncated response set true. using EXTERNAL_LINKS disposition, default byte_limit 100 GiB applied byte_limit explicitly set. disposition One \"INLINE\" (default) \"EXTERNAL_LINKS\". See docs details. format One \"JSON_ARRAY\" (default), \"ARROW_STREAM\", \"CSV\". See docs details. wait_timeout String, default \"10s\". time seconds call wait statement's result set Ns, N can set 0 value 5 50. set 0s, statement execute asynchronous mode call wait execution finish. case, call returns directly PENDING state statement ID can used polling db_sql_exec_status(). set 5 50 seconds, call behave synchronously timeout wait statement execution finish. execution finishes within time, call returns immediately manifest result data (FAILED state case execution error). statement takes longer execute, on_wait_timeout determines happen timeout reached. on_wait_timeout One \"CONTINUE\" (default) \"CANCEL\". wait_timeout > 0s, call block specified time. statement execution finish within time, on_wait_timeout determines whether execution continue canceled. set CONTINUE, statement execution continues asynchronously call returns statement ID can used polling db_sql_exec_status(). set CANCEL, statement execution canceled call returns CANCELED state. host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_sql_exec_query.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Execute SQL Query — db_sql_exec_query","text":"Refer web documentation detailed material interaction various parameters general recommendations","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_sql_exec_result.html","id":null,"dir":"Reference","previous_headings":"","what":"Get SQL Query Results — db_sql_exec_result","title":"Get SQL Query Results — db_sql_exec_result","text":"Get SQL Query Results","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_sql_exec_result.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get SQL Query Results — db_sql_exec_result","text":"","code":"db_sql_exec_result(   statement_id,   chunk_index,   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_sql_exec_result.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get SQL Query Results — db_sql_exec_result","text":"statement_id String, query execution statement_id chunk_index Integer, chunk index fetch result. Starts 0. host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_sql_exec_result.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get SQL Query Results — db_sql_exec_result","text":"statement execution SUCCEEDED, request can used fetch chunk index. Whereas first chunk chunk_index = 0 typically fetched db_sql_exec_result() db_sql_exec_status(), request can used fetch subsequent chunks response structure identical nested result element described db_sql_exec_result() request, similarly includes next_chunk_index next_chunk_internal_link fields simple iteration result set. Read Databricks API docs","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_sql_exec_status.html","id":null,"dir":"Reference","previous_headings":"","what":"Get SQL Query Status — db_sql_exec_status","title":"Get SQL Query Status — db_sql_exec_status","text":"Get SQL Query Status","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_sql_exec_status.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get SQL Query Status — db_sql_exec_status","text":"","code":"db_sql_exec_status(   statement_id,   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_sql_exec_status.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get SQL Query Status — db_sql_exec_status","text":"statement_id String, query execution statement_id host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_sql_exec_status.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get SQL Query Status — db_sql_exec_status","text":"request can used poll statement's status. status.state field SUCCEEDED also return result manifest first chunk result data. statement terminal states CANCELED, CLOSED FAILED, returns HTTP 200 state set. least 12 hours terminal state, statement removed warehouse calls receive HTTP 404 response. Read Databricks API docs","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_sql_fetch_results.html","id":null,"dir":"Reference","previous_headings":"","what":"Fetch SQL Query Results from Completed Query — db_sql_fetch_results","title":"Fetch SQL Query Results from Completed Query — db_sql_fetch_results","text":"Internal helper fetches processes results completed query. Handles Arrow stream processing data conversion.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_sql_fetch_results.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fetch SQL Query Results from Completed Query — db_sql_fetch_results","text":"","code":"db_sql_fetch_results(   statement_id,   manifest,   return_arrow = FALSE,   max_active_connections = 30,   row_limit = NULL,   host = db_host(),   token = db_token(),   show_progress = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_sql_fetch_results.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fetch SQL Query Results from Completed Query — db_sql_fetch_results","text":"statement_id Query statement ID manifest Query result manifest status response return_arrow Boolean, return arrow Table instead tibble max_active_connections Integer concurrent downloads row_limit Integer, limit number rows returned (applied fetch) host Databricks host token Databricks token show_progress TRUE, show progress updates result fetching (default: TRUE)","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_sql_fetch_results.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fetch SQL Query Results from Completed Query — db_sql_fetch_results","text":"tibble arrow Table query results","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_sql_global_warehouse_get.html","id":null,"dir":"Reference","previous_headings":"","what":"Get Global Warehouse Config — db_sql_global_warehouse_get","title":"Get Global Warehouse Config — db_sql_global_warehouse_get","text":"Get Global Warehouse Config","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_sql_global_warehouse_get.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get Global Warehouse Config — db_sql_global_warehouse_get","text":"","code":"db_sql_global_warehouse_get(   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_sql_global_warehouse_get.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get Global Warehouse Config — db_sql_global_warehouse_get","text":"host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_sql_process_inline.html","id":null,"dir":"Reference","previous_headings":"","what":"Process Inline SQL Query Results — db_sql_process_inline","title":"Process Inline SQL Query Results — db_sql_process_inline","text":"Internal helper processes inline JSON_ARRAY results completed query. Used metadata queries small result sets.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_sql_process_inline.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Process Inline SQL Query Results — db_sql_process_inline","text":"","code":"db_sql_process_inline(result_data, manifest, row_limit = NULL)"},{"path":"https://databrickslabs.github.io/brickster/reference/db_sql_process_inline.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Process Inline SQL Query Results — db_sql_process_inline","text":"result_data Result data inline query response manifest Query result manifest containing schema information row_limit Integer, limit number rows returned","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_sql_process_inline.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Process Inline SQL Query Results — db_sql_process_inline","text":"tibble query results","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_sql_query.html","id":null,"dir":"Reference","previous_headings":"","what":"Execute query with SQL Warehouse — db_sql_query","title":"Execute query with SQL Warehouse — db_sql_query","text":"Execute query SQL Warehouse","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_sql_query.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Execute query with SQL Warehouse — db_sql_query","text":"","code":"db_sql_query(   warehouse_id,   statement,   schema = NULL,   catalog = NULL,   parameters = NULL,   row_limit = NULL,   byte_limit = NULL,   return_arrow = FALSE,   max_active_connections = 30,   disposition = \"EXTERNAL_LINKS\",   host = db_host(),   token = db_token(),   show_progress = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_sql_query.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Execute query with SQL Warehouse — db_sql_query","text":"warehouse_id String, ID warehouse upon execute statement. statement String, SQL statement execute. statement can optionally parameterized, see parameters. schema String, sets default schema statement execution, similar USE SCHEMA SQL. catalog String, sets default catalog statement execution, similar USE CATALOG SQL. parameters List Named Lists, parameters pass SQL statement containing parameter markers. parameter consists name, value, optionally type. represent NULL value, value field may omitted set NULL explicitly. See docs details. row_limit Integer, applies given row limit statement's result set, unlike LIMIT clause SQL, also sets truncated field response indicate whether result trimmed due limit . byte_limit Integer, applies given byte limit statement's result size. Byte counts based internal data representations might match final size requested format. result truncated due byte limit, truncated response set true. using EXTERNAL_LINKS disposition, default byte_limit 100 GiB applied byte_limit explicitly set. return_arrow Boolean, determine result tibble::tibble arrow::Table. max_active_connections Integer decide concurrent downloads. disposition Disposition mode (\"INLINE\" \"EXTERNAL_LINKS\") host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). show_progress TRUE, show progress updates query execution (default: TRUE)","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_sql_query.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Execute query with SQL Warehouse — db_sql_query","text":"tibble::tibble arrow::Table.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_sql_query_history.html","id":null,"dir":"Reference","previous_headings":"","what":"List Warehouse Query History — db_sql_query_history","title":"List Warehouse Query History — db_sql_query_history","text":"details refer query history documentation. function elevates sub-components filter_by parameter R function directly.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_sql_query_history.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List Warehouse Query History — db_sql_query_history","text":"","code":"db_sql_query_history(   statuses = NULL,   user_ids = NULL,   endpoint_ids = NULL,   start_time_ms = NULL,   end_time_ms = NULL,   max_results = 100,   page_token = NULL,   include_metrics = FALSE,   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_sql_query_history.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List Warehouse Query History — db_sql_query_history","text":"statuses Allows filtering query status. Possible values : QUEUED, RUNNING, CANCELED, FAILED, FINISHED. Multiple permitted. user_ids Allows filtering user ID's. Multiple permitted. endpoint_ids Allows filtering endpoint ID's. Multiple permitted. start_time_ms Integer, limit results queries started time. end_time_ms Integer, limit results queries started time. max_results Limit number results returned one page. Default 100. page_token Opaque token used get next page results. Optional. include_metrics Whether include metrics query execution. host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_sql_query_history.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"List Warehouse Query History — db_sql_query_history","text":"default filter parameters statuses, user_ids, endpoints_ids NULL.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_sql_type_to_empty_vector.html","id":null,"dir":"Reference","previous_headings":"","what":"Create Empty R Vector from Databricks SQL Type — db_sql_type_to_empty_vector","title":"Create Empty R Vector from Databricks SQL Type — db_sql_type_to_empty_vector","text":"Internal helper maps Databricks SQL types appropriate empty R vectors. Used creating properly typed empty tibbles schema information.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_sql_type_to_empty_vector.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create Empty R Vector from Databricks SQL Type — db_sql_type_to_empty_vector","text":"","code":"db_sql_type_to_empty_vector(sql_type)"},{"path":"https://databrickslabs.github.io/brickster/reference/db_sql_type_to_empty_vector.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create Empty R Vector from Databricks SQL Type — db_sql_type_to_empty_vector","text":"sql_type Character string representing Databricks SQL type","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_sql_type_to_empty_vector.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create Empty R Vector from Databricks SQL Type — db_sql_type_to_empty_vector","text":"Empty R vector appropriate type","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_sql_warehouse_create.html","id":null,"dir":"Reference","previous_headings":"","what":"Create Warehouse — db_sql_warehouse_create","title":"Create Warehouse — db_sql_warehouse_create","text":"Create Warehouse","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_sql_warehouse_create.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create Warehouse — db_sql_warehouse_create","text":"","code":"db_sql_warehouse_create(   name,   cluster_size,   min_num_clusters = 1,   max_num_clusters = 1,   auto_stop_mins = 30,   tags = list(),   spot_instance_policy = c(\"COST_OPTIMIZED\", \"RELIABILITY_OPTIMIZED\"),   enable_photon = TRUE,   warehouse_type = c(\"CLASSIC\", \"PRO\"),   enable_serverless_compute = NULL,   disable_uc = FALSE,   channel = c(\"CHANNEL_NAME_CURRENT\", \"CHANNEL_NAME_PREVIEW\"),   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_sql_warehouse_create.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create Warehouse — db_sql_warehouse_create","text":"name Name SQL warehouse. Must unique. cluster_size Size clusters allocated warehouse. One 2X-Small, X-Small, Small, Medium, Large, X-Large, 2X-Large, 3X-Large, 4X-Large. min_num_clusters Minimum number clusters available SQL warehouse running. default 1. max_num_clusters Maximum number clusters available SQL warehouse running. multi-cluster load balancing enabled, limited 1. auto_stop_mins Time minutes idle SQL warehouse terminates clusters stops. Defaults 30. Serverless SQL warehouses (enable_serverless_compute = TRUE), set 10. tags Named list describes warehouse. Databricks tags warehouse resources tags. spot_instance_policy spot policy use allocating instances clusters. field used SQL warehouse Serverless SQL warehouse. enable_photon Whether queries executed native vectorized engine speeds query execution. default TRUE. warehouse_type Either \"CLASSIC\" (default), \"PRO\" enable_serverless_compute Whether SQL warehouse Serverless warehouse. use Serverless SQL warehouse, must enable Serverless SQL warehouses workspace. Serverless SQL warehouses disabled workspace, default FALSE Serverless SQL warehouses enabled workspace, default TRUE. disable_uc TRUE use Hive Metastore (HMS). FALSE (default), enabled Unity Catalog (UC). channel Whether use current SQL warehouse compute version preview version. Databricks recommend using preview versions production workloads. default CHANNEL_NAME_CURRENT. host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_sql_warehouse_delete.html","id":null,"dir":"Reference","previous_headings":"","what":"Delete Warehouse — db_sql_warehouse_delete","title":"Delete Warehouse — db_sql_warehouse_delete","text":"Delete Warehouse","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_sql_warehouse_delete.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Delete Warehouse — db_sql_warehouse_delete","text":"","code":"db_sql_warehouse_delete(   id,   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_sql_warehouse_delete.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Delete Warehouse — db_sql_warehouse_delete","text":"id ID SQL warehouse. host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_sql_warehouse_edit.html","id":null,"dir":"Reference","previous_headings":"","what":"Edit Warehouse — db_sql_warehouse_edit","title":"Edit Warehouse — db_sql_warehouse_edit","text":"Edit Warehouse","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_sql_warehouse_edit.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Edit Warehouse — db_sql_warehouse_edit","text":"","code":"db_sql_warehouse_edit(   id,   name = NULL,   cluster_size = NULL,   min_num_clusters = NULL,   max_num_clusters = NULL,   auto_stop_mins = NULL,   tags = NULL,   spot_instance_policy = NULL,   enable_photon = NULL,   warehouse_type = NULL,   enable_serverless_compute = NULL,   channel = NULL,   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_sql_warehouse_edit.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Edit Warehouse — db_sql_warehouse_edit","text":"id ID SQL warehouse. name Name SQL warehouse. Must unique. cluster_size Size clusters allocated warehouse. One 2X-Small, X-Small, Small, Medium, Large, X-Large, 2X-Large, 3X-Large, 4X-Large. min_num_clusters Minimum number clusters available SQL warehouse running. default 1. max_num_clusters Maximum number clusters available SQL warehouse running. multi-cluster load balancing enabled, limited 1. auto_stop_mins Time minutes idle SQL warehouse terminates clusters stops. Defaults 30. Serverless SQL warehouses (enable_serverless_compute = TRUE), set 10. tags Named list describes warehouse. Databricks tags warehouse resources tags. spot_instance_policy spot policy use allocating instances clusters. field used SQL warehouse Serverless SQL warehouse. enable_photon Whether queries executed native vectorized engine speeds query execution. default TRUE. warehouse_type Either \"CLASSIC\" (default), \"PRO\" enable_serverless_compute Whether SQL warehouse Serverless warehouse. use Serverless SQL warehouse, must enable Serverless SQL warehouses workspace. Serverless SQL warehouses disabled workspace, default FALSE Serverless SQL warehouses enabled workspace, default TRUE. channel Whether use current SQL warehouse compute version preview version. Databricks recommend using preview versions production workloads. default CHANNEL_NAME_CURRENT. host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_sql_warehouse_edit.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Edit Warehouse — db_sql_warehouse_edit","text":"Modify SQL warehouse. fields optional. Missing fields default current values.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_sql_warehouse_get.html","id":null,"dir":"Reference","previous_headings":"","what":"Get Warehouse — db_sql_warehouse_get","title":"Get Warehouse — db_sql_warehouse_get","text":"Get Warehouse","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_sql_warehouse_get.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get Warehouse — db_sql_warehouse_get","text":"","code":"db_sql_warehouse_get(   id,   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_sql_warehouse_get.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get Warehouse — db_sql_warehouse_get","text":"id ID SQL warehouse. host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_sql_warehouse_list.html","id":null,"dir":"Reference","previous_headings":"","what":"List Warehouses — db_sql_warehouse_list","title":"List Warehouses — db_sql_warehouse_list","text":"List Warehouses","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_sql_warehouse_list.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List Warehouses — db_sql_warehouse_list","text":"","code":"db_sql_warehouse_list(   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_sql_warehouse_list.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List Warehouses — db_sql_warehouse_list","text":"host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_sql_warehouse_start.html","id":null,"dir":"Reference","previous_headings":"","what":"Start Warehouse — db_sql_warehouse_start","title":"Start Warehouse — db_sql_warehouse_start","text":"Start Warehouse","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_sql_warehouse_start.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Start Warehouse — db_sql_warehouse_start","text":"","code":"db_sql_warehouse_start(   id,   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_sql_warehouse_start.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Start Warehouse — db_sql_warehouse_start","text":"id ID SQL warehouse. host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_sql_warehouse_stop.html","id":null,"dir":"Reference","previous_headings":"","what":"Stop Warehouse — db_sql_warehouse_stop","title":"Stop Warehouse — db_sql_warehouse_stop","text":"Stop Warehouse","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_sql_warehouse_stop.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Stop Warehouse — db_sql_warehouse_stop","text":"","code":"db_sql_warehouse_stop(   id,   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_sql_warehouse_stop.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Stop Warehouse — db_sql_warehouse_stop","text":"id ID SQL warehouse. host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_token.html","id":null,"dir":"Reference","previous_headings":"","what":"Fetch Databricks Token — db_token","title":"Fetch Databricks Token — db_token","text":"function check token DATABRICKS_HOST environment variable. .databrickscfg searched db_profile use_databrickscfg set Posit Workbench managed OAuth credentials detected. none found default using OAuth U2M flow. Refer api authentication docs","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_token.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fetch Databricks Token — db_token","text":"","code":"db_token(profile = default_config_profile())"},{"path":"https://databrickslabs.github.io/brickster/reference/db_token.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fetch Databricks Token — db_token","text":"profile Profile use fetching environment variable (e.g. .Renviron) .databricksfg file","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_token.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fetch Databricks Token — db_token","text":"databricks token","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_token.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Fetch Databricks Token — db_token","text":"behaviour subject change depending db_profile use_databrickscfg options set. use_databrickscfg: Boolean (default: FALSE), determines credentials fetched profile .databrickscfg .Renviron db_profile: String (default: NULL), determines profile used. .databrickscfg automatically used Posit Workbench managed OAuth credentials detected. See vignette authentication details.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_uc_catalogs_get.html","id":null,"dir":"Reference","previous_headings":"","what":"Get Catalog (Unity Catalog) — db_uc_catalogs_get","title":"Get Catalog (Unity Catalog) — db_uc_catalogs_get","text":"Get Catalog (Unity Catalog)","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_uc_catalogs_get.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get Catalog (Unity Catalog) — db_uc_catalogs_get","text":"","code":"db_uc_catalogs_get(   catalog,   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_uc_catalogs_get.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get Catalog (Unity Catalog) — db_uc_catalogs_get","text":"catalog name catalog. host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_uc_catalogs_get.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get Catalog (Unity Catalog) — db_uc_catalogs_get","text":"List","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_uc_catalogs_list.html","id":null,"dir":"Reference","previous_headings":"","what":"List Catalogs (Unity Catalog) — db_uc_catalogs_list","title":"List Catalogs (Unity Catalog) — db_uc_catalogs_list","text":"List Catalogs (Unity Catalog)","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_uc_catalogs_list.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List Catalogs (Unity Catalog) — db_uc_catalogs_list","text":"","code":"db_uc_catalogs_list(   max_results = 1000,   include_browse = TRUE,   page_token = NULL,   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_uc_catalogs_list.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List Catalogs (Unity Catalog) — db_uc_catalogs_list","text":"max_results Maximum number catalogs return (default: 1000). include_browse Whether include catalogs response principal can access selective metadata . page_token Opaque token used get next page results. Optional. host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_uc_catalogs_list.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"List Catalogs (Unity Catalog) — db_uc_catalogs_list","text":"List","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_uc_schemas_get.html","id":null,"dir":"Reference","previous_headings":"","what":"Get Schema (Unity Catalog) — db_uc_schemas_get","title":"Get Schema (Unity Catalog) — db_uc_schemas_get","text":"Get Schema (Unity Catalog)","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_uc_schemas_get.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get Schema (Unity Catalog) — db_uc_schemas_get","text":"","code":"db_uc_schemas_get(   catalog,   schema,   include_browse = TRUE,   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_uc_schemas_get.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get Schema (Unity Catalog) — db_uc_schemas_get","text":"catalog Parent catalog schema interest. schema Schema interest. include_browse Whether include catalogs response principal can access selective metadata . host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_uc_schemas_get.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get Schema (Unity Catalog) — db_uc_schemas_get","text":"List","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_uc_schemas_list.html","id":null,"dir":"Reference","previous_headings":"","what":"List Schemas (Unity Catalog) — db_uc_schemas_list","title":"List Schemas (Unity Catalog) — db_uc_schemas_list","text":"List Schemas (Unity Catalog)","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_uc_schemas_list.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List Schemas (Unity Catalog) — db_uc_schemas_list","text":"","code":"db_uc_schemas_list(   catalog,   max_results = 1000,   page_token = NULL,   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_uc_schemas_list.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List Schemas (Unity Catalog) — db_uc_schemas_list","text":"catalog Parent catalog schemas interest. max_results Maximum number schemas return (default: 1000). page_token Opaque token used get next page results. Optional. host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_uc_schemas_list.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"List Schemas (Unity Catalog) — db_uc_schemas_list","text":"List","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_uc_tables_delete.html","id":null,"dir":"Reference","previous_headings":"","what":"Delete Table (Unity Catalog) — db_uc_tables_delete","title":"Delete Table (Unity Catalog) — db_uc_tables_delete","text":"Delete Table (Unity Catalog)","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_uc_tables_delete.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Delete Table (Unity Catalog) — db_uc_tables_delete","text":"","code":"db_uc_tables_delete(   catalog,   schema,   table,   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_uc_tables_delete.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Delete Table (Unity Catalog) — db_uc_tables_delete","text":"catalog Parent catalog table. schema Parent schema table. table Table name. host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_uc_tables_delete.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Delete Table (Unity Catalog) — db_uc_tables_delete","text":"Boolean","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_uc_tables_exists.html","id":null,"dir":"Reference","previous_headings":"","what":"Check Table Exists (Unity Catalog) — db_uc_tables_exists","title":"Check Table Exists (Unity Catalog) — db_uc_tables_exists","text":"Check Table Exists (Unity Catalog)","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_uc_tables_exists.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check Table Exists (Unity Catalog) — db_uc_tables_exists","text":"","code":"db_uc_tables_exists(   catalog,   schema,   table,   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_uc_tables_exists.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check Table Exists (Unity Catalog) — db_uc_tables_exists","text":"catalog Parent catalog table. schema Parent schema table. table Table name. host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_uc_tables_exists.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check Table Exists (Unity Catalog) — db_uc_tables_exists","text":"List fields table_exists supports_foreign_metadata_update","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_uc_tables_get.html","id":null,"dir":"Reference","previous_headings":"","what":"Get Table (Unity Catalog) — db_uc_tables_get","title":"Get Table (Unity Catalog) — db_uc_tables_get","text":"Get Table (Unity Catalog)","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_uc_tables_get.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get Table (Unity Catalog) — db_uc_tables_get","text":"","code":"db_uc_tables_get(   catalog,   schema,   table,   omit_columns = TRUE,   omit_properties = TRUE,   omit_username = TRUE,   include_browse = TRUE,   include_delta_metadata = TRUE,   include_manifest_capabilities = FALSE,   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_uc_tables_get.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get Table (Unity Catalog) — db_uc_tables_get","text":"catalog Parent catalog table. schema Parent schema table. table Table name. omit_columns Whether omit columns table response . omit_properties Whether omit properties table response . omit_username Whether omit username table (e.g. owner, updated_by, created_by) response . include_browse Whether include tables response principal can access selective metadata . include_delta_metadata Whether delta metadata included response. include_manifest_capabilities Whether include manifest containing capabilities table . host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_uc_tables_get.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get Table (Unity Catalog) — db_uc_tables_get","text":"List","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_uc_tables_list.html","id":null,"dir":"Reference","previous_headings":"","what":"List Tables (Unity Catalog) — db_uc_tables_list","title":"List Tables (Unity Catalog) — db_uc_tables_list","text":"List Tables (Unity Catalog)","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_uc_tables_list.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List Tables (Unity Catalog) — db_uc_tables_list","text":"","code":"db_uc_tables_list(   catalog,   schema,   max_results = 50,   omit_columns = TRUE,   omit_properties = TRUE,   omit_username = TRUE,   include_browse = TRUE,   include_delta_metadata = FALSE,   include_manifest_capabilities = FALSE,   page_token = NULL,   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_uc_tables_list.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List Tables (Unity Catalog) — db_uc_tables_list","text":"catalog Name parent catalog tables interest. schema Parent schema tables. max_results Maximum number tables return (default: 50, max: 50). omit_columns Whether omit columns table response . omit_properties Whether omit properties table response . omit_username Whether omit username table (e.g. owner, updated_by, created_by) response . include_browse Whether include tables response principal can access selective metadata . include_delta_metadata Whether delta metadata included response. include_manifest_capabilities Whether include manifest containing capabilities table . page_token Opaque token used get next page results. Optional. host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_uc_tables_list.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"List Tables (Unity Catalog) — db_uc_tables_list","text":"List","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_uc_tables_summaries.html","id":null,"dir":"Reference","previous_headings":"","what":"List Table Summaries (Unity Catalog) — db_uc_tables_summaries","title":"List Table Summaries (Unity Catalog) — db_uc_tables_summaries","text":"List Table Summaries (Unity Catalog)","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_uc_tables_summaries.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List Table Summaries (Unity Catalog) — db_uc_tables_summaries","text":"","code":"db_uc_tables_summaries(   catalog,   schema_name_pattern = NULL,   table_name_pattern = NULL,   max_results = 10000,   include_manifest_capabilities = FALSE,   page_token = NULL,   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_uc_tables_summaries.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List Table Summaries (Unity Catalog) — db_uc_tables_summaries","text":"catalog Name parent catalog tables interest. schema_name_pattern sql LIKE pattern (% _) schema names. schemas returned set empty. table_name_pattern sql LIKE pattern (% _) table names. tables returned set empty. max_results Maximum number summaries tables return (default: 10000, max: 10000). set, page length set server configured value. include_manifest_capabilities Whether include manifest containing capabilities table . page_token Opaque token used get next page results. Optional. host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_uc_tables_summaries.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"List Table Summaries (Unity Catalog) — db_uc_tables_summaries","text":"List","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_uc_volumes_create.html","id":null,"dir":"Reference","previous_headings":"","what":"Update Volume (Unity Catalog) — db_uc_volumes_create","title":"Update Volume (Unity Catalog) — db_uc_volumes_create","text":"Update Volume (Unity Catalog)","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_uc_volumes_create.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Update Volume (Unity Catalog) — db_uc_volumes_create","text":"","code":"db_uc_volumes_create(   catalog,   schema,   volume,   volume_type = c(\"MANAGED\", \"EXTERNAL\"),   storage_location = NULL,   comment = NULL,   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_uc_volumes_create.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Update Volume (Unity Catalog) — db_uc_volumes_create","text":"catalog Parent catalog volume schema Parent schema volume volume Volume name. volume_type Either 'MANAGED' 'EXTERNAL'. storage_location storage location cloud, specified volume_type 'EXTERNAL'. comment comment attached volume. host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_uc_volumes_create.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Update Volume (Unity Catalog) — db_uc_volumes_create","text":"List","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_uc_volumes_delete.html","id":null,"dir":"Reference","previous_headings":"","what":"Delete Volume (Unity Catalog) — db_uc_volumes_delete","title":"Delete Volume (Unity Catalog) — db_uc_volumes_delete","text":"Delete Volume (Unity Catalog)","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_uc_volumes_delete.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Delete Volume (Unity Catalog) — db_uc_volumes_delete","text":"","code":"db_uc_volumes_delete(   catalog,   schema,   volume,   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_uc_volumes_delete.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Delete Volume (Unity Catalog) — db_uc_volumes_delete","text":"catalog Parent catalog volume schema Parent schema volume volume Volume name. host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_uc_volumes_delete.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Delete Volume (Unity Catalog) — db_uc_volumes_delete","text":"Boolean","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_uc_volumes_get.html","id":null,"dir":"Reference","previous_headings":"","what":"Get Volume (Unity Catalog) — db_uc_volumes_get","title":"Get Volume (Unity Catalog) — db_uc_volumes_get","text":"Get Volume (Unity Catalog)","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_uc_volumes_get.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get Volume (Unity Catalog) — db_uc_volumes_get","text":"","code":"db_uc_volumes_get(   catalog,   schema,   volume,   include_browse = TRUE,   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_uc_volumes_get.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get Volume (Unity Catalog) — db_uc_volumes_get","text":"catalog Parent catalog volume schema Parent schema volume volume Volume name. include_browse Whether include volumes response principal can access selective metadata . host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_uc_volumes_get.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get Volume (Unity Catalog) — db_uc_volumes_get","text":"List","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_uc_volumes_list.html","id":null,"dir":"Reference","previous_headings":"","what":"List Volumes (Unity Catalog) — db_uc_volumes_list","title":"List Volumes (Unity Catalog) — db_uc_volumes_list","text":"List Volumes (Unity Catalog)","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_uc_volumes_list.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List Volumes (Unity Catalog) — db_uc_volumes_list","text":"","code":"db_uc_volumes_list(   catalog,   schema,   max_results = 10000,   include_browse = TRUE,   page_token = NULL,   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_uc_volumes_list.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List Volumes (Unity Catalog) — db_uc_volumes_list","text":"catalog Parent catalog volume schema Parent schema volume max_results Maximum number volumes return (default: 10000). include_browse Whether include volumes response principal can access selective metadata . page_token Opaque token used get next page results. Optional. host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_uc_volumes_list.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"List Volumes (Unity Catalog) — db_uc_volumes_list","text":"List","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_uc_volumes_update.html","id":null,"dir":"Reference","previous_headings":"","what":"Update Volume (Unity Catalog) — db_uc_volumes_update","title":"Update Volume (Unity Catalog) — db_uc_volumes_update","text":"Update Volume (Unity Catalog)","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_uc_volumes_update.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Update Volume (Unity Catalog) — db_uc_volumes_update","text":"","code":"db_uc_volumes_update(   catalog,   schema,   volume,   owner = NULL,   comment = NULL,   new_name = NULL,   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_uc_volumes_update.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Update Volume (Unity Catalog) — db_uc_volumes_update","text":"catalog Parent catalog volume schema Parent schema volume volume Volume name. owner identifier user owns volume (Optional). comment comment attached volume (Optional). new_name New name volume (Optional). host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_uc_volumes_update.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Update Volume (Unity Catalog) — db_uc_volumes_update","text":"List","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_volume_delete.html","id":null,"dir":"Reference","previous_headings":"","what":"Volume FileSystem Delete — db_volume_delete","title":"Volume FileSystem Delete — db_volume_delete","text":"Volume FileSystem Delete","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_volume_delete.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Volume FileSystem Delete — db_volume_delete","text":"","code":"db_volume_delete(   path,   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_volume_delete.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Volume FileSystem Delete — db_volume_delete","text":"path Absolute path file Files API, omitting initial slash. host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_volume_dir_create.html","id":null,"dir":"Reference","previous_headings":"","what":"Volume FileSystem Create Directory — db_volume_dir_create","title":"Volume FileSystem Create Directory — db_volume_dir_create","text":"Volume FileSystem Create Directory","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_volume_dir_create.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Volume FileSystem Create Directory — db_volume_dir_create","text":"","code":"db_volume_dir_create(   path,   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_volume_dir_create.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Volume FileSystem Create Directory — db_volume_dir_create","text":"path Absolute path file Files API, omitting initial slash. host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_volume_dir_delete.html","id":null,"dir":"Reference","previous_headings":"","what":"Volume FileSystem Delete Directory — db_volume_dir_delete","title":"Volume FileSystem Delete Directory — db_volume_dir_delete","text":"Volume FileSystem Delete Directory","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_volume_dir_delete.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Volume FileSystem Delete Directory — db_volume_dir_delete","text":"","code":"db_volume_dir_delete(   path,   recursive = FALSE,   verbose = FALSE,   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_volume_dir_delete.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Volume FileSystem Delete Directory — db_volume_dir_delete","text":"path Absolute path file Files API, omitting initial slash. recursive TRUE, recursively delete directory contents (default: FALSE) verbose TRUE, announce file/directory deletion (default: FALSE) host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_volume_dir_exists.html","id":null,"dir":"Reference","previous_headings":"","what":"Volume FileSystem Check Directory Exists — db_volume_dir_exists","title":"Volume FileSystem Check Directory Exists — db_volume_dir_exists","text":"Volume FileSystem Check Directory Exists","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_volume_dir_exists.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Volume FileSystem Check Directory Exists — db_volume_dir_exists","text":"","code":"db_volume_dir_exists(   path,   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_volume_dir_exists.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Volume FileSystem Check Directory Exists — db_volume_dir_exists","text":"path Absolute path file Files API, omitting initial slash. host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_volume_file_exists.html","id":null,"dir":"Reference","previous_headings":"","what":"Volume FileSystem File Status — db_volume_file_exists","title":"Volume FileSystem File Status — db_volume_file_exists","text":"Volume FileSystem File Status","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_volume_file_exists.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Volume FileSystem File Status — db_volume_file_exists","text":"","code":"db_volume_file_exists(   path,   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_volume_file_exists.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Volume FileSystem File Status — db_volume_file_exists","text":"path Absolute path file Files API, omitting initial slash. host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_volume_list.html","id":null,"dir":"Reference","previous_headings":"","what":"Volume FileSystem List Directory Contents — db_volume_list","title":"Volume FileSystem List Directory Contents — db_volume_list","text":"Volume FileSystem List Directory Contents","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_volume_list.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Volume FileSystem List Directory Contents — db_volume_list","text":"","code":"db_volume_list(   path,   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_volume_list.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Volume FileSystem List Directory Contents — db_volume_list","text":"path Absolute path file Files API, omitting initial slash. host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_volume_read.html","id":null,"dir":"Reference","previous_headings":"","what":"Volume FileSystem Read — db_volume_read","title":"Volume FileSystem Read — db_volume_read","text":"Return contents file within volume (5GiB).","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_volume_read.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Volume FileSystem Read — db_volume_read","text":"","code":"db_volume_read(   path,   destination,   host = db_host(),   token = db_token(),   perform_request = TRUE,   progress = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_volume_read.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Volume FileSystem Read — db_volume_read","text":"path Absolute path file Files API, omitting initial slash. destination Path write downloaded file . host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed. progress TRUE, show progress bar file operations (default: TRUE uploads/downloads, FALSE operations)","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_volume_recursive_delete_contents.html","id":null,"dir":"Reference","previous_headings":"","what":"Recursively delete all contents of a volume directory — db_volume_recursive_delete_contents","title":"Recursively delete all contents of a volume directory — db_volume_recursive_delete_contents","text":"Recursively delete contents volume directory","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_volume_recursive_delete_contents.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Recursively delete all contents of a volume directory — db_volume_recursive_delete_contents","text":"","code":"db_volume_recursive_delete_contents(path, host, token, verbose = FALSE)"},{"path":"https://databrickslabs.github.io/brickster/reference/db_volume_upload_dir.html","id":null,"dir":"Reference","previous_headings":"","what":"Upload Directory to Volume in Parallel — db_volume_upload_dir","title":"Upload Directory to Volume in Parallel — db_volume_upload_dir","text":"Upload files local directory volume directory using parallel requests.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_volume_upload_dir.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Upload Directory to Volume in Parallel — db_volume_upload_dir","text":"","code":"db_volume_upload_dir(   local_dir,   volume_dir,   overwrite = TRUE,   preserve_structure = TRUE,   host = db_host(),   token = db_token() )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_volume_upload_dir.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Upload Directory to Volume in Parallel — db_volume_upload_dir","text":"local_dir Path local directory containing files upload volume_dir Volume directory path (must start /Volumes/) overwrite Flag overwrite existing files (default: TRUE) preserve_structure TRUE, preserve subdirectory structure (default: TRUE) host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token().","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_volume_upload_dir.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Upload Directory to Volume in Parallel — db_volume_upload_dir","text":"TRUE uploads successful","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_volume_write.html","id":null,"dir":"Reference","previous_headings":"","what":"Volume FileSystem Write — db_volume_write","title":"Volume FileSystem Write — db_volume_write","text":"Upload file volume filesystem.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_volume_write.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Volume FileSystem Write — db_volume_write","text":"","code":"db_volume_write(   path,   file = NULL,   overwrite = FALSE,   host = db_host(),   token = db_token(),   perform_request = TRUE,   progress = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_volume_write.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Volume FileSystem Write — db_volume_write","text":"path Absolute path file Files API, omitting initial slash. file Path file local system, takes precedent path. overwrite Flag (Default: FALSE) specifies whether overwrite existing files. host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed. progress TRUE, show progress bar file operations (default: TRUE uploads/downloads, FALSE operations)","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_volume_write.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Volume FileSystem Write — db_volume_write","text":"Uploads file 5 GiB.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_vs_endpoints_create.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a Vector Search Endpoint — db_vs_endpoints_create","title":"Create a Vector Search Endpoint — db_vs_endpoints_create","text":"Create Vector Search Endpoint","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_vs_endpoints_create.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a Vector Search Endpoint — db_vs_endpoints_create","text":"","code":"db_vs_endpoints_create(   name,   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_vs_endpoints_create.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a Vector Search Endpoint — db_vs_endpoints_create","text":"name Name vector search endpoint host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_vs_endpoints_create.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create a Vector Search Endpoint — db_vs_endpoints_create","text":"function can take moments run.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_vs_endpoints_delete.html","id":null,"dir":"Reference","previous_headings":"","what":"Delete a Vector Search Endpoint — db_vs_endpoints_delete","title":"Delete a Vector Search Endpoint — db_vs_endpoints_delete","text":"Delete Vector Search Endpoint","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_vs_endpoints_delete.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Delete a Vector Search Endpoint — db_vs_endpoints_delete","text":"","code":"db_vs_endpoints_delete(   endpoint,   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_vs_endpoints_delete.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Delete a Vector Search Endpoint — db_vs_endpoints_delete","text":"endpoint Name vector search endpoint host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_vs_endpoints_get.html","id":null,"dir":"Reference","previous_headings":"","what":"Get a Vector Search Endpoint — db_vs_endpoints_get","title":"Get a Vector Search Endpoint — db_vs_endpoints_get","text":"Get Vector Search Endpoint","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_vs_endpoints_get.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get a Vector Search Endpoint — db_vs_endpoints_get","text":"","code":"db_vs_endpoints_get(   endpoint,   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_vs_endpoints_get.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get a Vector Search Endpoint — db_vs_endpoints_get","text":"endpoint Name vector search endpoint host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_vs_endpoints_list.html","id":null,"dir":"Reference","previous_headings":"","what":"List Vector Search Endpoints — db_vs_endpoints_list","title":"List Vector Search Endpoints — db_vs_endpoints_list","text":"List Vector Search Endpoints","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_vs_endpoints_list.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List Vector Search Endpoints — db_vs_endpoints_list","text":"","code":"db_vs_endpoints_list(   page_token = NULL,   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_vs_endpoints_list.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List Vector Search Endpoints — db_vs_endpoints_list","text":"page_token Token pagination host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_vs_indexes_create.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a Vector Search Index — db_vs_indexes_create","title":"Create a Vector Search Index — db_vs_indexes_create","text":"Create Vector Search Index","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_vs_indexes_create.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a Vector Search Index — db_vs_indexes_create","text":"","code":"db_vs_indexes_create(   name,   endpoint,   primary_key,   spec,   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_vs_indexes_create.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a Vector Search Index — db_vs_indexes_create","text":"name Name vector search index endpoint Name vector search endpoint primary_key Vector search primary key column name spec Either delta_sync_index_spec() direct_access_index_spec(). host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_vs_indexes_delete.html","id":null,"dir":"Reference","previous_headings":"","what":"Delete a Vector Search Index — db_vs_indexes_delete","title":"Delete a Vector Search Index — db_vs_indexes_delete","text":"Delete Vector Search Index","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_vs_indexes_delete.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Delete a Vector Search Index — db_vs_indexes_delete","text":"","code":"db_vs_indexes_delete(   index,   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_vs_indexes_delete.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Delete a Vector Search Index — db_vs_indexes_delete","text":"index Name vector search index host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_vs_indexes_delete_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Delete Data from a Vector Search Index — db_vs_indexes_delete_data","title":"Delete Data from a Vector Search Index — db_vs_indexes_delete_data","text":"Delete Data Vector Search Index","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_vs_indexes_delete_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Delete Data from a Vector Search Index — db_vs_indexes_delete_data","text":"","code":"db_vs_indexes_delete_data(   index,   primary_keys,   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_vs_indexes_delete_data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Delete Data from a Vector Search Index — db_vs_indexes_delete_data","text":"index Name vector search index primary_keys primary keys deleted index host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_vs_indexes_get.html","id":null,"dir":"Reference","previous_headings":"","what":"Get a Vector Search Index — db_vs_indexes_get","title":"Get a Vector Search Index — db_vs_indexes_get","text":"Get Vector Search Index","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_vs_indexes_get.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get a Vector Search Index — db_vs_indexes_get","text":"","code":"db_vs_indexes_get(   index,   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_vs_indexes_get.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get a Vector Search Index — db_vs_indexes_get","text":"index Name vector search index host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_vs_indexes_list.html","id":null,"dir":"Reference","previous_headings":"","what":"List Vector Search Indexes — db_vs_indexes_list","title":"List Vector Search Indexes — db_vs_indexes_list","text":"List Vector Search Indexes","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_vs_indexes_list.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List Vector Search Indexes — db_vs_indexes_list","text":"","code":"db_vs_indexes_list(   endpoint,   page_token = NULL,   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_vs_indexes_list.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List Vector Search Indexes — db_vs_indexes_list","text":"endpoint Name vector search endpoint page_token page_token returned prior query host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_vs_indexes_query.html","id":null,"dir":"Reference","previous_headings":"","what":"Query a Vector Search Index — db_vs_indexes_query","title":"Query a Vector Search Index — db_vs_indexes_query","text":"Query Vector Search Index","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_vs_indexes_query.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Query a Vector Search Index — db_vs_indexes_query","text":"","code":"db_vs_indexes_query(   index,   columns,   filters_json,   query_vector = NULL,   query_text = NULL,   score_threshold = 0,   query_type = c(\"ANN\", \"HYBRID\"),   num_results = 10,   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_vs_indexes_query.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Query a Vector Search Index — db_vs_indexes_query","text":"index Name vector search index columns Column names include response filters_json JSON string representing query filters, see details. query_vector Numeric vector. Required direct vector access index delta sync index using self managed vectors. query_text Required delta sync index using model endpoint. score_threshold Numeric score threshold approximate nearest neighbour (ANN) search. Defaults 0.0. query_type One ANN (default) HYBRID num_results Number returns return (default: 10). host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_vs_indexes_query.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Query a Vector Search Index — db_vs_indexes_query","text":"specify query_vector query_text time. filter_jsons examples: '{\"id <\": 5}': Filter id less 5 '{\"id >\": 5}': Filter id greater 5 '{\"id <=\": 5}': Filter id less equal 5 '{\"id >=\": 5}': Filter id greater equal 5 '{\"id\": 5}': Filter id equal 5 '{\"id\": 5, \"age >=\": 18}': Filter id equal 5 age greater equal 18 filter_jsons convert attempt use jsonlite::toJSON non character vectors. Refer docs Vector Search.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_vs_indexes_query.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Query a Vector Search Index — db_vs_indexes_query","text":"","code":"if (FALSE) { # \\dontrun{ db_vs_indexes_sync(   index = \"myindex\",   columns = c(\"id\", \"text\"),   query_vector = c(1, 2, 3) ) } # }"},{"path":"https://databrickslabs.github.io/brickster/reference/db_vs_indexes_query_next_page.html","id":null,"dir":"Reference","previous_headings":"","what":"Query Vector Search Next Page — db_vs_indexes_query_next_page","title":"Query Vector Search Next Page — db_vs_indexes_query_next_page","text":"Query Vector Search Next Page","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_vs_indexes_query_next_page.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Query Vector Search Next Page — db_vs_indexes_query_next_page","text":"","code":"db_vs_indexes_query_next_page(   index,   endpoint,   page_token = NULL,   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_vs_indexes_query_next_page.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Query Vector Search Next Page — db_vs_indexes_query_next_page","text":"index Name vector search index endpoint Name vector search endpoint page_token page_token returned prior query host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_vs_indexes_scan.html","id":null,"dir":"Reference","previous_headings":"","what":"Scan a Vector Search Index — db_vs_indexes_scan","title":"Scan a Vector Search Index — db_vs_indexes_scan","text":"Scan Vector Search Index","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_vs_indexes_scan.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Scan a Vector Search Index — db_vs_indexes_scan","text":"","code":"db_vs_indexes_scan(   endpoint,   index,   last_primary_key,   num_results = 10,   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_vs_indexes_scan.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Scan a Vector Search Index — db_vs_indexes_scan","text":"endpoint Name vector search endpoint scan index Name vector search index scan last_primary_key Primary key last entry returned previous scan num_results Number returns return (default: 10) host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_vs_indexes_scan.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Scan a Vector Search Index — db_vs_indexes_scan","text":"Scan specified vector index return first num_results entries exclusive primary_key.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_vs_indexes_sync.html","id":null,"dir":"Reference","previous_headings":"","what":"Synchronize a Vector Search Index — db_vs_indexes_sync","title":"Synchronize a Vector Search Index — db_vs_indexes_sync","text":"Synchronize Vector Search Index","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_vs_indexes_sync.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Synchronize a Vector Search Index — db_vs_indexes_sync","text":"","code":"db_vs_indexes_sync(   index,   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_vs_indexes_sync.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Synchronize a Vector Search Index — db_vs_indexes_sync","text":"index Name vector search index host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_vs_indexes_sync.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Synchronize a Vector Search Index — db_vs_indexes_sync","text":"Triggers synchronization process specified vector index. index must 'Delta Sync' index.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_vs_indexes_upsert_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Upsert Data into a Vector Search Index — db_vs_indexes_upsert_data","title":"Upsert Data into a Vector Search Index — db_vs_indexes_upsert_data","text":"Upsert Data Vector Search Index","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_vs_indexes_upsert_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Upsert Data into a Vector Search Index — db_vs_indexes_upsert_data","text":"","code":"db_vs_indexes_upsert_data(   index,   df,   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_vs_indexes_upsert_data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Upsert Data into a Vector Search Index — db_vs_indexes_upsert_data","text":"index Name vector search index df data.frame containing data upsert host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_workspace_delete.html","id":null,"dir":"Reference","previous_headings":"","what":"Delete Object/Directory (Workspaces) — db_workspace_delete","title":"Delete Object/Directory (Workspaces) — db_workspace_delete","text":"Delete Object/Directory (Workspaces)","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_workspace_delete.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Delete Object/Directory (Workspaces) — db_workspace_delete","text":"","code":"db_workspace_delete(   path,   recursive = FALSE,   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_workspace_delete.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Delete Object/Directory (Workspaces) — db_workspace_delete","text":"path Absolute path notebook directory. recursive Flag specifies whether delete object recursively. False default. host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_workspace_delete.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Delete Object/Directory (Workspaces) — db_workspace_delete","text":"Delete object directory (optionally recursively deletes objects directory). path exist, call returns error RESOURCE_DOES_NOT_EXIST. path non-empty directory recursive set false, call returns error DIRECTORY_NOT_EMPTY. Object deletion undone deleting directory recursively atomic.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_workspace_export.html","id":null,"dir":"Reference","previous_headings":"","what":"Export Notebook or Directory (Workspaces) — db_workspace_export","title":"Export Notebook or Directory (Workspaces) — db_workspace_export","text":"Export Notebook Directory (Workspaces)","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_workspace_export.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Export Notebook or Directory (Workspaces) — db_workspace_export","text":"","code":"db_workspace_export(   path,   format = c(\"AUTO\", \"SOURCE\", \"HTML\", \"JUPYTER\", \"DBC\", \"R_MARKDOWN\"),   host = db_host(),   token = db_token(),   output_path = NULL,   direct_download = FALSE,   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_workspace_export.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Export Notebook or Directory (Workspaces) — db_workspace_export","text":"path Absolute path notebook directory. format One AUTO, SOURCE, HTML, JUPYTER, DBC, R_MARKDOWN. Default SOURCE. host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). output_path Path export file , ensure include correct suffix. direct_download Boolean (default: FALSE), TRUE download file contents directly file. Must also specify output_path. perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_workspace_export.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Export Notebook or Directory (Workspaces) — db_workspace_export","text":"base64 encoded string","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_workspace_export.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Export Notebook or Directory (Workspaces) — db_workspace_export","text":"Export notebook contents entire directory. path exist, call returns error RESOURCE_DOES_NOT_EXIST. can export directory DBC format. exported data exceeds size limit, call returns error MAX_NOTEBOOK_SIZE_EXCEEDED. API support exporting library. time support direct_download parameter returns base64 encoded string. See .","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_workspace_get_status.html","id":null,"dir":"Reference","previous_headings":"","what":"Get Object Status (Workspaces) — db_workspace_get_status","title":"Get Object Status (Workspaces) — db_workspace_get_status","text":"Gets status object directory.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_workspace_get_status.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get Object Status (Workspaces) — db_workspace_get_status","text":"","code":"db_workspace_get_status(   path,   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_workspace_get_status.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get Object Status (Workspaces) — db_workspace_get_status","text":"path Absolute path notebook directory. host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_workspace_get_status.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get Object Status (Workspaces) — db_workspace_get_status","text":"path exist, call returns error RESOURCE_DOES_NOT_EXIST.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_workspace_import.html","id":null,"dir":"Reference","previous_headings":"","what":"Import Notebook/Directory (Workspaces) — db_workspace_import","title":"Import Notebook/Directory (Workspaces) — db_workspace_import","text":"Import notebook contents entire directory.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_workspace_import.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Import Notebook/Directory (Workspaces) — db_workspace_import","text":"","code":"db_workspace_import(   path,   file = NULL,   content = NULL,   format = c(\"AUTO\", \"SOURCE\", \"HTML\", \"JUPYTER\", \"DBC\", \"R_MARKDOWN\"),   language = NULL,   overwrite = FALSE,   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_workspace_import.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Import Notebook/Directory (Workspaces) — db_workspace_import","text":"path Absolute path notebook directory. file Path local file upload. See formats parameter. content Content upload, base64-encoded limit 10MB. format One AUTO, SOURCE, HTML, JUPYTER, DBC, R_MARKDOWN. Default SOURCE. language One R, PYTHON, SCALA, SQL. Required format SOURCE otherwise ignored. overwrite Flag specifies whether overwrite existing object. FALSE default. DBC overwrite supported since may contain directory. host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_workspace_import.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Import Notebook/Directory (Workspaces) — db_workspace_import","text":"file content mutually exclusive. specified content ignored. path already exists overwrite set FALSE, call returns error RESOURCE_ALREADY_EXISTS. can use DBC format import directory.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_workspace_list.html","id":null,"dir":"Reference","previous_headings":"","what":"List Directory Contents (Workspaces) — db_workspace_list","title":"List Directory Contents (Workspaces) — db_workspace_list","text":"List Directory Contents (Workspaces)","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_workspace_list.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List Directory Contents (Workspaces) — db_workspace_list","text":"","code":"db_workspace_list(   path,   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_workspace_list.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"List Directory Contents (Workspaces) — db_workspace_list","text":"path Absolute path notebook directory. host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_workspace_list.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"List Directory Contents (Workspaces) — db_workspace_list","text":"List contents directory, object directory. input path exist, call returns error RESOURCE_DOES_NOT_EXIST.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_workspace_mkdirs.html","id":null,"dir":"Reference","previous_headings":"","what":"Make a Directory (Workspaces) — db_workspace_mkdirs","title":"Make a Directory (Workspaces) — db_workspace_mkdirs","text":"Make Directory (Workspaces)","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_workspace_mkdirs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Make a Directory (Workspaces) — db_workspace_mkdirs","text":"","code":"db_workspace_mkdirs(   path,   host = db_host(),   token = db_token(),   perform_request = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_workspace_mkdirs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Make a Directory (Workspaces) — db_workspace_mkdirs","text":"path Absolute path directory. host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). perform_request TRUE (default) request performed, FALSE httr2 request returned without performed.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_workspace_mkdirs.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Make a Directory (Workspaces) — db_workspace_mkdirs","text":"Create given directory necessary parent directories exists. exists object (directory) prefix input path, call returns error RESOURCE_ALREADY_EXISTS. operation fails may succeeded creating necessary parent directories.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/db_write_table_standard.html","id":null,"dir":"Reference","previous_headings":"","what":"Write table using standard SQL approach — db_write_table_standard","title":"Write table using standard SQL approach — db_write_table_standard","text":"Write table using standard SQL approach","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_write_table_standard.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Write table using standard SQL approach — db_write_table_standard","text":"","code":"db_write_table_standard(   conn,   quoted_name,   value,   overwrite,   append,   field.types,   temporary = FALSE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_write_table_volume.html","id":null,"dir":"Reference","previous_headings":"","what":"Write table using volume-based approach — db_write_table_volume","title":"Write table using volume-based approach — db_write_table_volume","text":"Write table using volume-based approach","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_write_table_volume.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Write table using volume-based approach — db_write_table_volume","text":"","code":"db_write_table_volume(   conn,   quoted_name,   value,   staging_volume,   append = FALSE,   progress = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/db_wsid.html","id":null,"dir":"Reference","previous_headings":"","what":"Fetch Databricks Workspace ID — db_wsid","title":"Fetch Databricks Workspace ID — db_wsid","text":"Workspace ID, optionally specified make connections pane powerful. Specified environment variable DATABRICKS_WSID. .databrickscfg searched db_profile use_databrickscfg set Posit Workbench managed OAuth credentials detected. Refer api authentication docs","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_wsid.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fetch Databricks Workspace ID — db_wsid","text":"","code":"db_wsid(profile = default_config_profile())"},{"path":"https://databrickslabs.github.io/brickster/reference/db_wsid.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fetch Databricks Workspace ID — db_wsid","text":"profile Profile use fetching environment variable (e.g. .Renviron) .databricksfg file","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_wsid.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fetch Databricks Workspace ID — db_wsid","text":"databricks workspace ID","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/db_wsid.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Fetch Databricks Workspace ID — db_wsid","text":"behaviour subject change depending db_profile use_databrickscfg options set. use_databrickscfg: Boolean (default: FALSE), determines credentials fetched profile .databrickscfg .Renviron db_profile: String (default: NULL), determines profile used. .databrickscfg automatically used Posit Workbench managed OAuth credentials detected. See vignette authentication details.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/dbfs_storage_info.html","id":null,"dir":"Reference","previous_headings":"","what":"DBFS Storage Information — dbfs_storage_info","title":"DBFS Storage Information — dbfs_storage_info","text":"DBFS Storage Information","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/dbfs_storage_info.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"DBFS Storage Information — dbfs_storage_info","text":"","code":"dbfs_storage_info(destination)"},{"path":"https://databrickslabs.github.io/brickster/reference/dbfs_storage_info.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"DBFS Storage Information — dbfs_storage_info","text":"destination DBFS destination. Example: dbfs://path.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/dbplyr_edition.DatabricksConnection.html","id":null,"dir":"Reference","previous_headings":"","what":"Declare dbplyr API version for Databricks connections — dbplyr_edition.DatabricksConnection","title":"Declare dbplyr API version for Databricks connections — dbplyr_edition.DatabricksConnection","text":"Declare dbplyr API version Databricks connections","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/dbplyr_edition.DatabricksConnection.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Declare dbplyr API version for Databricks connections — dbplyr_edition.DatabricksConnection","text":"","code":"# S3 method for class 'DatabricksConnection' dbplyr_edition(con)"},{"path":"https://databrickslabs.github.io/brickster/reference/dbplyr_edition.DatabricksConnection.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Declare dbplyr API version for Databricks connections — dbplyr_edition.DatabricksConnection","text":"con DatabricksConnection object","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/dbplyr_edition.DatabricksConnection.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Declare dbplyr API version for Databricks connections — dbplyr_edition.DatabricksConnection","text":"dbplyr edition number (2L)","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/default_config_profile.html","id":null,"dir":"Reference","previous_headings":"","what":"Returns the default config profile — default_config_profile","title":"Returns the default config profile — default_config_profile","text":"Returns default config profile","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/default_config_profile.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Returns the default config profile — default_config_profile","text":"","code":"default_config_profile()"},{"path":"https://databrickslabs.github.io/brickster/reference/default_config_profile.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Returns the default config profile — default_config_profile","text":"profile name","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/default_config_profile.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Returns the default config profile — default_config_profile","text":"Returns config profile first looking DATABRICKS_CONFIG_PROFILE db_profile option.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/delta_sync_index_spec.html","id":null,"dir":"Reference","previous_headings":"","what":"Delta Sync Vector Search Index Specification — delta_sync_index_spec","title":"Delta Sync Vector Search Index Specification — delta_sync_index_spec","text":"Delta Sync Vector Search Index Specification","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/delta_sync_index_spec.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Delta Sync Vector Search Index Specification — delta_sync_index_spec","text":"","code":"delta_sync_index_spec(   source_table,   embedding_writeback_table = NULL,   embedding_source_columns = NULL,   embedding_vector_columns = NULL,   pipeline_type = c(\"TRIGGERED\", \"CONTINUOUS\") )"},{"path":"https://databrickslabs.github.io/brickster/reference/delta_sync_index_spec.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Delta Sync Vector Search Index Specification — delta_sync_index_spec","text":"source_table name source table. embedding_writeback_table Name table sync index contents computed embeddings back delta table, see details. embedding_source_columns columns contain embedding source, must one list embedding_source_column() embedding_vector_columns columns contain embedding, must one list embedding_vector_column() pipeline_type Pipeline execution mode, see details.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/delta_sync_index_spec.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Delta Sync Vector Search Index Specification — delta_sync_index_spec","text":"pipeline_type either: \"TRIGGERED\":  pipeline uses triggered execution mode, system stops processing successfully refreshing source table pipeline , ensuring table updated based data available update started. \"CONTINUOUS\" pipeline uses continuous execution, pipeline processes new data arrives source table keep vector index fresh. supported naming convention embedding_writeback_table \"<index_name>_writeback_table\".","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/determine_brickster_venv.html","id":null,"dir":"Reference","previous_headings":"","what":"Determine brickster virtualenv — determine_brickster_venv","title":"Determine brickster virtualenv — determine_brickster_venv","text":"Determine brickster virtualenv","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/determine_brickster_venv.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Determine brickster virtualenv — determine_brickster_venv","text":"","code":"determine_brickster_venv()"},{"path":"https://databrickslabs.github.io/brickster/reference/determine_brickster_venv.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Determine brickster virtualenv — determine_brickster_venv","text":"Returns NULL running within Databricks, otherwise \"r-brickster\"","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/direct_access_index_spec.html","id":null,"dir":"Reference","previous_headings":"","what":"Delta Sync Vector Search Index Specification — direct_access_index_spec","title":"Delta Sync Vector Search Index Specification — direct_access_index_spec","text":"Delta Sync Vector Search Index Specification","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/direct_access_index_spec.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Delta Sync Vector Search Index Specification — direct_access_index_spec","text":"","code":"direct_access_index_spec(   embedding_source_columns = NULL,   embedding_vector_columns = NULL,   schema )"},{"path":"https://databrickslabs.github.io/brickster/reference/direct_access_index_spec.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Delta Sync Vector Search Index Specification — direct_access_index_spec","text":"embedding_source_columns columns contain embedding source, must one list embedding_source_column() embedding_vector_columns columns contain embedding, must one list embedding_vector_column() vectors. schema Named list, names column names, values types. See details.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/direct_access_index_spec.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Delta Sync Vector Search Index Specification — direct_access_index_spec","text":"supported types : \"integer\" \"long\" \"float\" \"double\" \"boolean\" \"string\" \"date\" \"timestamp\" \"array<float>\": supported vector columns \"array<double>\": supported vector columns","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/docker_image.html","id":null,"dir":"Reference","previous_headings":"","what":"Docker Image — docker_image","title":"Docker Image — docker_image","text":"Docker image connection information.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/docker_image.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Docker Image — docker_image","text":"","code":"docker_image(url, username, password)"},{"path":"https://databrickslabs.github.io/brickster/reference/docker_image.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Docker Image — docker_image","text":"url URL Docker image. username User name Docker repository. password Password Docker repository.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/docker_image.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Docker Image — docker_image","text":"Uses basic authentication, strongly recommended credentials stored scripts environment variables used.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/email_notifications.html","id":null,"dir":"Reference","previous_headings":"","what":"Email Notifications — email_notifications","title":"Email Notifications — email_notifications","text":"Email Notifications","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/email_notifications.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Email Notifications — email_notifications","text":"","code":"email_notifications(   on_start = NULL,   on_success = NULL,   on_failure = NULL,   no_alert_for_skipped_runs = TRUE )"},{"path":"https://databrickslabs.github.io/brickster/reference/email_notifications.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Email Notifications — email_notifications","text":"on_start List email addresses notified run begins. specified job creation, reset, update, list empty, notifications sent. on_success List email addresses notified run successfully completes. run considered completed successfully ends TERMINATED life_cycle_state SUCCESSFUL result_state. specified job creation, reset, update, list empty, notifications sent. on_failure List email addresses notified run unsuccessfully completes. run considered completed unsuccessfully ends INTERNAL_ERROR life_cycle_state SKIPPED, FAILED, TIMED_OUT result_state. specified job creation, reset, update list empty, notifications sent. no_alert_for_skipped_runs TRUE (default), send email recipients specified on_failure run skipped.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/embedding_source_column.html","id":null,"dir":"Reference","previous_headings":"","what":"Embedding Source Column — embedding_source_column","title":"Embedding Source Column — embedding_source_column","text":"Embedding Source Column","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/embedding_source_column.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Embedding Source Column — embedding_source_column","text":"","code":"embedding_source_column(name, model_endpoint_name)"},{"path":"https://databrickslabs.github.io/brickster/reference/embedding_source_column.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Embedding Source Column — embedding_source_column","text":"name Name column model_endpoint_name Name embedding model endpoint","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/embedding_vector_column.html","id":null,"dir":"Reference","previous_headings":"","what":"Embedding Vector Column — embedding_vector_column","title":"Embedding Vector Column — embedding_vector_column","text":"Embedding Vector Column","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/embedding_vector_column.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Embedding Vector Column — embedding_vector_column","text":"","code":"embedding_vector_column(name, dimension)"},{"path":"https://databrickslabs.github.io/brickster/reference/embedding_vector_column.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Embedding Vector Column — embedding_vector_column","text":"name Name column dimension dimension embedding vector","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/file_storage_info.html","id":null,"dir":"Reference","previous_headings":"","what":"File Storage Information — file_storage_info","title":"File Storage Information — file_storage_info","text":"File Storage Information","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/file_storage_info.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"File Storage Information — file_storage_info","text":"","code":"file_storage_info(destination)"},{"path":"https://databrickslabs.github.io/brickster/reference/file_storage_info.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"File Storage Information — file_storage_info","text":"destination File destination. Example: file://file.sh.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/file_storage_info.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"File Storage Information — file_storage_info","text":"file storage type available clusters set using Databricks Container Services.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/for_each_task.html","id":null,"dir":"Reference","previous_headings":"","what":"For Each Task — for_each_task","title":"For Each Task — for_each_task","text":"Task","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/for_each_task.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"For Each Task — for_each_task","text":"","code":"for_each_task(inputs, task, concurrency = 1)"},{"path":"https://databrickslabs.github.io/brickster/reference/for_each_task.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"For Each Task — for_each_task","text":"inputs Array task iterate . can JSON string reference array parameter. task Must job_task(). concurrency Maximum allowed number concurrent runs task.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/gcp_attributes.html","id":null,"dir":"Reference","previous_headings":"","what":"GCP Attributes — gcp_attributes","title":"GCP Attributes — gcp_attributes","text":"GCP Attributes","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/gcp_attributes.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"GCP Attributes — gcp_attributes","text":"","code":"gcp_attributes(use_preemptible_executors = TRUE, google_service_account = NULL)"},{"path":"https://databrickslabs.github.io/brickster/reference/gcp_attributes.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"GCP Attributes — gcp_attributes","text":"use_preemptible_executors Boolean (Default: TRUE). TRUE Uses preemptible executors google_service_account Google service account email address cluster uses authenticate Google Identity. field used authentication GCS BigQuery data sources.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/gcp_attributes.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"GCP Attributes — gcp_attributes","text":"use GCS BigQuery, Google service account use access data source must project SA specified setting Databricks account.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/generate_temp_name.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate unique temporary table/view name — generate_temp_name","title":"Generate unique temporary table/view name — generate_temp_name","text":"Generate unique temporary table/view name","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/generate_temp_name.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate unique temporary table/view name — generate_temp_name","text":"","code":"generate_temp_name(prefix = \"dbplyr_temp\")"},{"path":"https://databrickslabs.github.io/brickster/reference/generate_temp_name.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate unique temporary table/view name — generate_temp_name","text":"prefix Base name prefix (default: \"dbplyr_temp\")","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/generate_temp_name.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate unique temporary table/view name — generate_temp_name","text":"Unique temporary name","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/get_and_start_cluster.html","id":null,"dir":"Reference","previous_headings":"","what":"Get and Start Cluster — get_and_start_cluster","title":"Get and Start Cluster — get_and_start_cluster","text":"Get Start Cluster","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/get_and_start_cluster.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get and Start Cluster — get_and_start_cluster","text":"","code":"get_and_start_cluster(   cluster_id,   polling_interval = 5,   host = db_host(),   token = db_token(),   silent = FALSE )"},{"path":"https://databrickslabs.github.io/brickster/reference/get_and_start_cluster.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get and Start Cluster — get_and_start_cluster","text":"cluster_id Canonical identifier cluster. polling_interval Number seconds wait status checks host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). silent Boolean (default: FALSE), emit cluster state progress TRUE.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/get_and_start_cluster.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get and Start Cluster — get_and_start_cluster","text":"db_cluster_get()","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/get_and_start_cluster.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get and Start Cluster — get_and_start_cluster","text":"Get information regarding Databricks cluster. cluster inactive started wait cluster active.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/get_and_start_warehouse.html","id":null,"dir":"Reference","previous_headings":"","what":"Get and Start Warehouse — get_and_start_warehouse","title":"Get and Start Warehouse — get_and_start_warehouse","text":"Get Start Warehouse","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/get_and_start_warehouse.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get and Start Warehouse — get_and_start_warehouse","text":"","code":"get_and_start_warehouse(   id,   polling_interval = 5,   host = db_host(),   token = db_token() )"},{"path":"https://databrickslabs.github.io/brickster/reference/get_and_start_warehouse.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get and Start Warehouse — get_and_start_warehouse","text":"id ID SQL warehouse. polling_interval Number seconds wait status checks host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token().","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/get_and_start_warehouse.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get and Start Warehouse — get_and_start_warehouse","text":"db_sql_warehouse_get()","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/get_and_start_warehouse.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get and Start Warehouse — get_and_start_warehouse","text":"Get information regarding Databricks cluster. cluster inactive started wait cluster active.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/get_latest_dbr.html","id":null,"dir":"Reference","previous_headings":"","what":"Get Latest Databricks Runtime (DBR) — get_latest_dbr","title":"Get Latest Databricks Runtime (DBR) — get_latest_dbr","text":"Get Latest Databricks Runtime (DBR)","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/get_latest_dbr.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get Latest Databricks Runtime (DBR) — get_latest_dbr","text":"","code":"get_latest_dbr(lts, ml, gpu, photon, host = db_host(), token = db_token())"},{"path":"https://databrickslabs.github.io/brickster/reference/get_latest_dbr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get Latest Databricks Runtime (DBR) — get_latest_dbr","text":"lts Boolean, TRUE returns LTS runtimes ml Boolean, TRUE returns ML runtimes gpu Boolean, TRUE returns ML GPU runtimes photon Boolean, TRUE returns photon runtimes host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token().","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/get_latest_dbr.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get Latest Databricks Runtime (DBR) — get_latest_dbr","text":"Named list","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/get_latest_dbr.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get Latest Databricks Runtime (DBR) — get_latest_dbr","text":"runtime combinations possible, GPU/ML photon. function permit invalid combinations.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/git_source.html","id":null,"dir":"Reference","previous_headings":"","what":"Git Source for Job Notebook Tasks — git_source","title":"Git Source for Job Notebook Tasks — git_source","text":"Git Source Job Notebook Tasks","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/git_source.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Git Source for Job Notebook Tasks — git_source","text":"","code":"git_source(   git_url,   git_provider,   reference,   type = c(\"branch\", \"tag\", \"commit\") )"},{"path":"https://databrickslabs.github.io/brickster/reference/git_source.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Git Source for Job Notebook Tasks — git_source","text":"git_url URL repository cloned job. maximum length 300 characters. git_provider Unique identifier service used host Git repository. Must one : github, bitbucketcloud, azuredevopsservices, githubenterprise, bitbucketserver, gitlab, gitlabenterpriseedition, awscodecommit. reference Branch, tag, commit checked used job. type Type reference used, one : branch, tag, commit.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/in_databricks_nb.html","id":null,"dir":"Reference","previous_headings":"","what":"Detect if running within Databricks Notebook — in_databricks_nb","title":"Detect if running within Databricks Notebook — in_databricks_nb","text":"Detect running within Databricks Notebook","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/in_databricks_nb.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Detect if running within Databricks Notebook — in_databricks_nb","text":"","code":"in_databricks_nb()"},{"path":"https://databrickslabs.github.io/brickster/reference/in_databricks_nb.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Detect if running within Databricks Notebook — in_databricks_nb","text":"Boolean","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/in_databricks_nb.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Detect if running within Databricks Notebook — in_databricks_nb","text":"R sessions Databricks can detected via various environment variables directories.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/init_script_info.html","id":null,"dir":"Reference","previous_headings":"","what":"Init Script Info — init_script_info","title":"Init Script Info — init_script_info","text":"Init Script Info","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/init_script_info.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Init Script Info — init_script_info","text":"","code":"init_script_info(...)"},{"path":"https://databrickslabs.github.io/brickster/reference/init_script_info.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Init Script Info — init_script_info","text":"... Accepts multiple instances s3_storage_info(), file_storage_info(), dbfs_storage_info().","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/init_script_info.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Init Script Info — init_script_info","text":"file_storage_info() available clusters set using Databricks Container Services. instructions using init scripts Databricks Container Services, see Use init script.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/is.access_control_req_group.html","id":null,"dir":"Reference","previous_headings":"","what":"Test if object is of class AccessControlRequestForGroup — is.access_control_req_group","title":"Test if object is of class AccessControlRequestForGroup — is.access_control_req_group","text":"Test object class AccessControlRequestForGroup","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.access_control_req_group.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Test if object is of class AccessControlRequestForGroup — is.access_control_req_group","text":"","code":"is.access_control_req_group(x)"},{"path":"https://databrickslabs.github.io/brickster/reference/is.access_control_req_group.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Test if object is of class AccessControlRequestForGroup — is.access_control_req_group","text":"x object","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.access_control_req_group.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Test if object is of class AccessControlRequestForGroup — is.access_control_req_group","text":"TRUE object inherits AccessControlRequestForGroup class.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.access_control_req_user.html","id":null,"dir":"Reference","previous_headings":"","what":"Test if object is of class AccessControlRequestForUser — is.access_control_req_user","title":"Test if object is of class AccessControlRequestForUser — is.access_control_req_user","text":"Test object class AccessControlRequestForUser","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.access_control_req_user.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Test if object is of class AccessControlRequestForUser — is.access_control_req_user","text":"","code":"is.access_control_req_user(x)"},{"path":"https://databrickslabs.github.io/brickster/reference/is.access_control_req_user.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Test if object is of class AccessControlRequestForUser — is.access_control_req_user","text":"x object","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.access_control_req_user.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Test if object is of class AccessControlRequestForUser — is.access_control_req_user","text":"TRUE object inherits AccessControlRequestForUser class.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.access_control_request.html","id":null,"dir":"Reference","previous_headings":"","what":"Test if object is of class AccessControlRequest — is.access_control_request","title":"Test if object is of class AccessControlRequest — is.access_control_request","text":"Test object class AccessControlRequest","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.access_control_request.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Test if object is of class AccessControlRequest — is.access_control_request","text":"","code":"is.access_control_request(x)"},{"path":"https://databrickslabs.github.io/brickster/reference/is.access_control_request.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Test if object is of class AccessControlRequest — is.access_control_request","text":"x object","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.access_control_request.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Test if object is of class AccessControlRequest — is.access_control_request","text":"TRUE object inherits AccessControlRequest class.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.aws_attributes.html","id":null,"dir":"Reference","previous_headings":"","what":"Test if object is of class AwsAttributes — is.aws_attributes","title":"Test if object is of class AwsAttributes — is.aws_attributes","text":"Test object class AwsAttributes","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.aws_attributes.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Test if object is of class AwsAttributes — is.aws_attributes","text":"","code":"is.aws_attributes(x)"},{"path":"https://databrickslabs.github.io/brickster/reference/is.aws_attributes.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Test if object is of class AwsAttributes — is.aws_attributes","text":"x object","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.aws_attributes.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Test if object is of class AwsAttributes — is.aws_attributes","text":"TRUE object inherits AwsAttributes class.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.azure_attributes.html","id":null,"dir":"Reference","previous_headings":"","what":"Test if object is of class AzureAttributes — is.azure_attributes","title":"Test if object is of class AzureAttributes — is.azure_attributes","text":"Test object class AzureAttributes","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.azure_attributes.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Test if object is of class AzureAttributes — is.azure_attributes","text":"","code":"is.azure_attributes(x)"},{"path":"https://databrickslabs.github.io/brickster/reference/is.azure_attributes.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Test if object is of class AzureAttributes — is.azure_attributes","text":"x object","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.azure_attributes.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Test if object is of class AzureAttributes — is.azure_attributes","text":"TRUE object inherits AzureAttributes class.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.cluster_autoscale.html","id":null,"dir":"Reference","previous_headings":"","what":"Test if object is of class AutoScale — is.cluster_autoscale","title":"Test if object is of class AutoScale — is.cluster_autoscale","text":"Test object class AutoScale","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.cluster_autoscale.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Test if object is of class AutoScale — is.cluster_autoscale","text":"","code":"is.cluster_autoscale(x)"},{"path":"https://databrickslabs.github.io/brickster/reference/is.cluster_autoscale.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Test if object is of class AutoScale — is.cluster_autoscale","text":"x object","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.cluster_autoscale.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Test if object is of class AutoScale — is.cluster_autoscale","text":"TRUE object inherits AutoScale class.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.cluster_log_conf.html","id":null,"dir":"Reference","previous_headings":"","what":"Test if object is of class ClusterLogConf — is.cluster_log_conf","title":"Test if object is of class ClusterLogConf — is.cluster_log_conf","text":"Test object class ClusterLogConf","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.cluster_log_conf.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Test if object is of class ClusterLogConf — is.cluster_log_conf","text":"","code":"is.cluster_log_conf(x)"},{"path":"https://databrickslabs.github.io/brickster/reference/is.cluster_log_conf.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Test if object is of class ClusterLogConf — is.cluster_log_conf","text":"x object","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.cluster_log_conf.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Test if object is of class ClusterLogConf — is.cluster_log_conf","text":"TRUE object inherits ClusterLogConf class.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.condition_task.html","id":null,"dir":"Reference","previous_headings":"","what":"Test if object is of class ConditionTask — is.condition_task","title":"Test if object is of class ConditionTask — is.condition_task","text":"Test object class ConditionTask","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.condition_task.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Test if object is of class ConditionTask — is.condition_task","text":"","code":"is.condition_task(x)"},{"path":"https://databrickslabs.github.io/brickster/reference/is.condition_task.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Test if object is of class ConditionTask — is.condition_task","text":"x object","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.condition_task.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Test if object is of class ConditionTask — is.condition_task","text":"TRUE object inherits ConditionTask class.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.cron_schedule.html","id":null,"dir":"Reference","previous_headings":"","what":"Test if object is of class CronSchedule — is.cron_schedule","title":"Test if object is of class CronSchedule — is.cron_schedule","text":"Test object class CronSchedule","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.cron_schedule.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Test if object is of class CronSchedule — is.cron_schedule","text":"","code":"is.cron_schedule(x)"},{"path":"https://databrickslabs.github.io/brickster/reference/is.cron_schedule.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Test if object is of class CronSchedule — is.cron_schedule","text":"x object","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.cron_schedule.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Test if object is of class CronSchedule — is.cron_schedule","text":"TRUE object inherits CronSchedule class.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.dbfs_storage_info.html","id":null,"dir":"Reference","previous_headings":"","what":"Test if object is of class DbfsStorageInfo — is.dbfs_storage_info","title":"Test if object is of class DbfsStorageInfo — is.dbfs_storage_info","text":"Test object class DbfsStorageInfo","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.dbfs_storage_info.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Test if object is of class DbfsStorageInfo — is.dbfs_storage_info","text":"","code":"is.dbfs_storage_info(x)"},{"path":"https://databrickslabs.github.io/brickster/reference/is.dbfs_storage_info.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Test if object is of class DbfsStorageInfo — is.dbfs_storage_info","text":"x object","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.dbfs_storage_info.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Test if object is of class DbfsStorageInfo — is.dbfs_storage_info","text":"TRUE object inherits DbfsStorageInfo class.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.delta_sync_index.html","id":null,"dir":"Reference","previous_headings":"","what":"Test if object is of class DeltaSyncIndex — is.delta_sync_index","title":"Test if object is of class DeltaSyncIndex — is.delta_sync_index","text":"Test object class DeltaSyncIndex","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.delta_sync_index.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Test if object is of class DeltaSyncIndex — is.delta_sync_index","text":"","code":"is.delta_sync_index(x)"},{"path":"https://databrickslabs.github.io/brickster/reference/is.delta_sync_index.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Test if object is of class DeltaSyncIndex — is.delta_sync_index","text":"x object","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.delta_sync_index.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Test if object is of class DeltaSyncIndex — is.delta_sync_index","text":"TRUE object inherits DeltaSyncIndex class.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.direct_access_index.html","id":null,"dir":"Reference","previous_headings":"","what":"Test if object is of class DirectAccessIndex — is.direct_access_index","title":"Test if object is of class DirectAccessIndex — is.direct_access_index","text":"Test object class DirectAccessIndex","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.direct_access_index.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Test if object is of class DirectAccessIndex — is.direct_access_index","text":"","code":"is.direct_access_index(x)"},{"path":"https://databrickslabs.github.io/brickster/reference/is.direct_access_index.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Test if object is of class DirectAccessIndex — is.direct_access_index","text":"x object","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.direct_access_index.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Test if object is of class DirectAccessIndex — is.direct_access_index","text":"TRUE object inherits DirectAccessIndex class.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.docker_image.html","id":null,"dir":"Reference","previous_headings":"","what":"Test if object is of class DockerImage — is.docker_image","title":"Test if object is of class DockerImage — is.docker_image","text":"Test object class DockerImage","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.docker_image.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Test if object is of class DockerImage — is.docker_image","text":"","code":"is.docker_image(x)"},{"path":"https://databrickslabs.github.io/brickster/reference/is.docker_image.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Test if object is of class DockerImage — is.docker_image","text":"x object","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.docker_image.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Test if object is of class DockerImage — is.docker_image","text":"TRUE object inherits DockerImage class.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.email_notifications.html","id":null,"dir":"Reference","previous_headings":"","what":"Test if object is of class JobEmailNotifications — is.email_notifications","title":"Test if object is of class JobEmailNotifications — is.email_notifications","text":"Test object class JobEmailNotifications","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.email_notifications.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Test if object is of class JobEmailNotifications — is.email_notifications","text":"","code":"is.email_notifications(x)"},{"path":"https://databrickslabs.github.io/brickster/reference/is.email_notifications.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Test if object is of class JobEmailNotifications — is.email_notifications","text":"x object","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.email_notifications.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Test if object is of class JobEmailNotifications — is.email_notifications","text":"TRUE object inherits JobEmailNotifications class.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.embedding_source_column.html","id":null,"dir":"Reference","previous_headings":"","what":"Test if object is of class EmbeddingSourceColumn — is.embedding_source_column","title":"Test if object is of class EmbeddingSourceColumn — is.embedding_source_column","text":"Test object class EmbeddingSourceColumn","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.embedding_source_column.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Test if object is of class EmbeddingSourceColumn — is.embedding_source_column","text":"","code":"is.embedding_source_column(x)"},{"path":"https://databrickslabs.github.io/brickster/reference/is.embedding_source_column.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Test if object is of class EmbeddingSourceColumn — is.embedding_source_column","text":"x object","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.embedding_source_column.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Test if object is of class EmbeddingSourceColumn — is.embedding_source_column","text":"TRUE object inherits EmbeddingSourceColumn class.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.embedding_vector_column.html","id":null,"dir":"Reference","previous_headings":"","what":"Test if object is of class EmbeddingVectorColumn — is.embedding_vector_column","title":"Test if object is of class EmbeddingVectorColumn — is.embedding_vector_column","text":"Test object class EmbeddingVectorColumn","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.embedding_vector_column.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Test if object is of class EmbeddingVectorColumn — is.embedding_vector_column","text":"","code":"is.embedding_vector_column(x)"},{"path":"https://databrickslabs.github.io/brickster/reference/is.embedding_vector_column.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Test if object is of class EmbeddingVectorColumn — is.embedding_vector_column","text":"x object","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.embedding_vector_column.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Test if object is of class EmbeddingVectorColumn — is.embedding_vector_column","text":"TRUE object inherits EmbeddingVectorColumn class.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.file_storage_info.html","id":null,"dir":"Reference","previous_headings":"","what":"Test if object is of class FileStorageInfo — is.file_storage_info","title":"Test if object is of class FileStorageInfo — is.file_storage_info","text":"Test object class FileStorageInfo","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.file_storage_info.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Test if object is of class FileStorageInfo — is.file_storage_info","text":"","code":"is.file_storage_info(x)"},{"path":"https://databrickslabs.github.io/brickster/reference/is.file_storage_info.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Test if object is of class FileStorageInfo — is.file_storage_info","text":"x object","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.file_storage_info.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Test if object is of class FileStorageInfo — is.file_storage_info","text":"TRUE object inherits FileStorageInfo class.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.for_each_task.html","id":null,"dir":"Reference","previous_headings":"","what":"Test if object is of class ForEachTask — is.for_each_task","title":"Test if object is of class ForEachTask — is.for_each_task","text":"Test object class ForEachTask","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.for_each_task.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Test if object is of class ForEachTask — is.for_each_task","text":"","code":"is.for_each_task(x)"},{"path":"https://databrickslabs.github.io/brickster/reference/is.for_each_task.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Test if object is of class ForEachTask — is.for_each_task","text":"x object","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.for_each_task.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Test if object is of class ForEachTask — is.for_each_task","text":"TRUE object inherits ForEachTask class.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.gcp_attributes.html","id":null,"dir":"Reference","previous_headings":"","what":"Test if object is of class GcpAttributes — is.gcp_attributes","title":"Test if object is of class GcpAttributes — is.gcp_attributes","text":"Test object class GcpAttributes","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.gcp_attributes.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Test if object is of class GcpAttributes — is.gcp_attributes","text":"","code":"is.gcp_attributes(x)"},{"path":"https://databrickslabs.github.io/brickster/reference/is.gcp_attributes.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Test if object is of class GcpAttributes — is.gcp_attributes","text":"x object","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.gcp_attributes.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Test if object is of class GcpAttributes — is.gcp_attributes","text":"TRUE object inherits GcpAttributes class.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.git_source.html","id":null,"dir":"Reference","previous_headings":"","what":"Test if object is of class GitSource — is.git_source","title":"Test if object is of class GitSource — is.git_source","text":"Test object class GitSource","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.git_source.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Test if object is of class GitSource — is.git_source","text":"","code":"is.git_source(x)"},{"path":"https://databrickslabs.github.io/brickster/reference/is.git_source.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Test if object is of class GitSource — is.git_source","text":"x object","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.git_source.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Test if object is of class GitSource — is.git_source","text":"TRUE object inherits GitSource class.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.init_script_info.html","id":null,"dir":"Reference","previous_headings":"","what":"Test if object is of class InitScriptInfo — is.init_script_info","title":"Test if object is of class InitScriptInfo — is.init_script_info","text":"Test object class InitScriptInfo","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.init_script_info.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Test if object is of class InitScriptInfo — is.init_script_info","text":"","code":"is.init_script_info(x)"},{"path":"https://databrickslabs.github.io/brickster/reference/is.init_script_info.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Test if object is of class InitScriptInfo — is.init_script_info","text":"x object","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.init_script_info.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Test if object is of class InitScriptInfo — is.init_script_info","text":"TRUE object inherits InitScriptInfo class.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.job_task.html","id":null,"dir":"Reference","previous_headings":"","what":"Test if object is of class JobTaskSettings — is.job_task","title":"Test if object is of class JobTaskSettings — is.job_task","text":"Test object class JobTaskSettings","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.job_task.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Test if object is of class JobTaskSettings — is.job_task","text":"","code":"is.job_task(x)"},{"path":"https://databrickslabs.github.io/brickster/reference/is.job_task.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Test if object is of class JobTaskSettings — is.job_task","text":"x object","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.job_task.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Test if object is of class JobTaskSettings — is.job_task","text":"TRUE object inherits JobTaskSettings class.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.lib_cran.html","id":null,"dir":"Reference","previous_headings":"","what":"Test if object is of class CranLibrary — is.lib_cran","title":"Test if object is of class CranLibrary — is.lib_cran","text":"Test object class CranLibrary","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.lib_cran.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Test if object is of class CranLibrary — is.lib_cran","text":"","code":"is.lib_cran(x)"},{"path":"https://databrickslabs.github.io/brickster/reference/is.lib_cran.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Test if object is of class CranLibrary — is.lib_cran","text":"x object","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.lib_cran.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Test if object is of class CranLibrary — is.lib_cran","text":"TRUE object inherits CranLibrary class.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.lib_egg.html","id":null,"dir":"Reference","previous_headings":"","what":"Test if object is of class EggLibrary — is.lib_egg","title":"Test if object is of class EggLibrary — is.lib_egg","text":"Test object class EggLibrary","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.lib_egg.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Test if object is of class EggLibrary — is.lib_egg","text":"","code":"is.lib_egg(x)"},{"path":"https://databrickslabs.github.io/brickster/reference/is.lib_egg.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Test if object is of class EggLibrary — is.lib_egg","text":"x object","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.lib_egg.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Test if object is of class EggLibrary — is.lib_egg","text":"TRUE object inherits EggLibrary class.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.lib_jar.html","id":null,"dir":"Reference","previous_headings":"","what":"Test if object is of class JarLibrary — is.lib_jar","title":"Test if object is of class JarLibrary — is.lib_jar","text":"Test object class JarLibrary","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.lib_jar.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Test if object is of class JarLibrary — is.lib_jar","text":"","code":"is.lib_jar(x)"},{"path":"https://databrickslabs.github.io/brickster/reference/is.lib_jar.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Test if object is of class JarLibrary — is.lib_jar","text":"x object","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.lib_jar.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Test if object is of class JarLibrary — is.lib_jar","text":"TRUE object inherits JarLibrary class.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.lib_maven.html","id":null,"dir":"Reference","previous_headings":"","what":"Test if object is of class MavenLibrary — is.lib_maven","title":"Test if object is of class MavenLibrary — is.lib_maven","text":"Test object class MavenLibrary","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.lib_maven.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Test if object is of class MavenLibrary — is.lib_maven","text":"","code":"is.lib_maven(x)"},{"path":"https://databrickslabs.github.io/brickster/reference/is.lib_maven.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Test if object is of class MavenLibrary — is.lib_maven","text":"x object","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.lib_maven.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Test if object is of class MavenLibrary — is.lib_maven","text":"TRUE object inherits MavenLibrary class.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.lib_pypi.html","id":null,"dir":"Reference","previous_headings":"","what":"Test if object is of class PyPiLibrary — is.lib_pypi","title":"Test if object is of class PyPiLibrary — is.lib_pypi","text":"Test object class PyPiLibrary","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.lib_pypi.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Test if object is of class PyPiLibrary — is.lib_pypi","text":"","code":"is.lib_pypi(x)"},{"path":"https://databrickslabs.github.io/brickster/reference/is.lib_pypi.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Test if object is of class PyPiLibrary — is.lib_pypi","text":"x object","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.lib_pypi.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Test if object is of class PyPiLibrary — is.lib_pypi","text":"TRUE object inherits PyPiLibrary class.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.lib_whl.html","id":null,"dir":"Reference","previous_headings":"","what":"Test if object is of class WhlLibrary — is.lib_whl","title":"Test if object is of class WhlLibrary — is.lib_whl","text":"Test object class WhlLibrary","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.lib_whl.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Test if object is of class WhlLibrary — is.lib_whl","text":"","code":"is.lib_whl(x)"},{"path":"https://databrickslabs.github.io/brickster/reference/is.lib_whl.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Test if object is of class WhlLibrary — is.lib_whl","text":"x object","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.lib_whl.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Test if object is of class WhlLibrary — is.lib_whl","text":"TRUE object inherits WhlLibrary class.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.libraries.html","id":null,"dir":"Reference","previous_headings":"","what":"Test if object is of class Libraries — is.libraries","title":"Test if object is of class Libraries — is.libraries","text":"Test object class Libraries","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.libraries.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Test if object is of class Libraries — is.libraries","text":"","code":"is.libraries(x)"},{"path":"https://databrickslabs.github.io/brickster/reference/is.libraries.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Test if object is of class Libraries — is.libraries","text":"x object","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.libraries.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Test if object is of class Libraries — is.libraries","text":"TRUE object inherits Libraries class.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.library.html","id":null,"dir":"Reference","previous_headings":"","what":"Test if object is of class Library — is.library","title":"Test if object is of class Library — is.library","text":"Test object class Library","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.library.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Test if object is of class Library — is.library","text":"","code":"is.library(x)"},{"path":"https://databrickslabs.github.io/brickster/reference/is.library.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Test if object is of class Library — is.library","text":"x object","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.library.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Test if object is of class Library — is.library","text":"TRUE object inherits Library class.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.new_cluster.html","id":null,"dir":"Reference","previous_headings":"","what":"Test if object is of class NewCluster — is.new_cluster","title":"Test if object is of class NewCluster — is.new_cluster","text":"Test object class NewCluster","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.new_cluster.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Test if object is of class NewCluster — is.new_cluster","text":"","code":"is.new_cluster(x)"},{"path":"https://databrickslabs.github.io/brickster/reference/is.new_cluster.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Test if object is of class NewCluster — is.new_cluster","text":"x object","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.new_cluster.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Test if object is of class NewCluster — is.new_cluster","text":"TRUE object inherits NewCluster class.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.notebook_task.html","id":null,"dir":"Reference","previous_headings":"","what":"Test if object is of class NotebookTask — is.notebook_task","title":"Test if object is of class NotebookTask — is.notebook_task","text":"Test object class NotebookTask","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.notebook_task.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Test if object is of class NotebookTask — is.notebook_task","text":"","code":"is.notebook_task(x)"},{"path":"https://databrickslabs.github.io/brickster/reference/is.notebook_task.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Test if object is of class NotebookTask — is.notebook_task","text":"x object","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.notebook_task.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Test if object is of class NotebookTask — is.notebook_task","text":"TRUE object inherits NotebookTask class.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.pipeline_task.html","id":null,"dir":"Reference","previous_headings":"","what":"Test if object is of class PipelineTask — is.pipeline_task","title":"Test if object is of class PipelineTask — is.pipeline_task","text":"Test object class PipelineTask","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.pipeline_task.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Test if object is of class PipelineTask — is.pipeline_task","text":"","code":"is.pipeline_task(x)"},{"path":"https://databrickslabs.github.io/brickster/reference/is.pipeline_task.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Test if object is of class PipelineTask — is.pipeline_task","text":"x object","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.pipeline_task.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Test if object is of class PipelineTask — is.pipeline_task","text":"TRUE object inherits PipelineTask class.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.python_wheel_task.html","id":null,"dir":"Reference","previous_headings":"","what":"Test if object is of class PythonWheelTask — is.python_wheel_task","title":"Test if object is of class PythonWheelTask — is.python_wheel_task","text":"Test object class PythonWheelTask","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.python_wheel_task.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Test if object is of class PythonWheelTask — is.python_wheel_task","text":"","code":"is.python_wheel_task(x)"},{"path":"https://databrickslabs.github.io/brickster/reference/is.python_wheel_task.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Test if object is of class PythonWheelTask — is.python_wheel_task","text":"x object","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.python_wheel_task.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Test if object is of class PythonWheelTask — is.python_wheel_task","text":"TRUE object inherits PythonWheelTask class.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.run_job_task.html","id":null,"dir":"Reference","previous_headings":"","what":"Test if object is of class RunJobTask — is.run_job_task","title":"Test if object is of class RunJobTask — is.run_job_task","text":"Test object class RunJobTask","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.run_job_task.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Test if object is of class RunJobTask — is.run_job_task","text":"","code":"is.run_job_task(x)"},{"path":"https://databrickslabs.github.io/brickster/reference/is.run_job_task.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Test if object is of class RunJobTask — is.run_job_task","text":"x object","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.run_job_task.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Test if object is of class RunJobTask — is.run_job_task","text":"TRUE object inherits RunJobTask class.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.s3_storage_info.html","id":null,"dir":"Reference","previous_headings":"","what":"Test if object is of class S3StorageInfo — is.s3_storage_info","title":"Test if object is of class S3StorageInfo — is.s3_storage_info","text":"Test object class S3StorageInfo","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.s3_storage_info.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Test if object is of class S3StorageInfo — is.s3_storage_info","text":"","code":"is.s3_storage_info(x)"},{"path":"https://databrickslabs.github.io/brickster/reference/is.s3_storage_info.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Test if object is of class S3StorageInfo — is.s3_storage_info","text":"x object","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.s3_storage_info.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Test if object is of class S3StorageInfo — is.s3_storage_info","text":"TRUE object inherits S3StorageInfo class.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.spark_jar_task.html","id":null,"dir":"Reference","previous_headings":"","what":"Test if object is of class SparkJarTask — is.spark_jar_task","title":"Test if object is of class SparkJarTask — is.spark_jar_task","text":"Test object class SparkJarTask","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.spark_jar_task.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Test if object is of class SparkJarTask — is.spark_jar_task","text":"","code":"is.spark_jar_task(x)"},{"path":"https://databrickslabs.github.io/brickster/reference/is.spark_jar_task.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Test if object is of class SparkJarTask — is.spark_jar_task","text":"x object","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.spark_jar_task.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Test if object is of class SparkJarTask — is.spark_jar_task","text":"TRUE object inherits SparkJarTask class.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.spark_python_task.html","id":null,"dir":"Reference","previous_headings":"","what":"Test if object is of class SparkPythonTask — is.spark_python_task","title":"Test if object is of class SparkPythonTask — is.spark_python_task","text":"Test object class SparkPythonTask","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.spark_python_task.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Test if object is of class SparkPythonTask — is.spark_python_task","text":"","code":"is.spark_python_task(x)"},{"path":"https://databrickslabs.github.io/brickster/reference/is.spark_python_task.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Test if object is of class SparkPythonTask — is.spark_python_task","text":"x object","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.spark_python_task.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Test if object is of class SparkPythonTask — is.spark_python_task","text":"TRUE object inherits SparkPythonTask class.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.spark_submit_task.html","id":null,"dir":"Reference","previous_headings":"","what":"Test if object is of class SparkSubmitTask — is.spark_submit_task","title":"Test if object is of class SparkSubmitTask — is.spark_submit_task","text":"Test object class SparkSubmitTask","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.spark_submit_task.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Test if object is of class SparkSubmitTask — is.spark_submit_task","text":"","code":"is.spark_submit_task(x)"},{"path":"https://databrickslabs.github.io/brickster/reference/is.spark_submit_task.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Test if object is of class SparkSubmitTask — is.spark_submit_task","text":"x object","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.spark_submit_task.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Test if object is of class SparkSubmitTask — is.spark_submit_task","text":"TRUE object inherits SparkSubmitTask class.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.sql_file_task.html","id":null,"dir":"Reference","previous_headings":"","what":"Test if object is of class SqlFileTask — is.sql_file_task","title":"Test if object is of class SqlFileTask — is.sql_file_task","text":"Test object class SqlFileTask","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.sql_file_task.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Test if object is of class SqlFileTask — is.sql_file_task","text":"","code":"is.sql_file_task(x)"},{"path":"https://databrickslabs.github.io/brickster/reference/is.sql_file_task.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Test if object is of class SqlFileTask — is.sql_file_task","text":"x object","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.sql_file_task.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Test if object is of class SqlFileTask — is.sql_file_task","text":"TRUE object inherits SqlFileTask class.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.sql_query_task.html","id":null,"dir":"Reference","previous_headings":"","what":"Test if object is of class SqlQueryTask — is.sql_query_task","title":"Test if object is of class SqlQueryTask — is.sql_query_task","text":"Test object class SqlQueryTask","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.sql_query_task.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Test if object is of class SqlQueryTask — is.sql_query_task","text":"","code":"is.sql_query_task(x)"},{"path":"https://databrickslabs.github.io/brickster/reference/is.sql_query_task.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Test if object is of class SqlQueryTask — is.sql_query_task","text":"x object","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.sql_query_task.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Test if object is of class SqlQueryTask — is.sql_query_task","text":"TRUE object inherits SqlQueryTask class.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.valid_task_type.html","id":null,"dir":"Reference","previous_headings":"","what":"Test if object is of class JobTask — is.valid_task_type","title":"Test if object is of class JobTask — is.valid_task_type","text":"Test object class JobTask","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.valid_task_type.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Test if object is of class JobTask — is.valid_task_type","text":"","code":"is.valid_task_type(x)"},{"path":"https://databrickslabs.github.io/brickster/reference/is.valid_task_type.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Test if object is of class JobTask — is.valid_task_type","text":"x object","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.valid_task_type.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Test if object is of class JobTask — is.valid_task_type","text":"TRUE object inherits JobTask class.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.vector_search_index_spec.html","id":null,"dir":"Reference","previous_headings":"","what":"Test if object is of class VectorSearchIndexSpec — is.vector_search_index_spec","title":"Test if object is of class VectorSearchIndexSpec — is.vector_search_index_spec","text":"Test object class VectorSearchIndexSpec","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.vector_search_index_spec.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Test if object is of class VectorSearchIndexSpec — is.vector_search_index_spec","text":"","code":"is.vector_search_index_spec(x)"},{"path":"https://databrickslabs.github.io/brickster/reference/is.vector_search_index_spec.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Test if object is of class VectorSearchIndexSpec — is.vector_search_index_spec","text":"x object","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/is.vector_search_index_spec.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Test if object is of class VectorSearchIndexSpec — is.vector_search_index_spec","text":"TRUE object inherits VectorSearchIndexSpec class.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/job_task.html","id":null,"dir":"Reference","previous_headings":"","what":"Job Task — job_task","title":"Job Task — job_task","text":"Job Task","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/job_task.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Job Task — job_task","text":"","code":"job_task(   task_key,   description = NULL,   depends_on = c(),   existing_cluster_id = NULL,   new_cluster = NULL,   job_cluster_key = NULL,   task,   libraries = NULL,   email_notifications = NULL,   timeout_seconds = NULL,   max_retries = 0,   min_retry_interval_millis = 0,   retry_on_timeout = FALSE,   run_if = c(\"ALL_SUCCESS\", \"ALL_DONE\", \"NONE_FAILED\", \"AT_LEAST_ONE_SUCCESS\",     \"ALL_FAILED\", \"AT_LEAST_ONE_FAILED\") )"},{"path":"https://databrickslabs.github.io/brickster/reference/job_task.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Job Task — job_task","text":"task_key unique name task. field used refer task tasks. field required must unique within parent job. db_jobs_update() db_jobs_reset(), field used reference tasks updated reset. maximum length 100 characters. description optional description task. maximum length 4096 bytes. depends_on Vector task_key's specifying dependency graph task. task_key's specified field must complete successfully executing task. field required job consists one task. existing_cluster_id ID existing cluster used runs task. new_cluster Instance new_cluster(). job_cluster_key Task executed reusing cluster specified db_jobs_create() job_clusters parameter. task One notebook_task(), spark_jar_task(), spark_python_task(), spark_submit_task(), pipeline_task(), python_wheel_task(). libraries Instance libraries(). email_notifications Instance email_notifications. timeout_seconds optional timeout applied run job task. default behavior timeout. max_retries optional maximum number times retry unsuccessful run. run considered unsuccessful completes FAILED result_state INTERNAL_ERROR life_cycle_state. value -1 means retry indefinitely value 0 means never retry. default behavior never retry. min_retry_interval_millis Optional minimal interval milliseconds start failed run subsequent retry run. default behavior unsuccessful runs immediately retried. retry_on_timeout Optional policy specify whether retry task times . default behavior retry timeout. run_if condition determining whether task run dependencies completed.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/job_tasks.html","id":null,"dir":"Reference","previous_headings":"","what":"Job Tasks — job_tasks","title":"Job Tasks — job_tasks","text":"Job Tasks","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/job_tasks.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Job Tasks — job_tasks","text":"","code":"job_tasks(...)"},{"path":"https://databrickslabs.github.io/brickster/reference/job_tasks.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Job Tasks — job_tasks","text":"... Multiple Instance tasks job_task().","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/lib_cran.html","id":null,"dir":"Reference","previous_headings":"","what":"Cran Library (R) — lib_cran","title":"Cran Library (R) — lib_cran","text":"Cran Library (R)","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/lib_cran.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Cran Library (R) — lib_cran","text":"","code":"lib_cran(package, repo = NULL)"},{"path":"https://databrickslabs.github.io/brickster/reference/lib_cran.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Cran Library (R) — lib_cran","text":"package name CRAN package install. repo repository package can found. specified, default CRAN repo used.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/lib_egg.html","id":null,"dir":"Reference","previous_headings":"","what":"Egg Library (Python) — lib_egg","title":"Egg Library (Python) — lib_egg","text":"Egg Library (Python)","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/lib_egg.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Egg Library (Python) — lib_egg","text":"","code":"lib_egg(egg)"},{"path":"https://databrickslabs.github.io/brickster/reference/lib_egg.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Egg Library (Python) — lib_egg","text":"egg URI egg installed. DBFS S3 URIs supported. example: dbfs://egg s3://-bucket/egg. S3 used, make sure cluster read access library. may need launch cluster instance profile access S3 URI.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/lib_jar.html","id":null,"dir":"Reference","previous_headings":"","what":"Jar Library (Scala) — lib_jar","title":"Jar Library (Scala) — lib_jar","text":"Jar Library (Scala)","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/lib_jar.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Jar Library (Scala) — lib_jar","text":"","code":"lib_jar(jar)"},{"path":"https://databrickslabs.github.io/brickster/reference/lib_jar.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Jar Library (Scala) — lib_jar","text":"jar URI JAR installed. DBFS S3 URIs supported. example: dbfs:/mnt/databricks/library.jar s3://-bucket/library.jar. S3 used, make sure cluster read access library. may need launch cluster instance profile access S3 URI.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/lib_maven.html","id":null,"dir":"Reference","previous_headings":"","what":"Maven Library (Scala) — lib_maven","title":"Maven Library (Scala) — lib_maven","text":"Maven Library (Scala)","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/lib_maven.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Maven Library (Scala) — lib_maven","text":"","code":"lib_maven(coordinates, repo = NULL, exclusions = NULL)"},{"path":"https://databrickslabs.github.io/brickster/reference/lib_maven.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Maven Library (Scala) — lib_maven","text":"coordinates Gradle-style Maven coordinates. example: org.jsoup:jsoup:1.7.2. repo Maven repo install Maven package . omitted, Maven Central Repository Spark Packages searched. exclusions List dependencies exclude. example: list(\"slf4j:slf4j\", \"*:hadoop-client\"). Maven dependency exclusions.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/lib_pypi.html","id":null,"dir":"Reference","previous_headings":"","what":"PyPi Library (Python) — lib_pypi","title":"PyPi Library (Python) — lib_pypi","text":"PyPi Library (Python)","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/lib_pypi.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"PyPi Library (Python) — lib_pypi","text":"","code":"lib_pypi(package, repo = NULL)"},{"path":"https://databrickslabs.github.io/brickster/reference/lib_pypi.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"PyPi Library (Python) — lib_pypi","text":"package name PyPI package install. optional exact version specification also supported. Examples: simplejson simplejson==3.8.0. repo repository package can found. specified, default pip index used.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/lib_whl.html","id":null,"dir":"Reference","previous_headings":"","what":"Wheel Library (Python) — lib_whl","title":"Wheel Library (Python) — lib_whl","text":"Wheel Library (Python)","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/lib_whl.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Wheel Library (Python) — lib_whl","text":"","code":"lib_whl(whl)"},{"path":"https://databrickslabs.github.io/brickster/reference/lib_whl.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Wheel Library (Python) — lib_whl","text":"whl URI wheel zipped wheels installed. DBFS S3 URIs supported. example: dbfs://whl s3://-bucket/whl. S3 used, make sure cluster read access library. may need launch cluster instance profile access S3 URI. Also wheel file name needs use correct convention. zipped wheels installed, file name suffix .wheelhouse.zip.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/libraries.html","id":null,"dir":"Reference","previous_headings":"","what":"Libraries — libraries","title":"Libraries — libraries","text":"Libraries","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/libraries.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Libraries — libraries","text":"","code":"libraries(...)"},{"path":"https://databrickslabs.github.io/brickster/reference/libraries.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Libraries — libraries","text":"... Accepts multiple instances lib_jar(), lib_cran(), lib_maven(), lib_pypi(), lib_whl(), lib_egg().","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/libraries.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Libraries — libraries","text":"Optional list libraries installed cluster executes task.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/new_cluster.html","id":null,"dir":"Reference","previous_headings":"","what":"New Cluster — new_cluster","title":"New Cluster — new_cluster","text":"New Cluster","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/new_cluster.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"New Cluster — new_cluster","text":"","code":"new_cluster(   num_workers,   spark_version,   node_type_id,   driver_node_type_id = NULL,   autoscale = NULL,   cloud_attrs = NULL,   spark_conf = NULL,   spark_env_vars = NULL,   custom_tags = NULL,   ssh_public_keys = NULL,   log_conf = NULL,   init_scripts = NULL,   enable_elastic_disk = TRUE,   driver_instance_pool_id = NULL,   instance_pool_id = NULL,   kind = c(\"CLASSIC_PREVIEW\"),   data_security_mode = c(\"NONE\", \"SINGLE_USER\", \"USER_ISOLATION\", \"LEGACY_TABLE_ACL\",     \"LEGACY_PASSTHROUGH\", \"LEGACY_SINGLE_USER\", \"LEGACY_SINGLE_USER_STANDARD\",     \"DATA_SECURITY_MODE_STANDARD\", \"DATA_SECURITY_MODE_DEDICATED\",     \"DATA_SECURITY_MODE_AUTO\") )"},{"path":"https://databrickslabs.github.io/brickster/reference/new_cluster.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"New Cluster — new_cluster","text":"num_workers Number worker nodes cluster . cluster one Spark driver num_workers executors total num_workers + 1 Spark nodes. spark_version runtime version cluster. can retrieve list available runtime versions using db_cluster_runtime_versions(). node_type_id node type worker nodes. db_cluster_list_node_types() can used see available node types. driver_node_type_id node type Spark driver. field optional; unset, driver node type set value node_type_id defined . db_cluster_list_node_types() can used see available node types. autoscale Instance cluster_autoscale(). cloud_attrs Attributes related clusters running specific cloud provider. Defaults aws_attributes(). Must one aws_attributes(), azure_attributes(), gcp_attributes(). spark_conf Named list. object containing set optional, user-specified Spark configuration key-value pairs. can also pass string extra JVM options driver executors via spark.driver.extraJavaOptions spark.executor.extraJavaOptions respectively. E.g. list(\"spark.speculation\" = true, \"spark.streaming.ui.retainedBatches\" = 5). spark_env_vars Named list. User-specified environment variable key-value pairs. order specify additional set SPARK_DAEMON_JAVA_OPTS, recommend appending $SPARK_DAEMON_JAVA_OPTS shown following example. ensures default Databricks managed environmental variables included well. E.g. {\"SPARK_DAEMON_JAVA_OPTS\": \"$SPARK_DAEMON_JAVA_OPTS -Dspark.shuffle.service.enabled=true\"} custom_tags Named list. object containing set tags cluster resources. Databricks tags cluster resources tags addition default_tags. Databricks allows 45 custom tags. ssh_public_keys List. SSH public key contents added Spark node cluster. corresponding private keys can used login user name ubuntu port 2200. 10 keys can specified. log_conf Instance cluster_log_conf(). init_scripts Instance init_script_info(). enable_elastic_disk enabled, cluster dynamically acquire additional disk space Spark workers running low disk space. driver_instance_pool_id ID instance pool use driver node. must also specify instance_pool_id. Optional. instance_pool_id ID instance pool use cluster nodes. driver_instance_pool_id present, instance_pool_id used worker nodes . Otherwise, used driver worker nodes. Optional. kind kind compute described compute specification. data_security_mode Data security mode decides data governance model use accessing data cluster.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/notebook_task.html","id":null,"dir":"Reference","previous_headings":"","what":"Notebook Task — notebook_task","title":"Notebook Task — notebook_task","text":"Notebook Task","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/notebook_task.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Notebook Task — notebook_task","text":"","code":"notebook_task(notebook_path, base_parameters = NULL)"},{"path":"https://databrickslabs.github.io/brickster/reference/notebook_task.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Notebook Task — notebook_task","text":"notebook_path absolute path notebook run Databricks workspace. path must begin slash. base_parameters Named list base parameters used run job.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/notebook_task.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Notebook Task — notebook_task","text":"run initiated call db_jobs_run_now() parameters specified, two parameters maps merged. key specified base_parameters run-now, value run-now used. Use Task parameter variables set parameters containing information job runs. notebook takes parameter specified job’s base_parameters run-now override parameters, default value notebook used. Retrieve parameters notebook using dbutils.widgets.get.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/open_workspace.html","id":null,"dir":"Reference","previous_headings":"","what":"Connect to Databricks Workspace — open_workspace","title":"Connect to Databricks Workspace — open_workspace","text":"Connect Databricks Workspace","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/open_workspace.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Connect to Databricks Workspace — open_workspace","text":"","code":"open_workspace(host = db_host(), token = db_token(), name = NULL)"},{"path":"https://databrickslabs.github.io/brickster/reference/open_workspace.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Connect to Databricks Workspace — open_workspace","text":"host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token(). name Desired name assign connection","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/open_workspace.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Connect to Databricks Workspace — open_workspace","text":"","code":"if (FALSE) { # \\dontrun{ open_workspace(host = db_host(), token = db_token, name = \"MyWorkspace\") } # }"},{"path":"https://databrickslabs.github.io/brickster/reference/pipeline_task.html","id":null,"dir":"Reference","previous_headings":"","what":"Pipeline Task — pipeline_task","title":"Pipeline Task — pipeline_task","text":"Pipeline Task","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/pipeline_task.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Pipeline Task — pipeline_task","text":"","code":"pipeline_task(pipeline_id)"},{"path":"https://databrickslabs.github.io/brickster/reference/pipeline_task.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Pipeline Task — pipeline_task","text":"pipeline_id full name pipeline task execute.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/python_wheel_task.html","id":null,"dir":"Reference","previous_headings":"","what":"Python Wheel Task — python_wheel_task","title":"Python Wheel Task — python_wheel_task","text":"Python Wheel Task","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/python_wheel_task.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Python Wheel Task — python_wheel_task","text":"","code":"python_wheel_task(package_name, entry_point = NULL, parameters = list())"},{"path":"https://databrickslabs.github.io/brickster/reference/python_wheel_task.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Python Wheel Task — python_wheel_task","text":"package_name Name package execute. entry_point Named entry point use, exist metadata package executes function package directly using $packageName.$entryPoint(). parameters Command-line parameters passed python wheel task.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/read_databrickscfg.html","id":null,"dir":"Reference","previous_headings":"","what":"Reads Databricks CLI Config — read_databrickscfg","title":"Reads Databricks CLI Config — read_databrickscfg","text":"Reads Databricks CLI Config","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/read_databrickscfg.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Reads Databricks CLI Config — read_databrickscfg","text":"","code":"read_databrickscfg(key = c(\"token\", \"host\", \"wsid\"), profile = NULL)"},{"path":"https://databrickslabs.github.io/brickster/reference/read_databrickscfg.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Reads Databricks CLI Config — read_databrickscfg","text":"key value fetch profile. One token, host, wsid profile Character, name profile retrieve values","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/read_databrickscfg.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Reads Databricks CLI Config — read_databrickscfg","text":"named list values associated profile","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/read_databrickscfg.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Reads Databricks CLI Config — read_databrickscfg","text":"Reads .databrickscfg file retrieves values associated given profile. Brickster searches config file user's home directory default. see can run Sys.getenv(\"HOME\") unix-like operating systems, , Sys.getenv(\"USERPROFILE\") windows. alternate location used environment variable DATABRICKS_CONFIG_FILE set.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/read_env_var.html","id":null,"dir":"Reference","previous_headings":"","what":"Reads Environment Variables — read_env_var","title":"Reads Environment Variables — read_env_var","text":"Reads Environment Variables","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/read_env_var.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Reads Environment Variables — read_env_var","text":"","code":"read_env_var(key = c(\"token\", \"host\", \"wsid\"), profile = NULL, error = TRUE)"},{"path":"https://databrickslabs.github.io/brickster/reference/read_env_var.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Reads Environment Variables — read_env_var","text":"key value fetch profile. One token, host, wsid profile Character, name profile retrieve values error Boolean, key found error raised","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/read_env_var.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Reads Environment Variables — read_env_var","text":"named list values associated profile","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/read_env_var.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Reads Environment Variables — read_env_var","text":"Fetches relevant environment variables based profile","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/remove_lib_path.html","id":null,"dir":"Reference","previous_headings":"","what":"Remove Library Path — remove_lib_path","title":"Remove Library Path — remove_lib_path","text":"Remove Library Path","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/remove_lib_path.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Remove Library Path — remove_lib_path","text":"","code":"remove_lib_path(path, version = FALSE)"},{"path":"https://databrickslabs.github.io/brickster/reference/remove_lib_path.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Remove Library Path — remove_lib_path","text":"path Directory remove .libPaths(). version TRUE add R version string end path removal.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/run_job_task.html","id":null,"dir":"Reference","previous_headings":"","what":"Run Job Task — run_job_task","title":"Run Job Task — run_job_task","text":"Run Job Task","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/run_job_task.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Run Job Task — run_job_task","text":"","code":"run_job_task(job_id, job_parameters, full_refresh = FALSE)"},{"path":"https://databrickslabs.github.io/brickster/reference/run_job_task.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Run Job Task — run_job_task","text":"job_id ID job trigger. job_parameters Named list, job-level parameters used trigger job. full_refresh pipeline perform full refresh.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/s3_storage_info.html","id":null,"dir":"Reference","previous_headings":"","what":"S3 Storage Info — s3_storage_info","title":"S3 Storage Info — s3_storage_info","text":"S3 Storage Info","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/s3_storage_info.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"S3 Storage Info — s3_storage_info","text":"","code":"s3_storage_info(   destination,   region = NULL,   endpoint = NULL,   enable_encryption = FALSE,   encryption_type = c(\"sse-s3\", \"sse-kms\"),   kms_key = NULL,   canned_acl = NULL )"},{"path":"https://databrickslabs.github.io/brickster/reference/s3_storage_info.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"S3 Storage Info — s3_storage_info","text":"destination S3 destination. example: s3://-bucket/-prefix. must configure cluster instance profile instance profile must write access destination. use AWS keys. region S3 region. example: us-west-2. Either region endpoint must set. set, endpoint used. endpoint S3 endpoint. example: https://s3-us-west-2.amazonaws.com. Either region endpoint must set. set, endpoint used. enable_encryption Boolean (Default: FALSE). TRUE Enable server side encryption. encryption_type Encryption type, sse-s3 sse-kms. used encryption enabled default type sse-s3. kms_key KMS key used encryption enabled encryption type set sse-kms. canned_acl Set canned access control list. example: bucket-owner-full-control. canned_acl set, cluster instance profile must s3:PutObjectAcl permission destination bucket prefix. full list possible canned ACLs can found docs. default object owner gets full control. using cross account role writing data, may want set bucket-owner-full-control make bucket owner able read logs.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/show-DatabricksConnection-method.html","id":null,"dir":"Reference","previous_headings":"","what":"Show method for DatabricksConnection — show,DatabricksConnection-method","title":"Show method for DatabricksConnection — show,DatabricksConnection-method","text":"Show method DatabricksConnection","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/show-DatabricksConnection-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Show method for DatabricksConnection — show,DatabricksConnection-method","text":"","code":"# S4 method for class 'DatabricksConnection' show(object)"},{"path":"https://databrickslabs.github.io/brickster/reference/show-DatabricksConnection-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Show method for DatabricksConnection — show,DatabricksConnection-method","text":"object DatabricksConnection object","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/show-DatabricksDriver-method.html","id":null,"dir":"Reference","previous_headings":"","what":"Show method for DatabricksDriver — show,DatabricksDriver-method","title":"Show method for DatabricksDriver — show,DatabricksDriver-method","text":"Show method DatabricksDriver","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/show-DatabricksDriver-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Show method for DatabricksDriver — show,DatabricksDriver-method","text":"","code":"# S4 method for class 'DatabricksDriver' show(object)"},{"path":"https://databrickslabs.github.io/brickster/reference/show-DatabricksDriver-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Show method for DatabricksDriver — show,DatabricksDriver-method","text":"object DatabricksDriver object","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/show-DatabricksResult-method.html","id":null,"dir":"Reference","previous_headings":"","what":"Show method for DatabricksResult — show,DatabricksResult-method","title":"Show method for DatabricksResult — show,DatabricksResult-method","text":"Show method DatabricksResult","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/show-DatabricksResult-method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Show method for DatabricksResult — show,DatabricksResult-method","text":"","code":"# S4 method for class 'DatabricksResult' show(object)"},{"path":"https://databrickslabs.github.io/brickster/reference/show-DatabricksResult-method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Show method for DatabricksResult — show,DatabricksResult-method","text":"object DatabricksResult object","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/spark_jar_task.html","id":null,"dir":"Reference","previous_headings":"","what":"Spark Jar Task — spark_jar_task","title":"Spark Jar Task — spark_jar_task","text":"Spark Jar Task","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/spark_jar_task.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Spark Jar Task — spark_jar_task","text":"","code":"spark_jar_task(main_class_name, parameters = list())"},{"path":"https://databrickslabs.github.io/brickster/reference/spark_jar_task.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Spark Jar Task — spark_jar_task","text":"main_class_name full name class containing main method executed. class must contained JAR provided library. code must use SparkContext.getOrCreate obtain Spark context; otherwise, runs job fail. parameters Named list. Parameters passed main method. Use Task parameter variables set parameters containing information job runs.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/spark_python_task.html","id":null,"dir":"Reference","previous_headings":"","what":"Spark Python Task — spark_python_task","title":"Spark Python Task — spark_python_task","text":"Spark Python Task","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/spark_python_task.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Spark Python Task — spark_python_task","text":"","code":"spark_python_task(python_file, parameters = list())"},{"path":"https://databrickslabs.github.io/brickster/reference/spark_python_task.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Spark Python Task — spark_python_task","text":"python_file URI Python file executed. DBFS S3 paths supported. parameters List. Command line parameters passed Python file. Use Task parameter variables set parameters containing information job runs.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/spark_submit_task.html","id":null,"dir":"Reference","previous_headings":"","what":"Spark Submit Task — spark_submit_task","title":"Spark Submit Task — spark_submit_task","text":"Spark Submit Task","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/spark_submit_task.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Spark Submit Task — spark_submit_task","text":"","code":"spark_submit_task(parameters)"},{"path":"https://databrickslabs.github.io/brickster/reference/spark_submit_task.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Spark Submit Task — spark_submit_task","text":"parameters List. Command-line parameters passed spark submit. Use Task parameter variables set parameters containing information job runs.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/sql_file_task.html","id":null,"dir":"Reference","previous_headings":"","what":"SQL File Task — sql_file_task","title":"SQL File Task — sql_file_task","text":"SQL File Task","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/sql_file_task.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"SQL File Task — sql_file_task","text":"","code":"sql_file_task(path, warehouse_id, source = NULL, parameters = NULL)"},{"path":"https://databrickslabs.github.io/brickster/reference/sql_file_task.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"SQL File Task — sql_file_task","text":"path Path SQL file. Must relative source remote Git repository absolute workspace paths. warehouse_id canonical identifier SQL warehouse. source Optional location type SQL file. set WORKSPACE, SQL file retrieved local Databricks workspace. set GIT, SQL file retrieved Git repository defined git_source() value empty, task use GIT git_source() defined WORKSPACE otherwise. parameters Named list paramters used run job.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/sql_query_fields.DatabricksConnection.html","id":null,"dir":"Reference","previous_headings":"","what":"SQL Query Fields for Databricks connections — sql_query_fields.DatabricksConnection","title":"SQL Query Fields for Databricks connections — sql_query_fields.DatabricksConnection","text":"Generate SQL field discovery queries optimized Databricks. method generates appropriate SQL discovering table fields.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/sql_query_fields.DatabricksConnection.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"SQL Query Fields for Databricks connections — sql_query_fields.DatabricksConnection","text":"","code":"# S3 method for class 'DatabricksConnection' sql_query_fields(con, sql, ...)"},{"path":"https://databrickslabs.github.io/brickster/reference/sql_query_fields.DatabricksConnection.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"SQL Query Fields for Databricks connections — sql_query_fields.DatabricksConnection","text":"con DatabricksConnection object sql SQL query discover fields ... Additional arguments passed methods","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/sql_query_fields.DatabricksConnection.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"SQL Query Fields for Databricks connections — sql_query_fields.DatabricksConnection","text":"SQL object field discovery","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/sql_query_save.DatabricksConnection.html","id":null,"dir":"Reference","previous_headings":"","what":"Create temporary views and tables in Databricks — sql_query_save.DatabricksConnection","title":"Create temporary views and tables in Databricks — sql_query_save.DatabricksConnection","text":"Create temporary views tables Databricks","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/sql_query_save.DatabricksConnection.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create temporary views and tables in Databricks — sql_query_save.DatabricksConnection","text":"","code":"# S3 method for class 'DatabricksConnection' sql_query_save(con, sql, name, temporary = TRUE, ...)"},{"path":"https://databrickslabs.github.io/brickster/reference/sql_query_save.DatabricksConnection.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create temporary views and tables in Databricks — sql_query_save.DatabricksConnection","text":"con DatabricksConnection object sql SQL query save table/view name Name temporary view table temporary Whether object temporary (default: TRUE) ... Additional arguments (ignored)","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/sql_query_save.DatabricksConnection.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create temporary views and tables in Databricks — sql_query_save.DatabricksConnection","text":"table/view name (invisibly)","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/sql_query_task.html","id":null,"dir":"Reference","previous_headings":"","what":"SQL Query Task — sql_query_task","title":"SQL Query Task — sql_query_task","text":"SQL Query Task","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/sql_query_task.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"SQL Query Task — sql_query_task","text":"","code":"sql_query_task(query_id, warehouse_id, parameters = NULL)"},{"path":"https://databrickslabs.github.io/brickster/reference/sql_query_task.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"SQL Query Task — sql_query_task","text":"query_id canonical identifier SQL query. warehouse_id canonical identifier SQL warehouse. parameters Named list paramters used run job.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/reference/sql_table_analyze.DatabricksConnection.html","id":null,"dir":"Reference","previous_headings":"","what":"Handle table analysis for Databricks — sql_table_analyze.DatabricksConnection","title":"Handle table analysis for Databricks — sql_table_analyze.DatabricksConnection","text":"Handle table analysis Databricks","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/sql_table_analyze.DatabricksConnection.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Handle table analysis for Databricks — sql_table_analyze.DatabricksConnection","text":"","code":"# S3 method for class 'DatabricksConnection' sql_table_analyze(con, table, ...)"},{"path":"https://databrickslabs.github.io/brickster/reference/sql_table_analyze.DatabricksConnection.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Handle table analysis for Databricks — sql_table_analyze.DatabricksConnection","text":"con DatabricksConnection object table Table name analyze ... Additional arguments (ignored)","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/sql_table_analyze.DatabricksConnection.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Handle table analysis for Databricks — sql_table_analyze.DatabricksConnection","text":"SQL statement table analysis","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/sql_translation.DatabricksConnection.html","id":null,"dir":"Reference","previous_headings":"","what":"SQL translation environment for Databricks SQL — sql_translation.DatabricksConnection","title":"SQL translation environment for Databricks SQL — sql_translation.DatabricksConnection","text":"SQL translation environment Databricks SQL","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/sql_translation.DatabricksConnection.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"SQL translation environment for Databricks SQL — sql_translation.DatabricksConnection","text":"","code":"# S3 method for class 'DatabricksConnection' sql_translation(con)"},{"path":"https://databrickslabs.github.io/brickster/reference/use_databricks_cfg.html","id":null,"dir":"Reference","previous_headings":"","what":"Returns whether or not to use a .databrickscfg file — use_databricks_cfg","title":"Returns whether or not to use a .databrickscfg file — use_databricks_cfg","text":"Returns whether use .databrickscfg file","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/use_databricks_cfg.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Returns whether or not to use a .databrickscfg file — use_databricks_cfg","text":"","code":"use_databricks_cfg()"},{"path":"https://databrickslabs.github.io/brickster/reference/use_databricks_cfg.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Returns whether or not to use a .databrickscfg file — use_databricks_cfg","text":"boolean","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/use_databricks_cfg.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Returns whether or not to use a .databrickscfg file — use_databricks_cfg","text":"Indicates .databrickscfg used instead environment variables either use_databrickscfg option set Posit Workbench managed OAuth credentials detected.","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/wait_for_lib_installs.html","id":null,"dir":"Reference","previous_headings":"","what":"Wait for Libraries to Install on Databricks Cluster — wait_for_lib_installs","title":"Wait for Libraries to Install on Databricks Cluster — wait_for_lib_installs","text":"Wait Libraries Install Databricks Cluster","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/wait_for_lib_installs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Wait for Libraries to Install on Databricks Cluster — wait_for_lib_installs","text":"","code":"wait_for_lib_installs(   cluster_id,   polling_interval = 5,   allow_failures = FALSE,   host = db_host(),   token = db_token() )"},{"path":"https://databrickslabs.github.io/brickster/reference/wait_for_lib_installs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Wait for Libraries to Install on Databricks Cluster — wait_for_lib_installs","text":"cluster_id Unique identifier Databricks cluster. polling_interval Number seconds wait status checks allow_failures FALSE (default) error libraries status FAILED. TRUE FAILED installs presented warning. host Databricks workspace URL, defaults calling db_host(). token Databricks workspace token, defaults calling db_token().","code":""},{"path":"https://databrickslabs.github.io/brickster/reference/wait_for_lib_installs.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Wait for Libraries to Install on Databricks Cluster — wait_for_lib_installs","text":"Library installs Databricks clusters asynchronous, function allows repeatedly check installation status library. Can used block scripts required dependencies installed.","code":""},{"path":[]},{"path":"https://databrickslabs.github.io/brickster/news/index.html","id":"brickster-029","dir":"Changelog","previous_headings":"","what":"brickster 0.2.9","title":"brickster 0.2.9","text":"Added DBI + dbplyr backend support: DatabricksSQL() driver standard DBI operations","code":""},{"path":"https://databrickslabs.github.io/brickster/news/index.html","id":"brickster-028","dir":"Changelog","previous_headings":"","what":"brickster 0.2.8","title":"brickster 0.2.8","text":"CRAN release: 2025-06-06 Added SQL Queries API coverage Updated Jobs 2.2 Added additional tasks jobs: for_each_task, condition_task, sql_query_task, sql_file_task, run_job_task Removing Python SQL connector db_sql_query supersedes . Added db_sql_query simplify execution SQL Adjusted db_repl handle mulit-line expressions (R ) Removed RStudio Addins send lines/selection/files console Moved arrow Suggests","code":""},{"path":"https://databrickslabs.github.io/brickster/news/index.html","id":"brickster-027","dir":"Changelog","previous_headings":"","what":"brickster 0.2.7","title":"brickster 0.2.7","text":"CRAN release: 2025-05-06 Exporting UC table functions (db_uc_table*) (#72) Adding support direct_download option db_workspace_export() Exporting UC Catalog/Schema get/list functions (#72) Adding support UC Volume management (#72) Fixing command execution context cancel (#86, #87) Adding stricter version requirements {httr2} (#81, #63)","code":""},{"path":"https://databrickslabs.github.io/brickster/news/index.html","id":"brickster-026","dir":"Changelog","previous_headings":"","what":"brickster 0.2.6","title":"brickster 0.2.6","text":"CRAN release: 2025-01-21 Fixing db_volume_delete() function (#73, @vladimirobucina) Adjustments ensure {httr2} changes don’t break things (#75, @hadley)","code":""},{"path":"https://databrickslabs.github.io/brickster/news/index.html","id":"brickster-025","dir":"Changelog","previous_headings":"","what":"brickster 0.2.5","title":"brickster 0.2.5","text":"CRAN release: 2024-11-13 Adding db_repl() remote REPL Databricks cluster (#53) Removing defunct RStudio add-browsing Databricks compute Changes DESCRIPTION file preperation CRAN (#64) Removal notebook_use_posit_repo() notebook_enable_htmlwidgets() incompatible CRAN (#64) Removing kntir engine due many render edge cases solvable Adding shortcuts REPL addins Added db_context_command_run_and_wait Adjusted tests use withr (#68)","code":""},{"path":"https://databrickslabs.github.io/brickster/news/index.html","id":"brickster-024","dir":"Changelog","previous_headings":"","what":"brickster 0.2.4","title":"brickster 0.2.4","text":"open_workspace() rstudio connection pane heavily revised enhance browsing unity catalog also remove DBFS WSFS browsing (#52)","code":""},{"path":"https://databrickslabs.github.io/brickster/news/index.html","id":"brickster-023","dir":"Changelog","previous_headings":"","what":"brickster 0.2.3","title":"brickster 0.2.3","text":"Adding NEWS.md Renamed set_lib_path add_lib_path added parameter Adding OAuth U2M support (workspace level), considered default DATABRICKS_TOKEN isn’t specified (e.g db_token() returns NULL) Updating authentication vignette include information OAuth Updating README.md include quick start clearer information Adding vector search index functions","code":""}]
