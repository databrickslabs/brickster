% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/data-structures.R
\name{sql_file_task}
\alias{sql_file_task}
\title{SQL File Task}
\usage{
sql_file_task(path, source = NULL, warehouse_id, parameters)
}
\arguments{
\item{path}{Path of the SQL file. Must be relative if the source is a remote
Git repository and absolute for workspace paths.}

\item{source}{Optional location type of the SQL file. When set to \code{WORKSPACE},
the SQL file will be retrieved from the local Databricks workspace. When set
to \code{GIT}, the SQL file will be retrieved from a Git repository defined in
\code{\link[=git_source]{git_source()}} If the value is empty, the task will use \code{GIT} if
\code{\link[=git_source]{git_source()}} is defined and \code{WORKSPACE} otherwise.}

\item{warehouse_id}{The canonical identifier of the SQL warehouse.}

\item{parameters}{Named list of paramters to be used for each run of this job.}
}
\description{
SQL File Task
}
\seealso{
Other Task Objects: 
\code{\link{condition_task}()},
\code{\link{email_notifications}()},
\code{\link{for_each_task}()},
\code{\link{libraries}()},
\code{\link{new_cluster}()},
\code{\link{notebook_task}()},
\code{\link{pipeline_task}()},
\code{\link{python_wheel_task}()},
\code{\link{run_job_task}()},
\code{\link{spark_jar_task}()},
\code{\link{spark_python_task}()},
\code{\link{spark_submit_task}()},
\code{\link{sql_query_task}()}
}
\concept{Task Objects}
